
N:\Caffe\examples\mnist>REM go to the caffe root 

N:\Caffe\examples\mnist>cd ../../ 

N:\Caffe>set BIN=build/x64/Release 

N:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/mnist/simpnet_nodrp_solver.prototxt --weights=examples/mnist/simpnet_nodrp_iter_600.caffemodel 
I1101 19:20:25.678902  2200 caffe.cpp:186] Using GPUs 0
I1101 19:20:25.894623  2200 caffe.cpp:191] GPU 0: GeForce GTX 980
I1101 19:20:26.103435  2200 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1101 19:20:26.103435  2200 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 600
base_lr: 0.0192415
display: 100
max_iter: 800000
lr_policy: "fixed"
momentum: 0.9
snapshot: 600
snapshot_prefix: "examples/mnist/simpnet_nodrp"
solver_mode: GPU
device_id: 0
net: "examples/mnist/simpnet_nodrp_train_test.prototxt"
momentum2: 0.999
I1101 19:20:26.104436  2200 solver.cpp:91] Creating training net from net file: examples/mnist/simpnet_nodrp_train_test.prototxt
I1101 19:20:26.104936  2200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1101 19:20:26.104936  2200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1101 19:20:26.104936  2200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1101 19:20:26.104936  2200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1101 19:20:26.104936  2200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1101 19:20:26.104936  2200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1101 19:20:26.105437  2200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1101 19:20:26.105437  2200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1101 19:20:26.105437  2200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1101 19:20:26.105437  2200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1101 19:20:26.105437  2200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1101 19:20:26.105437  2200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1101 19:20:26.105437  2200 net.cpp:49] Initializing net from parameters: 
name: "SimpNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb_norm2"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "bn1"
  top: "scale1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "relu1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "bn1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "bn1_0"
  top: "scale1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "scale1_0"
  top: "relu1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "bn2"
  top: "scale2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "relu2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "bn2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "bn2_1"
  top: "scale2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "scale2_1"
  top: "relu2_1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "relu2_1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "bn2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "bn2_2"
  top: "scale2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "scale2_2"
  top: "relu2_2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "relu2_2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "bn3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "bn3"
  top: "scale3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "pool4"
  top: "bn4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "bn4"
  top: "scale4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "relu4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "bn4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "bn4_1"
  top: "scale4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "scale4_1"
  top: "relu4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "relu4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "bn4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "bn4_2"
  top: "scale4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "scale4_2"
  top: "relu4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "relu4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "bn4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "bn4_0"
  top: "scale4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "scale4_0"
  top: "relu4_0"
}
layer {
  name: "cccp4"
  type: "Convolution"
  bottom: "relu4_0"
  top: "cccp4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2048
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp4"
  type: "ReLU"
  bottom: "cccp4"
  top: "cccp4"
}
layer {
  name: "cccp5"
  type: "Convolution"
  bottom: "cccp4"
  top: "cccp5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp5"
  type: "ReLU"
  bottom: "cccp5"
  top: "cccp5"
}
layer {
  name: "poolcp5"
  type: "Pooling"
  bottom: "cccp5"
  top: "poolcp5"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "cccp6"
  type: "Convolution"
  bottom: "poolcp5"
  top: "cccp6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp6"
  type: "ReLU"
  bottom: "cccp6"
  top: "cccp6"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "cccp6"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1101 19:20:26.105937  2200 layer_factory.hpp:77] Creating layer mnist
I1101 19:20:26.105937  2200 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1101 19:20:26.106938  2200 net.cpp:91] Creating Layer mnist
I1101 19:20:26.106938  2200 net.cpp:399] mnist -> data
I1101 19:20:26.106938  2200 net.cpp:399] mnist -> label
I1101 19:20:26.107939 11884 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1101 19:20:26.109940 11884 db_lmdb.cpp:52] Opened lmdb examples/mnist/mnist_train_lmdb_norm2
I1101 19:20:26.132455  2200 data_layer.cpp:41] output data size: 100,1,28,28
I1101 19:20:26.134812  2200 net.cpp:141] Setting up mnist
I1101 19:20:26.134812  2200 net.cpp:148] Top shape: 100 1 28 28 (78400)
I1101 19:20:26.134812  2200 net.cpp:148] Top shape: 100 (100)
I1101 19:20:26.134812  2200 net.cpp:156] Memory required for data: 314000
I1101 19:20:26.134812  2200 layer_factory.hpp:77] Creating layer conv1
I1101 19:20:26.134812  2200 net.cpp:91] Creating Layer conv1
I1101 19:20:26.134812  2200 net.cpp:425] conv1 <- data
I1101 19:20:26.134812  2200 net.cpp:399] conv1 -> conv1
I1101 19:20:26.135813 12404 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1101 19:20:26.370965  2200 net.cpp:141] Setting up conv1
I1101 19:20:26.370965  2200 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 19:20:26.370965  2200 net.cpp:156] Memory required for data: 20384400
I1101 19:20:26.370965  2200 layer_factory.hpp:77] Creating layer bn1
I1101 19:20:26.370965  2200 net.cpp:91] Creating Layer bn1
I1101 19:20:26.370965  2200 net.cpp:425] bn1 <- conv1
I1101 19:20:26.370965  2200 net.cpp:399] bn1 -> bn1
I1101 19:20:26.370965  2200 net.cpp:141] Setting up bn1
I1101 19:20:26.370965  2200 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 19:20:26.371465  2200 net.cpp:156] Memory required for data: 40454800
I1101 19:20:26.371465  2200 layer_factory.hpp:77] Creating layer scale1
I1101 19:20:26.371465  2200 net.cpp:91] Creating Layer scale1
I1101 19:20:26.371465  2200 net.cpp:425] scale1 <- bn1
I1101 19:20:26.371465  2200 net.cpp:399] scale1 -> scale1
I1101 19:20:26.371465  2200 layer_factory.hpp:77] Creating layer scale1
I1101 19:20:26.371465  2200 net.cpp:141] Setting up scale1
I1101 19:20:26.371465  2200 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 19:20:26.371465  2200 net.cpp:156] Memory required for data: 60525200
I1101 19:20:26.371465  2200 layer_factory.hpp:77] Creating layer relu1
I1101 19:20:26.371465  2200 net.cpp:91] Creating Layer relu1
I1101 19:20:26.371465  2200 net.cpp:425] relu1 <- scale1
I1101 19:20:26.371465  2200 net.cpp:399] relu1 -> relu1
I1101 19:20:26.371966  2200 net.cpp:141] Setting up relu1
I1101 19:20:26.371966  2200 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 19:20:26.371966  2200 net.cpp:156] Memory required for data: 80595600
I1101 19:20:26.371966  2200 layer_factory.hpp:77] Creating layer conv1_0
I1101 19:20:26.372467  2200 net.cpp:91] Creating Layer conv1_0
I1101 19:20:26.372467  2200 net.cpp:425] conv1_0 <- relu1
I1101 19:20:26.372467  2200 net.cpp:399] conv1_0 -> conv1_0
I1101 19:20:26.374467  2200 net.cpp:141] Setting up conv1_0
I1101 19:20:26.374969  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.374969  2200 net.cpp:156] Memory required for data: 120736400
I1101 19:20:26.374969  2200 layer_factory.hpp:77] Creating layer bn1_0
I1101 19:20:26.374969  2200 net.cpp:91] Creating Layer bn1_0
I1101 19:20:26.374969  2200 net.cpp:425] bn1_0 <- conv1_0
I1101 19:20:26.374969  2200 net.cpp:399] bn1_0 -> bn1_0
I1101 19:20:26.374969  2200 net.cpp:141] Setting up bn1_0
I1101 19:20:26.374969  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.374969  2200 net.cpp:156] Memory required for data: 160877200
I1101 19:20:26.374969  2200 layer_factory.hpp:77] Creating layer scale1_0
I1101 19:20:26.374969  2200 net.cpp:91] Creating Layer scale1_0
I1101 19:20:26.374969  2200 net.cpp:425] scale1_0 <- bn1_0
I1101 19:20:26.374969  2200 net.cpp:399] scale1_0 -> scale1_0
I1101 19:20:26.374969  2200 layer_factory.hpp:77] Creating layer scale1_0
I1101 19:20:26.374969  2200 net.cpp:141] Setting up scale1_0
I1101 19:20:26.374969  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.375468  2200 net.cpp:156] Memory required for data: 201018000
I1101 19:20:26.375468  2200 layer_factory.hpp:77] Creating layer relu1_0
I1101 19:20:26.375468  2200 net.cpp:91] Creating Layer relu1_0
I1101 19:20:26.375468  2200 net.cpp:425] relu1_0 <- scale1_0
I1101 19:20:26.375468  2200 net.cpp:399] relu1_0 -> relu1_0
I1101 19:20:26.375845  2200 net.cpp:141] Setting up relu1_0
I1101 19:20:26.375845  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.375845  2200 net.cpp:156] Memory required for data: 241158800
I1101 19:20:26.375845  2200 layer_factory.hpp:77] Creating layer conv2
I1101 19:20:26.375845  2200 net.cpp:91] Creating Layer conv2
I1101 19:20:26.375845  2200 net.cpp:425] conv2 <- relu1_0
I1101 19:20:26.375845  2200 net.cpp:399] conv2 -> conv2
I1101 19:20:26.379195  2200 net.cpp:141] Setting up conv2
I1101 19:20:26.379195  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.379195  2200 net.cpp:156] Memory required for data: 281299600
I1101 19:20:26.379195  2200 layer_factory.hpp:77] Creating layer bn2
I1101 19:20:26.379195  2200 net.cpp:91] Creating Layer bn2
I1101 19:20:26.379195  2200 net.cpp:425] bn2 <- conv2
I1101 19:20:26.379195  2200 net.cpp:399] bn2 -> bn2
I1101 19:20:26.379649  2200 net.cpp:141] Setting up bn2
I1101 19:20:26.379649  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.379649  2200 net.cpp:156] Memory required for data: 321440400
I1101 19:20:26.379649  2200 layer_factory.hpp:77] Creating layer scale2
I1101 19:20:26.379649  2200 net.cpp:91] Creating Layer scale2
I1101 19:20:26.379649  2200 net.cpp:425] scale2 <- bn2
I1101 19:20:26.379649  2200 net.cpp:399] scale2 -> scale2
I1101 19:20:26.379649  2200 layer_factory.hpp:77] Creating layer scale2
I1101 19:20:26.379649  2200 net.cpp:141] Setting up scale2
I1101 19:20:26.379649  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.379649  2200 net.cpp:156] Memory required for data: 361581200
I1101 19:20:26.379649  2200 layer_factory.hpp:77] Creating layer relu2
I1101 19:20:26.379649  2200 net.cpp:91] Creating Layer relu2
I1101 19:20:26.379649  2200 net.cpp:425] relu2 <- scale2
I1101 19:20:26.379649  2200 net.cpp:399] relu2 -> relu2
I1101 19:20:26.380151  2200 net.cpp:141] Setting up relu2
I1101 19:20:26.380151  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.380151  2200 net.cpp:156] Memory required for data: 401722000
I1101 19:20:26.380151  2200 layer_factory.hpp:77] Creating layer conv2_1
I1101 19:20:26.380151  2200 net.cpp:91] Creating Layer conv2_1
I1101 19:20:26.380151  2200 net.cpp:425] conv2_1 <- relu2
I1101 19:20:26.380151  2200 net.cpp:399] conv2_1 -> conv2_1
I1101 19:20:26.383654  2200 net.cpp:141] Setting up conv2_1
I1101 19:20:26.383654  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.383654  2200 net.cpp:156] Memory required for data: 441862800
I1101 19:20:26.383654  2200 layer_factory.hpp:77] Creating layer bn2_1
I1101 19:20:26.384155  2200 net.cpp:91] Creating Layer bn2_1
I1101 19:20:26.384155  2200 net.cpp:425] bn2_1 <- conv2_1
I1101 19:20:26.384155  2200 net.cpp:399] bn2_1 -> bn2_1
I1101 19:20:26.384155  2200 net.cpp:141] Setting up bn2_1
I1101 19:20:26.385155  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.385155  2200 net.cpp:156] Memory required for data: 482003600
I1101 19:20:26.385155  2200 layer_factory.hpp:77] Creating layer scale2_1
I1101 19:20:26.385155  2200 net.cpp:91] Creating Layer scale2_1
I1101 19:20:26.385155  2200 net.cpp:425] scale2_1 <- bn2_1
I1101 19:20:26.385155  2200 net.cpp:399] scale2_1 -> scale2_1
I1101 19:20:26.385155  2200 layer_factory.hpp:77] Creating layer scale2_1
I1101 19:20:26.385155  2200 net.cpp:141] Setting up scale2_1
I1101 19:20:26.385155  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.385155  2200 net.cpp:156] Memory required for data: 522144400
I1101 19:20:26.385155  2200 layer_factory.hpp:77] Creating layer relu2_1
I1101 19:20:26.385155  2200 net.cpp:91] Creating Layer relu2_1
I1101 19:20:26.385155  2200 net.cpp:425] relu2_1 <- scale2_1
I1101 19:20:26.385155  2200 net.cpp:399] relu2_1 -> relu2_1
I1101 19:20:26.386155  2200 net.cpp:141] Setting up relu2_1
I1101 19:20:26.386155  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.386155  2200 net.cpp:156] Memory required for data: 562285200
I1101 19:20:26.386155  2200 layer_factory.hpp:77] Creating layer pool2_1
I1101 19:20:26.386155  2200 net.cpp:91] Creating Layer pool2_1
I1101 19:20:26.386155  2200 net.cpp:425] pool2_1 <- relu2_1
I1101 19:20:26.386155  2200 net.cpp:399] pool2_1 -> pool2_1
I1101 19:20:26.386155  2200 net.cpp:141] Setting up pool2_1
I1101 19:20:26.386155  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.386155  2200 net.cpp:156] Memory required for data: 572320400
I1101 19:20:26.386155  2200 layer_factory.hpp:77] Creating layer conv2_2
I1101 19:20:26.386155  2200 net.cpp:91] Creating Layer conv2_2
I1101 19:20:26.386155  2200 net.cpp:425] conv2_2 <- pool2_1
I1101 19:20:26.386155  2200 net.cpp:399] conv2_2 -> conv2_2
I1101 19:20:26.389921  2200 net.cpp:141] Setting up conv2_2
I1101 19:20:26.389921  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.389921  2200 net.cpp:156] Memory required for data: 582355600
I1101 19:20:26.389921  2200 layer_factory.hpp:77] Creating layer bn2_2
I1101 19:20:26.389921  2200 net.cpp:91] Creating Layer bn2_2
I1101 19:20:26.389921  2200 net.cpp:425] bn2_2 <- conv2_2
I1101 19:20:26.389921  2200 net.cpp:399] bn2_2 -> bn2_2
I1101 19:20:26.389921  2200 net.cpp:141] Setting up bn2_2
I1101 19:20:26.389921  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.389921  2200 net.cpp:156] Memory required for data: 592390800
I1101 19:20:26.389921  2200 layer_factory.hpp:77] Creating layer scale2_2
I1101 19:20:26.389921  2200 net.cpp:91] Creating Layer scale2_2
I1101 19:20:26.389921  2200 net.cpp:425] scale2_2 <- bn2_2
I1101 19:20:26.389921  2200 net.cpp:399] scale2_2 -> scale2_2
I1101 19:20:26.389921  2200 layer_factory.hpp:77] Creating layer scale2_2
I1101 19:20:26.389921  2200 net.cpp:141] Setting up scale2_2
I1101 19:20:26.389921  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.390421  2200 net.cpp:156] Memory required for data: 602426000
I1101 19:20:26.390421  2200 layer_factory.hpp:77] Creating layer relu2_2
I1101 19:20:26.390421  2200 net.cpp:91] Creating Layer relu2_2
I1101 19:20:26.390421  2200 net.cpp:425] relu2_2 <- scale2_2
I1101 19:20:26.390421  2200 net.cpp:399] relu2_2 -> relu2_2
I1101 19:20:26.390805  2200 net.cpp:141] Setting up relu2_2
I1101 19:20:26.390805  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.390805  2200 net.cpp:156] Memory required for data: 612461200
I1101 19:20:26.390805  2200 layer_factory.hpp:77] Creating layer conv3
I1101 19:20:26.390805  2200 net.cpp:91] Creating Layer conv3
I1101 19:20:26.390805  2200 net.cpp:425] conv3 <- relu2_2
I1101 19:20:26.390805  2200 net.cpp:399] conv3 -> conv3
I1101 19:20:26.393808  2200 net.cpp:141] Setting up conv3
I1101 19:20:26.393808  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.393808  2200 net.cpp:156] Memory required for data: 622496400
I1101 19:20:26.393808  2200 layer_factory.hpp:77] Creating layer bn3
I1101 19:20:26.394320  2200 net.cpp:91] Creating Layer bn3
I1101 19:20:26.394320  2200 net.cpp:425] bn3 <- conv3
I1101 19:20:26.394320  2200 net.cpp:399] bn3 -> bn3
I1101 19:20:26.394320  2200 net.cpp:141] Setting up bn3
I1101 19:20:26.394320  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.394320  2200 net.cpp:156] Memory required for data: 632531600
I1101 19:20:26.394320  2200 layer_factory.hpp:77] Creating layer scale3
I1101 19:20:26.394320  2200 net.cpp:91] Creating Layer scale3
I1101 19:20:26.394320  2200 net.cpp:425] scale3 <- bn3
I1101 19:20:26.394320  2200 net.cpp:399] scale3 -> scale3
I1101 19:20:26.394320  2200 layer_factory.hpp:77] Creating layer scale3
I1101 19:20:26.394320  2200 net.cpp:141] Setting up scale3
I1101 19:20:26.394320  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.394320  2200 net.cpp:156] Memory required for data: 642566800
I1101 19:20:26.394320  2200 layer_factory.hpp:77] Creating layer relu3
I1101 19:20:26.394320  2200 net.cpp:91] Creating Layer relu3
I1101 19:20:26.394320  2200 net.cpp:425] relu3 <- scale3
I1101 19:20:26.394320  2200 net.cpp:399] relu3 -> relu3
I1101 19:20:26.394819  2200 net.cpp:141] Setting up relu3
I1101 19:20:26.394819  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.394819  2200 net.cpp:156] Memory required for data: 652602000
I1101 19:20:26.394819  2200 layer_factory.hpp:77] Creating layer conv4
I1101 19:20:26.394819  2200 net.cpp:91] Creating Layer conv4
I1101 19:20:26.394819  2200 net.cpp:425] conv4 <- relu3
I1101 19:20:26.394819  2200 net.cpp:399] conv4 -> conv4
I1101 19:20:26.398813  2200 net.cpp:141] Setting up conv4
I1101 19:20:26.398813  2200 net.cpp:148] Top shape: 100 256 14 14 (5017600)
I1101 19:20:26.398813  2200 net.cpp:156] Memory required for data: 672672400
I1101 19:20:26.398813  2200 layer_factory.hpp:77] Creating layer pool4
I1101 19:20:26.398813  2200 net.cpp:91] Creating Layer pool4
I1101 19:20:26.398813  2200 net.cpp:425] pool4 <- conv4
I1101 19:20:26.398813  2200 net.cpp:399] pool4 -> pool4
I1101 19:20:26.398813  2200 net.cpp:141] Setting up pool4
I1101 19:20:26.398813  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.398813  2200 net.cpp:156] Memory required for data: 677690000
I1101 19:20:26.398813  2200 layer_factory.hpp:77] Creating layer bn4
I1101 19:20:26.398813  2200 net.cpp:91] Creating Layer bn4
I1101 19:20:26.398813  2200 net.cpp:425] bn4 <- pool4
I1101 19:20:26.398813  2200 net.cpp:399] bn4 -> bn4
I1101 19:20:26.399312  2200 net.cpp:141] Setting up bn4
I1101 19:20:26.399312  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.399312  2200 net.cpp:156] Memory required for data: 682707600
I1101 19:20:26.399312  2200 layer_factory.hpp:77] Creating layer scale4
I1101 19:20:26.399312  2200 net.cpp:91] Creating Layer scale4
I1101 19:20:26.399312  2200 net.cpp:425] scale4 <- bn4
I1101 19:20:26.399312  2200 net.cpp:399] scale4 -> scale4
I1101 19:20:26.399312  2200 layer_factory.hpp:77] Creating layer scale4
I1101 19:20:26.399312  2200 net.cpp:141] Setting up scale4
I1101 19:20:26.399312  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.399312  2200 net.cpp:156] Memory required for data: 687725200
I1101 19:20:26.399312  2200 layer_factory.hpp:77] Creating layer relu4
I1101 19:20:26.399312  2200 net.cpp:91] Creating Layer relu4
I1101 19:20:26.399312  2200 net.cpp:425] relu4 <- scale4
I1101 19:20:26.399312  2200 net.cpp:399] relu4 -> relu4
I1101 19:20:26.400113  2200 net.cpp:141] Setting up relu4
I1101 19:20:26.400113  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.400113  2200 net.cpp:156] Memory required for data: 692742800
I1101 19:20:26.400113  2200 layer_factory.hpp:77] Creating layer conv4_1
I1101 19:20:26.400113  2200 net.cpp:91] Creating Layer conv4_1
I1101 19:20:26.400615  2200 net.cpp:425] conv4_1 <- relu4
I1101 19:20:26.400615  2200 net.cpp:399] conv4_1 -> conv4_1
I1101 19:20:26.407662  2200 net.cpp:141] Setting up conv4_1
I1101 19:20:26.407662  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.407662  2200 net.cpp:156] Memory required for data: 697760400
I1101 19:20:26.407662  2200 layer_factory.hpp:77] Creating layer bn4_1
I1101 19:20:26.407662  2200 net.cpp:91] Creating Layer bn4_1
I1101 19:20:26.407662  2200 net.cpp:425] bn4_1 <- conv4_1
I1101 19:20:26.407662  2200 net.cpp:399] bn4_1 -> bn4_1
I1101 19:20:26.407662  2200 net.cpp:141] Setting up bn4_1
I1101 19:20:26.407662  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.407662  2200 net.cpp:156] Memory required for data: 702778000
I1101 19:20:26.407662  2200 layer_factory.hpp:77] Creating layer scale4_1
I1101 19:20:26.407662  2200 net.cpp:91] Creating Layer scale4_1
I1101 19:20:26.407662  2200 net.cpp:425] scale4_1 <- bn4_1
I1101 19:20:26.407662  2200 net.cpp:399] scale4_1 -> scale4_1
I1101 19:20:26.408164  2200 layer_factory.hpp:77] Creating layer scale4_1
I1101 19:20:26.408164  2200 net.cpp:141] Setting up scale4_1
I1101 19:20:26.408164  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.408164  2200 net.cpp:156] Memory required for data: 707795600
I1101 19:20:26.408164  2200 layer_factory.hpp:77] Creating layer relu4_1
I1101 19:20:26.408164  2200 net.cpp:91] Creating Layer relu4_1
I1101 19:20:26.408164  2200 net.cpp:425] relu4_1 <- scale4_1
I1101 19:20:26.408164  2200 net.cpp:399] relu4_1 -> relu4_1
I1101 19:20:26.408634  2200 net.cpp:141] Setting up relu4_1
I1101 19:20:26.408634  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.408634  2200 net.cpp:156] Memory required for data: 712813200
I1101 19:20:26.408634  2200 layer_factory.hpp:77] Creating layer conv4_2
I1101 19:20:26.408634  2200 net.cpp:91] Creating Layer conv4_2
I1101 19:20:26.408634  2200 net.cpp:425] conv4_2 <- relu4_1
I1101 19:20:26.408634  2200 net.cpp:399] conv4_2 -> conv4_2
I1101 19:20:26.415343  2200 net.cpp:141] Setting up conv4_2
I1101 19:20:26.415343  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.415343  2200 net.cpp:156] Memory required for data: 717830800
I1101 19:20:26.415343  2200 layer_factory.hpp:77] Creating layer bn4_2
I1101 19:20:26.415343  2200 net.cpp:91] Creating Layer bn4_2
I1101 19:20:26.415343  2200 net.cpp:425] bn4_2 <- conv4_2
I1101 19:20:26.415343  2200 net.cpp:399] bn4_2 -> bn4_2
I1101 19:20:26.415845  2200 net.cpp:141] Setting up bn4_2
I1101 19:20:26.415845  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.415845  2200 net.cpp:156] Memory required for data: 722848400
I1101 19:20:26.415845  2200 layer_factory.hpp:77] Creating layer scale4_2
I1101 19:20:26.415845  2200 net.cpp:91] Creating Layer scale4_2
I1101 19:20:26.415845  2200 net.cpp:425] scale4_2 <- bn4_2
I1101 19:20:26.415845  2200 net.cpp:399] scale4_2 -> scale4_2
I1101 19:20:26.415845  2200 layer_factory.hpp:77] Creating layer scale4_2
I1101 19:20:26.415845  2200 net.cpp:141] Setting up scale4_2
I1101 19:20:26.415845  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.415845  2200 net.cpp:156] Memory required for data: 727866000
I1101 19:20:26.415845  2200 layer_factory.hpp:77] Creating layer relu4_2
I1101 19:20:26.415845  2200 net.cpp:91] Creating Layer relu4_2
I1101 19:20:26.415845  2200 net.cpp:425] relu4_2 <- scale4_2
I1101 19:20:26.415845  2200 net.cpp:399] relu4_2 -> relu4_2
I1101 19:20:26.416345  2200 net.cpp:141] Setting up relu4_2
I1101 19:20:26.416345  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.416345  2200 net.cpp:156] Memory required for data: 732883600
I1101 19:20:26.416345  2200 layer_factory.hpp:77] Creating layer pool4_2
I1101 19:20:26.416345  2200 net.cpp:91] Creating Layer pool4_2
I1101 19:20:26.416345  2200 net.cpp:425] pool4_2 <- relu4_2
I1101 19:20:26.416345  2200 net.cpp:399] pool4_2 -> pool4_2
I1101 19:20:26.416345  2200 net.cpp:141] Setting up pool4_2
I1101 19:20:26.416345  2200 net.cpp:148] Top shape: 100 256 4 4 (409600)
I1101 19:20:26.416345  2200 net.cpp:156] Memory required for data: 734522000
I1101 19:20:26.416345  2200 layer_factory.hpp:77] Creating layer conv4_0
I1101 19:20:26.416345  2200 net.cpp:91] Creating Layer conv4_0
I1101 19:20:26.416345  2200 net.cpp:425] conv4_0 <- pool4_2
I1101 19:20:26.416345  2200 net.cpp:399] conv4_0 -> conv4_0
I1101 19:20:26.429354  2200 net.cpp:141] Setting up conv4_0
I1101 19:20:26.429354  2200 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 19:20:26.429354  2200 net.cpp:156] Memory required for data: 737798800
I1101 19:20:26.429354  2200 layer_factory.hpp:77] Creating layer bn4_0
I1101 19:20:26.429354  2200 net.cpp:91] Creating Layer bn4_0
I1101 19:20:26.429354  2200 net.cpp:425] bn4_0 <- conv4_0
I1101 19:20:26.429354  2200 net.cpp:399] bn4_0 -> bn4_0
I1101 19:20:26.429855  2200 net.cpp:141] Setting up bn4_0
I1101 19:20:26.429855  2200 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 19:20:26.429855  2200 net.cpp:156] Memory required for data: 741075600
I1101 19:20:26.429855  2200 layer_factory.hpp:77] Creating layer scale4_0
I1101 19:20:26.429855  2200 net.cpp:91] Creating Layer scale4_0
I1101 19:20:26.429855  2200 net.cpp:425] scale4_0 <- bn4_0
I1101 19:20:26.429855  2200 net.cpp:399] scale4_0 -> scale4_0
I1101 19:20:26.429855  2200 layer_factory.hpp:77] Creating layer scale4_0
I1101 19:20:26.429855  2200 net.cpp:141] Setting up scale4_0
I1101 19:20:26.429855  2200 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 19:20:26.429855  2200 net.cpp:156] Memory required for data: 744352400
I1101 19:20:26.429855  2200 layer_factory.hpp:77] Creating layer relu4_0
I1101 19:20:26.429855  2200 net.cpp:91] Creating Layer relu4_0
I1101 19:20:26.429855  2200 net.cpp:425] relu4_0 <- scale4_0
I1101 19:20:26.429855  2200 net.cpp:399] relu4_0 -> relu4_0
I1101 19:20:26.430609  2200 net.cpp:141] Setting up relu4_0
I1101 19:20:26.430609  2200 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 19:20:26.430609  2200 net.cpp:156] Memory required for data: 747629200
I1101 19:20:26.430609  2200 layer_factory.hpp:77] Creating layer cccp4
I1101 19:20:26.430609  2200 net.cpp:91] Creating Layer cccp4
I1101 19:20:26.430609  2200 net.cpp:425] cccp4 <- relu4_0
I1101 19:20:26.430609  2200 net.cpp:399] cccp4 -> cccp4
I1101 19:20:26.441618  2200 net.cpp:141] Setting up cccp4
I1101 19:20:26.441618  2200 net.cpp:148] Top shape: 100 2048 4 4 (3276800)
I1101 19:20:26.441618  2200 net.cpp:156] Memory required for data: 760736400
I1101 19:20:26.441618  2200 layer_factory.hpp:77] Creating layer relu_cccp4
I1101 19:20:26.441618  2200 net.cpp:91] Creating Layer relu_cccp4
I1101 19:20:26.441618  2200 net.cpp:425] relu_cccp4 <- cccp4
I1101 19:20:26.441618  2200 net.cpp:386] relu_cccp4 -> cccp4 (in-place)
I1101 19:20:26.442119  2200 net.cpp:141] Setting up relu_cccp4
I1101 19:20:26.442119  2200 net.cpp:148] Top shape: 100 2048 4 4 (3276800)
I1101 19:20:26.442119  2200 net.cpp:156] Memory required for data: 773843600
I1101 19:20:26.442119  2200 layer_factory.hpp:77] Creating layer cccp5
I1101 19:20:26.442119  2200 net.cpp:91] Creating Layer cccp5
I1101 19:20:26.442119  2200 net.cpp:425] cccp5 <- cccp4
I1101 19:20:26.442119  2200 net.cpp:399] cccp5 -> cccp5
I1101 19:20:26.448334  2200 net.cpp:141] Setting up cccp5
I1101 19:20:26.448334  2200 net.cpp:148] Top shape: 100 256 4 4 (409600)
I1101 19:20:26.448334  2200 net.cpp:156] Memory required for data: 775482000
I1101 19:20:26.448334  2200 layer_factory.hpp:77] Creating layer relu_cccp5
I1101 19:20:26.448834  2200 net.cpp:91] Creating Layer relu_cccp5
I1101 19:20:26.448834  2200 net.cpp:425] relu_cccp5 <- cccp5
I1101 19:20:26.448834  2200 net.cpp:386] relu_cccp5 -> cccp5 (in-place)
I1101 19:20:26.448834  2200 net.cpp:141] Setting up relu_cccp5
I1101 19:20:26.448834  2200 net.cpp:148] Top shape: 100 256 4 4 (409600)
I1101 19:20:26.448834  2200 net.cpp:156] Memory required for data: 777120400
I1101 19:20:26.448834  2200 layer_factory.hpp:77] Creating layer poolcp5
I1101 19:20:26.448834  2200 net.cpp:91] Creating Layer poolcp5
I1101 19:20:26.448834  2200 net.cpp:425] poolcp5 <- cccp5
I1101 19:20:26.448834  2200 net.cpp:399] poolcp5 -> poolcp5
I1101 19:20:26.448834  2200 net.cpp:141] Setting up poolcp5
I1101 19:20:26.448834  2200 net.cpp:148] Top shape: 100 256 2 2 (102400)
I1101 19:20:26.448834  2200 net.cpp:156] Memory required for data: 777530000
I1101 19:20:26.448834  2200 layer_factory.hpp:77] Creating layer cccp6
I1101 19:20:26.449334  2200 net.cpp:91] Creating Layer cccp6
I1101 19:20:26.449334  2200 net.cpp:425] cccp6 <- poolcp5
I1101 19:20:26.449334  2200 net.cpp:399] cccp6 -> cccp6
I1101 19:20:26.456341  2200 net.cpp:141] Setting up cccp6
I1101 19:20:26.456341  2200 net.cpp:148] Top shape: 100 256 2 2 (102400)
I1101 19:20:26.456341  2200 net.cpp:156] Memory required for data: 777939600
I1101 19:20:26.456341  2200 layer_factory.hpp:77] Creating layer relu_cccp6
I1101 19:20:26.456341  2200 net.cpp:91] Creating Layer relu_cccp6
I1101 19:20:26.456341  2200 net.cpp:425] relu_cccp6 <- cccp6
I1101 19:20:26.456341  2200 net.cpp:386] relu_cccp6 -> cccp6 (in-place)
I1101 19:20:26.457116  2200 net.cpp:141] Setting up relu_cccp6
I1101 19:20:26.457116  2200 net.cpp:148] Top shape: 100 256 2 2 (102400)
I1101 19:20:26.457116  2200 net.cpp:156] Memory required for data: 778349200
I1101 19:20:26.457116  2200 layer_factory.hpp:77] Creating layer poolcp6
I1101 19:20:26.457116  2200 net.cpp:91] Creating Layer poolcp6
I1101 19:20:26.457116  2200 net.cpp:425] poolcp6 <- cccp6
I1101 19:20:26.457116  2200 net.cpp:399] poolcp6 -> poolcp6
I1101 19:20:26.457116  2200 net.cpp:141] Setting up poolcp6
I1101 19:20:26.457116  2200 net.cpp:148] Top shape: 100 256 1 1 (25600)
I1101 19:20:26.457116  2200 net.cpp:156] Memory required for data: 778451600
I1101 19:20:26.457116  2200 layer_factory.hpp:77] Creating layer ip1
I1101 19:20:26.457116  2200 net.cpp:91] Creating Layer ip1
I1101 19:20:26.457116  2200 net.cpp:425] ip1 <- poolcp6
I1101 19:20:26.457116  2200 net.cpp:399] ip1 -> ip1
I1101 19:20:26.457618  2200 net.cpp:141] Setting up ip1
I1101 19:20:26.457618  2200 net.cpp:148] Top shape: 100 10 (1000)
I1101 19:20:26.457618  2200 net.cpp:156] Memory required for data: 778455600
I1101 19:20:26.457618  2200 layer_factory.hpp:77] Creating layer loss
I1101 19:20:26.457618  2200 net.cpp:91] Creating Layer loss
I1101 19:20:26.457618  2200 net.cpp:425] loss <- ip1
I1101 19:20:26.457618  2200 net.cpp:425] loss <- label
I1101 19:20:26.457618  2200 net.cpp:399] loss -> loss
I1101 19:20:26.457618  2200 layer_factory.hpp:77] Creating layer loss
I1101 19:20:26.457618  2200 net.cpp:141] Setting up loss
I1101 19:20:26.457618  2200 net.cpp:148] Top shape: (1)
I1101 19:20:26.457618  2200 net.cpp:151]     with loss weight 1
I1101 19:20:26.457618  2200 net.cpp:156] Memory required for data: 778455604
I1101 19:20:26.457618  2200 net.cpp:217] loss needs backward computation.
I1101 19:20:26.457618  2200 net.cpp:217] ip1 needs backward computation.
I1101 19:20:26.457618  2200 net.cpp:217] poolcp6 needs backward computation.
I1101 19:20:26.457618  2200 net.cpp:217] relu_cccp6 needs backward computation.
I1101 19:20:26.457618  2200 net.cpp:217] cccp6 needs backward computation.
I1101 19:20:26.457618  2200 net.cpp:217] poolcp5 needs backward computation.
I1101 19:20:26.457618  2200 net.cpp:217] relu_cccp5 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] cccp5 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] relu_cccp4 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] cccp4 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] relu4_0 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] scale4_0 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] bn4_0 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] conv4_0 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] pool4_2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] relu4_2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] scale4_2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] bn4_2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] conv4_2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] relu4_1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] scale4_1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] bn4_1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] conv4_1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] relu4 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] scale4 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] bn4 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] pool4 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] conv4 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] relu3 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] scale3 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] bn3 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] conv3 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] relu2_2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] scale2_2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] bn2_2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] conv2_2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] pool2_1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] relu2_1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] scale2_1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] bn2_1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] conv2_1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] relu2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] scale2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] bn2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] conv2 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] relu1_0 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] scale1_0 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] bn1_0 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] conv1_0 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] relu1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] scale1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] bn1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:217] conv1 needs backward computation.
I1101 19:20:26.458118  2200 net.cpp:219] mnist does not need backward computation.
I1101 19:20:26.458118  2200 net.cpp:261] This network produces output loss
I1101 19:20:26.458118  2200 net.cpp:274] Network initialization done.
I1101 19:20:26.459120  2200 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/simpnet_nodrp_train_test.prototxt
I1101 19:20:26.459120  2200 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1101 19:20:26.459120  2200 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1101 19:20:26.459120  2200 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1101 19:20:26.459120  2200 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1101 19:20:26.459120  2200 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1101 19:20:26.459619  2200 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1101 19:20:26.459619  2200 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1101 19:20:26.459619  2200 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1101 19:20:26.459619  2200 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1101 19:20:26.459619  2200 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1101 19:20:26.459619  2200 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1101 19:20:26.459619  2200 net.cpp:49] Initializing net from parameters: 
name: "SimpNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb_norm2"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "bn1"
  top: "scale1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "relu1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "bn1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "bn1_0"
  top: "scale1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "scale1_0"
  top: "relu1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "bn2"
  top: "scale2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "relu2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "bn2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "bn2_1"
  top: "scale2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "scale2_1"
  top: "relu2_1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "relu2_1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "bn2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "bn2_2"
  top: "scale2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "scale2_2"
  top: "relu2_2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "relu2_2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "bn3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "bn3"
  top: "scale3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "pool4"
  top: "bn4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "bn4"
  top: "scale4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "relu4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "bn4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "bn4_1"
  top: "scale4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "scale4_1"
  top: "relu4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "relu4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "bn4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "bn4_2"
  top: "scale4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "scale4_2"
  top: "relu4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "relu4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "bn4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "bn4_0"
  top: "scale4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "scale4_0"
  top: "relu4_0"
}
layer {
  name: "cccp4"
  type: "Convolution"
  bottom: "relu4_0"
  top: "cccp4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2048
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp4"
  type: "ReLU"
  bottom: "cccp4"
  top: "cccp4"
}
layer {
  name: "cccp5"
  type: "Convolution"
  bottom: "cccp4"
  top: "cccp5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp5"
  type: "ReLU"
  bottom: "cccp5"
  top: "cccp5"
}
layer {
  name: "poolcp5"
  type: "Pooling"
  bottom: "cccp5"
  top: "poolcp5"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "cccp6"
  type: "Convolution"
  bottom: "poolcp5"
  top: "cccp6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp6"
  type: "ReLU"
  bottom: "cccp6"
  top: "cccp6"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "cccp6"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1101 19:20:26.460120  2200 layer_factory.hpp:77] Creating layer mnist
I1101 19:20:26.461122  2200 net.cpp:91] Creating Layer mnist
I1101 19:20:26.461122  2200 net.cpp:399] mnist -> data
I1101 19:20:26.461122  2200 net.cpp:399] mnist -> label
I1101 19:20:26.462121 12104 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1101 19:20:26.469126 12104 db_lmdb.cpp:52] Opened lmdb examples/mnist/mnist_test_lmdb_norm2
I1101 19:20:26.469627  2200 data_layer.cpp:41] output data size: 100,1,28,28
I1101 19:20:26.471128  2200 net.cpp:141] Setting up mnist
I1101 19:20:26.471128  2200 net.cpp:148] Top shape: 100 1 28 28 (78400)
I1101 19:20:26.471128  2200 net.cpp:148] Top shape: 100 (100)
I1101 19:20:26.471128  2200 net.cpp:156] Memory required for data: 314000
I1101 19:20:26.471128  2200 layer_factory.hpp:77] Creating layer label_mnist_1_split
I1101 19:20:26.471128  2200 net.cpp:91] Creating Layer label_mnist_1_split
I1101 19:20:26.471128  2200 net.cpp:425] label_mnist_1_split <- label
I1101 19:20:26.471128  2200 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0
I1101 19:20:26.471128  2200 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1
I1101 19:20:26.471628  2200 net.cpp:141] Setting up label_mnist_1_split
I1101 19:20:26.471628  2200 net.cpp:148] Top shape: 100 (100)
I1101 19:20:26.471628  2200 net.cpp:148] Top shape: 100 (100)
I1101 19:20:26.471628  2200 net.cpp:156] Memory required for data: 314800
I1101 19:20:26.471628  2200 layer_factory.hpp:77] Creating layer conv1
I1101 19:20:26.471628  2200 net.cpp:91] Creating Layer conv1
I1101 19:20:26.471628  2200 net.cpp:425] conv1 <- data
I1101 19:20:26.471628  2200 net.cpp:399] conv1 -> conv1
I1101 19:20:26.473629  2200 net.cpp:141] Setting up conv1
I1101 19:20:26.473629  2200 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 19:20:26.473629  2200 net.cpp:156] Memory required for data: 20385200
I1101 19:20:26.473629  2200 layer_factory.hpp:77] Creating layer bn1
I1101 19:20:26.473629  2200 net.cpp:91] Creating Layer bn1
I1101 19:20:26.473629  2200 net.cpp:425] bn1 <- conv1
I1101 19:20:26.473629 13284 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1101 19:20:26.473629  2200 net.cpp:399] bn1 -> bn1
I1101 19:20:26.474130  2200 net.cpp:141] Setting up bn1
I1101 19:20:26.474130  2200 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 19:20:26.474130  2200 net.cpp:156] Memory required for data: 40455600
I1101 19:20:26.474130  2200 layer_factory.hpp:77] Creating layer scale1
I1101 19:20:26.474130  2200 net.cpp:91] Creating Layer scale1
I1101 19:20:26.474130  2200 net.cpp:425] scale1 <- bn1
I1101 19:20:26.474130  2200 net.cpp:399] scale1 -> scale1
I1101 19:20:26.474130  2200 layer_factory.hpp:77] Creating layer scale1
I1101 19:20:26.474130  2200 net.cpp:141] Setting up scale1
I1101 19:20:26.474130  2200 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 19:20:26.474130  2200 net.cpp:156] Memory required for data: 60526000
I1101 19:20:26.474130  2200 layer_factory.hpp:77] Creating layer relu1
I1101 19:20:26.474130  2200 net.cpp:91] Creating Layer relu1
I1101 19:20:26.474130  2200 net.cpp:425] relu1 <- scale1
I1101 19:20:26.474130  2200 net.cpp:399] relu1 -> relu1
I1101 19:20:26.474630  2200 net.cpp:141] Setting up relu1
I1101 19:20:26.474630  2200 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 19:20:26.474630  2200 net.cpp:156] Memory required for data: 80596400
I1101 19:20:26.474630  2200 layer_factory.hpp:77] Creating layer conv1_0
I1101 19:20:26.474630  2200 net.cpp:91] Creating Layer conv1_0
I1101 19:20:26.474630  2200 net.cpp:425] conv1_0 <- relu1
I1101 19:20:26.474630  2200 net.cpp:399] conv1_0 -> conv1_0
I1101 19:20:26.477632  2200 net.cpp:141] Setting up conv1_0
I1101 19:20:26.477632  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.477632  2200 net.cpp:156] Memory required for data: 120737200
I1101 19:20:26.477632  2200 layer_factory.hpp:77] Creating layer bn1_0
I1101 19:20:26.477632  2200 net.cpp:91] Creating Layer bn1_0
I1101 19:20:26.477632  2200 net.cpp:425] bn1_0 <- conv1_0
I1101 19:20:26.477632  2200 net.cpp:399] bn1_0 -> bn1_0
I1101 19:20:26.478133  2200 net.cpp:141] Setting up bn1_0
I1101 19:20:26.478133  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.478133  2200 net.cpp:156] Memory required for data: 160878000
I1101 19:20:26.478133  2200 layer_factory.hpp:77] Creating layer scale1_0
I1101 19:20:26.478133  2200 net.cpp:91] Creating Layer scale1_0
I1101 19:20:26.478133  2200 net.cpp:425] scale1_0 <- bn1_0
I1101 19:20:26.478133  2200 net.cpp:399] scale1_0 -> scale1_0
I1101 19:20:26.478133  2200 layer_factory.hpp:77] Creating layer scale1_0
I1101 19:20:26.478633  2200 net.cpp:141] Setting up scale1_0
I1101 19:20:26.478633  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.478633  2200 net.cpp:156] Memory required for data: 201018800
I1101 19:20:26.478633  2200 layer_factory.hpp:77] Creating layer relu1_0
I1101 19:20:26.478633  2200 net.cpp:91] Creating Layer relu1_0
I1101 19:20:26.478633  2200 net.cpp:425] relu1_0 <- scale1_0
I1101 19:20:26.478633  2200 net.cpp:399] relu1_0 -> relu1_0
I1101 19:20:26.479133  2200 net.cpp:141] Setting up relu1_0
I1101 19:20:26.479133  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.479133  2200 net.cpp:156] Memory required for data: 241159600
I1101 19:20:26.479133  2200 layer_factory.hpp:77] Creating layer conv2
I1101 19:20:26.479133  2200 net.cpp:91] Creating Layer conv2
I1101 19:20:26.479133  2200 net.cpp:425] conv2 <- relu1_0
I1101 19:20:26.479133  2200 net.cpp:399] conv2 -> conv2
I1101 19:20:26.482636  2200 net.cpp:141] Setting up conv2
I1101 19:20:26.482636  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.482636  2200 net.cpp:156] Memory required for data: 281300400
I1101 19:20:26.482636  2200 layer_factory.hpp:77] Creating layer bn2
I1101 19:20:26.483137  2200 net.cpp:91] Creating Layer bn2
I1101 19:20:26.483137  2200 net.cpp:425] bn2 <- conv2
I1101 19:20:26.483137  2200 net.cpp:399] bn2 -> bn2
I1101 19:20:26.483137  2200 net.cpp:141] Setting up bn2
I1101 19:20:26.483137  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.483137  2200 net.cpp:156] Memory required for data: 321441200
I1101 19:20:26.483137  2200 layer_factory.hpp:77] Creating layer scale2
I1101 19:20:26.483137  2200 net.cpp:91] Creating Layer scale2
I1101 19:20:26.483137  2200 net.cpp:425] scale2 <- bn2
I1101 19:20:26.483137  2200 net.cpp:399] scale2 -> scale2
I1101 19:20:26.483137  2200 layer_factory.hpp:77] Creating layer scale2
I1101 19:20:26.483638  2200 net.cpp:141] Setting up scale2
I1101 19:20:26.483638  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.483638  2200 net.cpp:156] Memory required for data: 361582000
I1101 19:20:26.483638  2200 layer_factory.hpp:77] Creating layer relu2
I1101 19:20:26.483638  2200 net.cpp:91] Creating Layer relu2
I1101 19:20:26.483638  2200 net.cpp:425] relu2 <- scale2
I1101 19:20:26.483638  2200 net.cpp:399] relu2 -> relu2
I1101 19:20:26.483638  2200 net.cpp:141] Setting up relu2
I1101 19:20:26.483638  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.483638  2200 net.cpp:156] Memory required for data: 401722800
I1101 19:20:26.483638  2200 layer_factory.hpp:77] Creating layer conv2_1
I1101 19:20:26.483638  2200 net.cpp:91] Creating Layer conv2_1
I1101 19:20:26.483638  2200 net.cpp:425] conv2_1 <- relu2
I1101 19:20:26.483638  2200 net.cpp:399] conv2_1 -> conv2_1
I1101 19:20:26.487639  2200 net.cpp:141] Setting up conv2_1
I1101 19:20:26.488140  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.488140  2200 net.cpp:156] Memory required for data: 441863600
I1101 19:20:26.488140  2200 layer_factory.hpp:77] Creating layer bn2_1
I1101 19:20:26.488140  2200 net.cpp:91] Creating Layer bn2_1
I1101 19:20:26.488140  2200 net.cpp:425] bn2_1 <- conv2_1
I1101 19:20:26.488140  2200 net.cpp:399] bn2_1 -> bn2_1
I1101 19:20:26.488140  2200 net.cpp:141] Setting up bn2_1
I1101 19:20:26.488140  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.488140  2200 net.cpp:156] Memory required for data: 482004400
I1101 19:20:26.488140  2200 layer_factory.hpp:77] Creating layer scale2_1
I1101 19:20:26.488140  2200 net.cpp:91] Creating Layer scale2_1
I1101 19:20:26.488140  2200 net.cpp:425] scale2_1 <- bn2_1
I1101 19:20:26.488140  2200 net.cpp:399] scale2_1 -> scale2_1
I1101 19:20:26.488140  2200 layer_factory.hpp:77] Creating layer scale2_1
I1101 19:20:26.488641  2200 net.cpp:141] Setting up scale2_1
I1101 19:20:26.488641  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.488641  2200 net.cpp:156] Memory required for data: 522145200
I1101 19:20:26.488641  2200 layer_factory.hpp:77] Creating layer relu2_1
I1101 19:20:26.488641  2200 net.cpp:91] Creating Layer relu2_1
I1101 19:20:26.488641  2200 net.cpp:425] relu2_1 <- scale2_1
I1101 19:20:26.488641  2200 net.cpp:399] relu2_1 -> relu2_1
I1101 19:20:26.488641  2200 net.cpp:141] Setting up relu2_1
I1101 19:20:26.488641  2200 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 19:20:26.489140  2200 net.cpp:156] Memory required for data: 562286000
I1101 19:20:26.489140  2200 layer_factory.hpp:77] Creating layer pool2_1
I1101 19:20:26.489140  2200 net.cpp:91] Creating Layer pool2_1
I1101 19:20:26.489140  2200 net.cpp:425] pool2_1 <- relu2_1
I1101 19:20:26.489140  2200 net.cpp:399] pool2_1 -> pool2_1
I1101 19:20:26.489140  2200 net.cpp:141] Setting up pool2_1
I1101 19:20:26.489140  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.489140  2200 net.cpp:156] Memory required for data: 572321200
I1101 19:20:26.489140  2200 layer_factory.hpp:77] Creating layer conv2_2
I1101 19:20:26.489140  2200 net.cpp:91] Creating Layer conv2_2
I1101 19:20:26.489140  2200 net.cpp:425] conv2_2 <- pool2_1
I1101 19:20:26.489140  2200 net.cpp:399] conv2_2 -> conv2_2
I1101 19:20:26.492643  2200 net.cpp:141] Setting up conv2_2
I1101 19:20:26.492643  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.492643  2200 net.cpp:156] Memory required for data: 582356400
I1101 19:20:26.492643  2200 layer_factory.hpp:77] Creating layer bn2_2
I1101 19:20:26.493144  2200 net.cpp:91] Creating Layer bn2_2
I1101 19:20:26.493144  2200 net.cpp:425] bn2_2 <- conv2_2
I1101 19:20:26.493144  2200 net.cpp:399] bn2_2 -> bn2_2
I1101 19:20:26.493144  2200 net.cpp:141] Setting up bn2_2
I1101 19:20:26.493144  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.493144  2200 net.cpp:156] Memory required for data: 592391600
I1101 19:20:26.493144  2200 layer_factory.hpp:77] Creating layer scale2_2
I1101 19:20:26.493144  2200 net.cpp:91] Creating Layer scale2_2
I1101 19:20:26.493144  2200 net.cpp:425] scale2_2 <- bn2_2
I1101 19:20:26.493144  2200 net.cpp:399] scale2_2 -> scale2_2
I1101 19:20:26.493144  2200 layer_factory.hpp:77] Creating layer scale2_2
I1101 19:20:26.493144  2200 net.cpp:141] Setting up scale2_2
I1101 19:20:26.493144  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.493144  2200 net.cpp:156] Memory required for data: 602426800
I1101 19:20:26.493144  2200 layer_factory.hpp:77] Creating layer relu2_2
I1101 19:20:26.493144  2200 net.cpp:91] Creating Layer relu2_2
I1101 19:20:26.493144  2200 net.cpp:425] relu2_2 <- scale2_2
I1101 19:20:26.493644  2200 net.cpp:399] relu2_2 -> relu2_2
I1101 19:20:26.495645  2200 net.cpp:141] Setting up relu2_2
I1101 19:20:26.496146  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.496146  2200 net.cpp:156] Memory required for data: 612462000
I1101 19:20:26.496146  2200 layer_factory.hpp:77] Creating layer conv3
I1101 19:20:26.496146  2200 net.cpp:91] Creating Layer conv3
I1101 19:20:26.496146  2200 net.cpp:425] conv3 <- relu2_2
I1101 19:20:26.496146  2200 net.cpp:399] conv3 -> conv3
I1101 19:20:26.503651  2200 net.cpp:141] Setting up conv3
I1101 19:20:26.503651  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.503651  2200 net.cpp:156] Memory required for data: 622497200
I1101 19:20:26.503651  2200 layer_factory.hpp:77] Creating layer bn3
I1101 19:20:26.503651  2200 net.cpp:91] Creating Layer bn3
I1101 19:20:26.503651  2200 net.cpp:425] bn3 <- conv3
I1101 19:20:26.503651  2200 net.cpp:399] bn3 -> bn3
I1101 19:20:26.503651  2200 net.cpp:141] Setting up bn3
I1101 19:20:26.504153  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.504153  2200 net.cpp:156] Memory required for data: 632532400
I1101 19:20:26.504153  2200 layer_factory.hpp:77] Creating layer scale3
I1101 19:20:26.504153  2200 net.cpp:91] Creating Layer scale3
I1101 19:20:26.504153  2200 net.cpp:425] scale3 <- bn3
I1101 19:20:26.504153  2200 net.cpp:399] scale3 -> scale3
I1101 19:20:26.504153  2200 layer_factory.hpp:77] Creating layer scale3
I1101 19:20:26.504153  2200 net.cpp:141] Setting up scale3
I1101 19:20:26.504153  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.504153  2200 net.cpp:156] Memory required for data: 642567600
I1101 19:20:26.504153  2200 layer_factory.hpp:77] Creating layer relu3
I1101 19:20:26.504153  2200 net.cpp:91] Creating Layer relu3
I1101 19:20:26.504153  2200 net.cpp:425] relu3 <- scale3
I1101 19:20:26.504153  2200 net.cpp:399] relu3 -> relu3
I1101 19:20:26.504652  2200 net.cpp:141] Setting up relu3
I1101 19:20:26.504652  2200 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 19:20:26.504652  2200 net.cpp:156] Memory required for data: 652602800
I1101 19:20:26.504652  2200 layer_factory.hpp:77] Creating layer conv4
I1101 19:20:26.504652  2200 net.cpp:91] Creating Layer conv4
I1101 19:20:26.504652  2200 net.cpp:425] conv4 <- relu3
I1101 19:20:26.504652  2200 net.cpp:399] conv4 -> conv4
I1101 19:20:26.509155  2200 net.cpp:141] Setting up conv4
I1101 19:20:26.509155  2200 net.cpp:148] Top shape: 100 256 14 14 (5017600)
I1101 19:20:26.509155  2200 net.cpp:156] Memory required for data: 672673200
I1101 19:20:26.509155  2200 layer_factory.hpp:77] Creating layer pool4
I1101 19:20:26.509155  2200 net.cpp:91] Creating Layer pool4
I1101 19:20:26.509155  2200 net.cpp:425] pool4 <- conv4
I1101 19:20:26.509155  2200 net.cpp:399] pool4 -> pool4
I1101 19:20:26.509155  2200 net.cpp:141] Setting up pool4
I1101 19:20:26.509155  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.509155  2200 net.cpp:156] Memory required for data: 677690800
I1101 19:20:26.509155  2200 layer_factory.hpp:77] Creating layer bn4
I1101 19:20:26.509155  2200 net.cpp:91] Creating Layer bn4
I1101 19:20:26.509155  2200 net.cpp:425] bn4 <- pool4
I1101 19:20:26.509155  2200 net.cpp:399] bn4 -> bn4
I1101 19:20:26.509155  2200 net.cpp:141] Setting up bn4
I1101 19:20:26.509155  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.509155  2200 net.cpp:156] Memory required for data: 682708400
I1101 19:20:26.509155  2200 layer_factory.hpp:77] Creating layer scale4
I1101 19:20:26.509655  2200 net.cpp:91] Creating Layer scale4
I1101 19:20:26.509655  2200 net.cpp:425] scale4 <- bn4
I1101 19:20:26.509655  2200 net.cpp:399] scale4 -> scale4
I1101 19:20:26.509655  2200 layer_factory.hpp:77] Creating layer scale4
I1101 19:20:26.509655  2200 net.cpp:141] Setting up scale4
I1101 19:20:26.509655  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.509655  2200 net.cpp:156] Memory required for data: 687726000
I1101 19:20:26.509655  2200 layer_factory.hpp:77] Creating layer relu4
I1101 19:20:26.509655  2200 net.cpp:91] Creating Layer relu4
I1101 19:20:26.509655  2200 net.cpp:425] relu4 <- scale4
I1101 19:20:26.509655  2200 net.cpp:399] relu4 -> relu4
I1101 19:20:26.509655  2200 net.cpp:141] Setting up relu4
I1101 19:20:26.509655  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.509655  2200 net.cpp:156] Memory required for data: 692743600
I1101 19:20:26.509655  2200 layer_factory.hpp:77] Creating layer conv4_1
I1101 19:20:26.509655  2200 net.cpp:91] Creating Layer conv4_1
I1101 19:20:26.510155  2200 net.cpp:425] conv4_1 <- relu4
I1101 19:20:26.510155  2200 net.cpp:399] conv4_1 -> conv4_1
I1101 19:20:26.516660  2200 net.cpp:141] Setting up conv4_1
I1101 19:20:26.516660  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.516660  2200 net.cpp:156] Memory required for data: 697761200
I1101 19:20:26.516660  2200 layer_factory.hpp:77] Creating layer bn4_1
I1101 19:20:26.516660  2200 net.cpp:91] Creating Layer bn4_1
I1101 19:20:26.516660  2200 net.cpp:425] bn4_1 <- conv4_1
I1101 19:20:26.516660  2200 net.cpp:399] bn4_1 -> bn4_1
I1101 19:20:26.517160  2200 net.cpp:141] Setting up bn4_1
I1101 19:20:26.517160  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.517160  2200 net.cpp:156] Memory required for data: 702778800
I1101 19:20:26.517160  2200 layer_factory.hpp:77] Creating layer scale4_1
I1101 19:20:26.517160  2200 net.cpp:91] Creating Layer scale4_1
I1101 19:20:26.517160  2200 net.cpp:425] scale4_1 <- bn4_1
I1101 19:20:26.517160  2200 net.cpp:399] scale4_1 -> scale4_1
I1101 19:20:26.517160  2200 layer_factory.hpp:77] Creating layer scale4_1
I1101 19:20:26.517160  2200 net.cpp:141] Setting up scale4_1
I1101 19:20:26.517160  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.517160  2200 net.cpp:156] Memory required for data: 707796400
I1101 19:20:26.517160  2200 layer_factory.hpp:77] Creating layer relu4_1
I1101 19:20:26.517160  2200 net.cpp:91] Creating Layer relu4_1
I1101 19:20:26.517160  2200 net.cpp:425] relu4_1 <- scale4_1
I1101 19:20:26.517160  2200 net.cpp:399] relu4_1 -> relu4_1
I1101 19:20:26.519162  2200 net.cpp:141] Setting up relu4_1
I1101 19:20:26.519162  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.519162  2200 net.cpp:156] Memory required for data: 712814000
I1101 19:20:26.519162  2200 layer_factory.hpp:77] Creating layer conv4_2
I1101 19:20:26.519162  2200 net.cpp:91] Creating Layer conv4_2
I1101 19:20:26.519162  2200 net.cpp:425] conv4_2 <- relu4_1
I1101 19:20:26.519162  2200 net.cpp:399] conv4_2 -> conv4_2
I1101 19:20:26.525666  2200 net.cpp:141] Setting up conv4_2
I1101 19:20:26.526167  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.526167  2200 net.cpp:156] Memory required for data: 717831600
I1101 19:20:26.526167  2200 layer_factory.hpp:77] Creating layer bn4_2
I1101 19:20:26.526167  2200 net.cpp:91] Creating Layer bn4_2
I1101 19:20:26.526167  2200 net.cpp:425] bn4_2 <- conv4_2
I1101 19:20:26.526167  2200 net.cpp:399] bn4_2 -> bn4_2
I1101 19:20:26.526167  2200 net.cpp:141] Setting up bn4_2
I1101 19:20:26.526167  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.526167  2200 net.cpp:156] Memory required for data: 722849200
I1101 19:20:26.526167  2200 layer_factory.hpp:77] Creating layer scale4_2
I1101 19:20:26.526167  2200 net.cpp:91] Creating Layer scale4_2
I1101 19:20:26.526167  2200 net.cpp:425] scale4_2 <- bn4_2
I1101 19:20:26.526167  2200 net.cpp:399] scale4_2 -> scale4_2
I1101 19:20:26.526167  2200 layer_factory.hpp:77] Creating layer scale4_2
I1101 19:20:26.526667  2200 net.cpp:141] Setting up scale4_2
I1101 19:20:26.526667  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.526667  2200 net.cpp:156] Memory required for data: 727866800
I1101 19:20:26.526667  2200 layer_factory.hpp:77] Creating layer relu4_2
I1101 19:20:26.526667  2200 net.cpp:91] Creating Layer relu4_2
I1101 19:20:26.526667  2200 net.cpp:425] relu4_2 <- scale4_2
I1101 19:20:26.526667  2200 net.cpp:399] relu4_2 -> relu4_2
I1101 19:20:26.526667  2200 net.cpp:141] Setting up relu4_2
I1101 19:20:26.526667  2200 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 19:20:26.526667  2200 net.cpp:156] Memory required for data: 732884400
I1101 19:20:26.527168  2200 layer_factory.hpp:77] Creating layer pool4_2
I1101 19:20:26.527168  2200 net.cpp:91] Creating Layer pool4_2
I1101 19:20:26.527168  2200 net.cpp:425] pool4_2 <- relu4_2
I1101 19:20:26.527168  2200 net.cpp:399] pool4_2 -> pool4_2
I1101 19:20:26.527168  2200 net.cpp:141] Setting up pool4_2
I1101 19:20:26.527168  2200 net.cpp:148] Top shape: 100 256 4 4 (409600)
I1101 19:20:26.527168  2200 net.cpp:156] Memory required for data: 734522800
I1101 19:20:26.527168  2200 layer_factory.hpp:77] Creating layer conv4_0
I1101 19:20:26.527168  2200 net.cpp:91] Creating Layer conv4_0
I1101 19:20:26.527168  2200 net.cpp:425] conv4_0 <- pool4_2
I1101 19:20:26.527168  2200 net.cpp:399] conv4_0 -> conv4_0
I1101 19:20:26.538676  2200 net.cpp:141] Setting up conv4_0
I1101 19:20:26.538676  2200 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 19:20:26.538676  2200 net.cpp:156] Memory required for data: 737799600
I1101 19:20:26.538676  2200 layer_factory.hpp:77] Creating layer bn4_0
I1101 19:20:26.538676  2200 net.cpp:91] Creating Layer bn4_0
I1101 19:20:26.538676  2200 net.cpp:425] bn4_0 <- conv4_0
I1101 19:20:26.538676  2200 net.cpp:399] bn4_0 -> bn4_0
I1101 19:20:26.538676  2200 net.cpp:141] Setting up bn4_0
I1101 19:20:26.539176  2200 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 19:20:26.539176  2200 net.cpp:156] Memory required for data: 741076400
I1101 19:20:26.539176  2200 layer_factory.hpp:77] Creating layer scale4_0
I1101 19:20:26.539176  2200 net.cpp:91] Creating Layer scale4_0
I1101 19:20:26.539176  2200 net.cpp:425] scale4_0 <- bn4_0
I1101 19:20:26.539176  2200 net.cpp:399] scale4_0 -> scale4_0
I1101 19:20:26.539176  2200 layer_factory.hpp:77] Creating layer scale4_0
I1101 19:20:26.539176  2200 net.cpp:141] Setting up scale4_0
I1101 19:20:26.539176  2200 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 19:20:26.539176  2200 net.cpp:156] Memory required for data: 744353200
I1101 19:20:26.539176  2200 layer_factory.hpp:77] Creating layer relu4_0
I1101 19:20:26.539176  2200 net.cpp:91] Creating Layer relu4_0
I1101 19:20:26.539176  2200 net.cpp:425] relu4_0 <- scale4_0
I1101 19:20:26.539176  2200 net.cpp:399] relu4_0 -> relu4_0
I1101 19:20:26.539676  2200 net.cpp:141] Setting up relu4_0
I1101 19:20:26.539676  2200 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 19:20:26.539676  2200 net.cpp:156] Memory required for data: 747630000
I1101 19:20:26.539676  2200 layer_factory.hpp:77] Creating layer cccp4
I1101 19:20:26.539676  2200 net.cpp:91] Creating Layer cccp4
I1101 19:20:26.539676  2200 net.cpp:425] cccp4 <- relu4_0
I1101 19:20:26.539676  2200 net.cpp:399] cccp4 -> cccp4
I1101 19:20:26.550184  2200 net.cpp:141] Setting up cccp4
I1101 19:20:26.550184  2200 net.cpp:148] Top shape: 100 2048 4 4 (3276800)
I1101 19:20:26.550184  2200 net.cpp:156] Memory required for data: 760737200
I1101 19:20:26.550184  2200 layer_factory.hpp:77] Creating layer relu_cccp4
I1101 19:20:26.550184  2200 net.cpp:91] Creating Layer relu_cccp4
I1101 19:20:26.550184  2200 net.cpp:425] relu_cccp4 <- cccp4
I1101 19:20:26.550184  2200 net.cpp:386] relu_cccp4 -> cccp4 (in-place)
I1101 19:20:26.550609  2200 net.cpp:141] Setting up relu_cccp4
I1101 19:20:26.550609  2200 net.cpp:148] Top shape: 100 2048 4 4 (3276800)
I1101 19:20:26.550609  2200 net.cpp:156] Memory required for data: 773844400
I1101 19:20:26.550609  2200 layer_factory.hpp:77] Creating layer cccp5
I1101 19:20:26.550609  2200 net.cpp:91] Creating Layer cccp5
I1101 19:20:26.550609  2200 net.cpp:425] cccp5 <- cccp4
I1101 19:20:26.550609  2200 net.cpp:399] cccp5 -> cccp5
I1101 19:20:26.557116  2200 net.cpp:141] Setting up cccp5
I1101 19:20:26.557116  2200 net.cpp:148] Top shape: 100 256 4 4 (409600)
I1101 19:20:26.557116  2200 net.cpp:156] Memory required for data: 775482800
I1101 19:20:26.557116  2200 layer_factory.hpp:77] Creating layer relu_cccp5
I1101 19:20:26.557116  2200 net.cpp:91] Creating Layer relu_cccp5
I1101 19:20:26.557116  2200 net.cpp:425] relu_cccp5 <- cccp5
I1101 19:20:26.557116  2200 net.cpp:386] relu_cccp5 -> cccp5 (in-place)
I1101 19:20:26.557616  2200 net.cpp:141] Setting up relu_cccp5
I1101 19:20:26.557616  2200 net.cpp:148] Top shape: 100 256 4 4 (409600)
I1101 19:20:26.557616  2200 net.cpp:156] Memory required for data: 777121200
I1101 19:20:26.557616  2200 layer_factory.hpp:77] Creating layer poolcp5
I1101 19:20:26.557616  2200 net.cpp:91] Creating Layer poolcp5
I1101 19:20:26.557616  2200 net.cpp:425] poolcp5 <- cccp5
I1101 19:20:26.557616  2200 net.cpp:399] poolcp5 -> poolcp5
I1101 19:20:26.557616  2200 net.cpp:141] Setting up poolcp5
I1101 19:20:26.557616  2200 net.cpp:148] Top shape: 100 256 2 2 (102400)
I1101 19:20:26.557616  2200 net.cpp:156] Memory required for data: 777530800
I1101 19:20:26.557616  2200 layer_factory.hpp:77] Creating layer cccp6
I1101 19:20:26.557616  2200 net.cpp:91] Creating Layer cccp6
I1101 19:20:26.557616  2200 net.cpp:425] cccp6 <- poolcp5
I1101 19:20:26.557616  2200 net.cpp:399] cccp6 -> cccp6
I1101 19:20:26.564113  2200 net.cpp:141] Setting up cccp6
I1101 19:20:26.564113  2200 net.cpp:148] Top shape: 100 256 2 2 (102400)
I1101 19:20:26.564113  2200 net.cpp:156] Memory required for data: 777940400
I1101 19:20:26.564113  2200 layer_factory.hpp:77] Creating layer relu_cccp6
I1101 19:20:26.564113  2200 net.cpp:91] Creating Layer relu_cccp6
I1101 19:20:26.564113  2200 net.cpp:425] relu_cccp6 <- cccp6
I1101 19:20:26.564574  2200 net.cpp:386] relu_cccp6 -> cccp6 (in-place)
I1101 19:20:26.564574  2200 net.cpp:141] Setting up relu_cccp6
I1101 19:20:26.564574  2200 net.cpp:148] Top shape: 100 256 2 2 (102400)
I1101 19:20:26.564574  2200 net.cpp:156] Memory required for data: 778350000
I1101 19:20:26.564574  2200 layer_factory.hpp:77] Creating layer poolcp6
I1101 19:20:26.564574  2200 net.cpp:91] Creating Layer poolcp6
I1101 19:20:26.564574  2200 net.cpp:425] poolcp6 <- cccp6
I1101 19:20:26.564574  2200 net.cpp:399] poolcp6 -> poolcp6
I1101 19:20:26.564574  2200 net.cpp:141] Setting up poolcp6
I1101 19:20:26.564574  2200 net.cpp:148] Top shape: 100 256 1 1 (25600)
I1101 19:20:26.564574  2200 net.cpp:156] Memory required for data: 778452400
I1101 19:20:26.564574  2200 layer_factory.hpp:77] Creating layer ip1
I1101 19:20:26.564574  2200 net.cpp:91] Creating Layer ip1
I1101 19:20:26.564574  2200 net.cpp:425] ip1 <- poolcp6
I1101 19:20:26.564574  2200 net.cpp:399] ip1 -> ip1
I1101 19:20:26.565076  2200 net.cpp:141] Setting up ip1
I1101 19:20:26.565076  2200 net.cpp:148] Top shape: 100 10 (1000)
I1101 19:20:26.565076  2200 net.cpp:156] Memory required for data: 778456400
I1101 19:20:26.565076  2200 layer_factory.hpp:77] Creating layer ip1_ip1_0_split
I1101 19:20:26.565076  2200 net.cpp:91] Creating Layer ip1_ip1_0_split
I1101 19:20:26.565076  2200 net.cpp:425] ip1_ip1_0_split <- ip1
I1101 19:20:26.565076  2200 net.cpp:399] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1101 19:20:26.565076  2200 net.cpp:399] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1101 19:20:26.565076  2200 net.cpp:141] Setting up ip1_ip1_0_split
I1101 19:20:26.565076  2200 net.cpp:148] Top shape: 100 10 (1000)
I1101 19:20:26.565076  2200 net.cpp:148] Top shape: 100 10 (1000)
I1101 19:20:26.565076  2200 net.cpp:156] Memory required for data: 778464400
I1101 19:20:26.565076  2200 layer_factory.hpp:77] Creating layer accuracy
I1101 19:20:26.565076  2200 net.cpp:91] Creating Layer accuracy
I1101 19:20:26.565076  2200 net.cpp:425] accuracy <- ip1_ip1_0_split_0
I1101 19:20:26.565076  2200 net.cpp:425] accuracy <- label_mnist_1_split_0
I1101 19:20:26.565076  2200 net.cpp:399] accuracy -> accuracy
I1101 19:20:26.565076  2200 net.cpp:141] Setting up accuracy
I1101 19:20:26.565076  2200 net.cpp:148] Top shape: (1)
I1101 19:20:26.565076  2200 net.cpp:156] Memory required for data: 778464404
I1101 19:20:26.565076  2200 layer_factory.hpp:77] Creating layer loss
I1101 19:20:26.565076  2200 net.cpp:91] Creating Layer loss
I1101 19:20:26.565076  2200 net.cpp:425] loss <- ip1_ip1_0_split_1
I1101 19:20:26.565076  2200 net.cpp:425] loss <- label_mnist_1_split_1
I1101 19:20:26.565076  2200 net.cpp:399] loss -> loss
I1101 19:20:26.565076  2200 layer_factory.hpp:77] Creating layer loss
I1101 19:20:26.565676  2200 net.cpp:141] Setting up loss
I1101 19:20:26.565676  2200 net.cpp:148] Top shape: (1)
I1101 19:20:26.565676  2200 net.cpp:151]     with loss weight 1
I1101 19:20:26.565676  2200 net.cpp:156] Memory required for data: 778464408
I1101 19:20:26.565676  2200 net.cpp:217] loss needs backward computation.
I1101 19:20:26.565676  2200 net.cpp:219] accuracy does not need backward computation.
I1101 19:20:26.565676  2200 net.cpp:217] ip1_ip1_0_split needs backward computation.
I1101 19:20:26.565676  2200 net.cpp:217] ip1 needs backward computation.
I1101 19:20:26.565676  2200 net.cpp:217] poolcp6 needs backward computation.
I1101 19:20:26.565676  2200 net.cpp:217] relu_cccp6 needs backward computation.
I1101 19:20:26.565676  2200 net.cpp:217] cccp6 needs backward computation.
I1101 19:20:26.565676  2200 net.cpp:217] poolcp5 needs backward computation.
I1101 19:20:26.565676  2200 net.cpp:217] relu_cccp5 needs backward computation.
I1101 19:20:26.565676  2200 net.cpp:217] cccp5 needs backward computation.
I1101 19:20:26.565676  2200 net.cpp:217] relu_cccp4 needs backward computation.
I1101 19:20:26.565676  2200 net.cpp:217] cccp4 needs backward computation.
I1101 19:20:26.565676  2200 net.cpp:217] relu4_0 needs backward computation.
I1101 19:20:26.565676  2200 net.cpp:217] scale4_0 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] bn4_0 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] conv4_0 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] pool4_2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] relu4_2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] scale4_2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] bn4_2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] conv4_2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] relu4_1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] scale4_1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] bn4_1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] conv4_1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] relu4 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] scale4 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] bn4 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] pool4 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] conv4 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] relu3 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] scale3 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] bn3 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] conv3 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] relu2_2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] scale2_2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] bn2_2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] conv2_2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] pool2_1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] relu2_1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] scale2_1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] bn2_1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] conv2_1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] relu2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] scale2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] bn2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] conv2 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] relu1_0 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] scale1_0 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] bn1_0 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] conv1_0 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] relu1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] scale1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] bn1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:217] conv1 needs backward computation.
I1101 19:20:26.566179  2200 net.cpp:219] label_mnist_1_split does not need backward computation.
I1101 19:20:26.566179  2200 net.cpp:219] mnist does not need backward computation.
I1101 19:20:26.566179  2200 net.cpp:261] This network produces output accuracy
I1101 19:20:26.566179  2200 net.cpp:261] This network produces output loss
I1101 19:20:26.566179  2200 net.cpp:274] Network initialization done.
I1101 19:20:26.566679  2200 solver.cpp:60] Solver scaffolding done.
I1101 19:20:26.570180  2200 caffe.cpp:129] Finetuning from examples/mnist/simpnet_nodrp_iter_600.caffemodel
I1101 19:20:26.670253  2200 caffe.cpp:220] Starting Optimization
I1101 19:20:26.670253  2200 solver.cpp:279] Solving SimpNet
I1101 19:20:26.670253  2200 solver.cpp:280] Learning Rate Policy: fixed
I1101 19:20:26.682261  2200 solver.cpp:337] Iteration 0, Testing net (#0)
I1101 19:20:31.853937  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9974
I1101 19:20:31.853937  2200 solver.cpp:404]     Test net output #1: loss = 0.012731 (* 1 = 0.012731 loss)
I1101 19:20:32.226323  2200 solver.cpp:228] Iteration 0, loss = 4.1066e-005
I1101 19:20:32.226323  2200 solver.cpp:244]     Train net output #0: loss = 4.1066e-005 (* 1 = 4.1066e-005 loss)
I1101 19:20:32.226323  2200 sgd_solver.cpp:106] Iteration 0, lr = 0.0192415
I1101 19:20:48.747262  2200 solver.cpp:228] Iteration 100, loss = 4.26018e-005
I1101 19:20:48.747262  2200 solver.cpp:244]     Train net output #0: loss = 4.26018e-005 (* 1 = 4.26018e-005 loss)
I1101 19:20:48.747262  2200 sgd_solver.cpp:106] Iteration 100, lr = 0.0192415
I1101 19:21:05.069329  2200 solver.cpp:228] Iteration 200, loss = 5.99507e-005
I1101 19:21:05.069329  2200 solver.cpp:244]     Train net output #0: loss = 5.99507e-005 (* 1 = 5.99507e-005 loss)
I1101 19:21:05.069329  2200 sgd_solver.cpp:106] Iteration 200, lr = 0.0192415
I1101 19:21:21.525738  2200 solver.cpp:228] Iteration 300, loss = 5.56181e-005
I1101 19:21:21.525738  2200 solver.cpp:244]     Train net output #0: loss = 5.56181e-005 (* 1 = 5.56181e-005 loss)
I1101 19:21:21.525738  2200 sgd_solver.cpp:106] Iteration 300, lr = 0.0192415
I1101 19:21:38.276365  2200 solver.cpp:228] Iteration 400, loss = 1.78723e-005
I1101 19:21:38.276365  2200 solver.cpp:244]     Train net output #0: loss = 1.78723e-005 (* 1 = 1.78723e-005 loss)
I1101 19:21:38.276365  2200 sgd_solver.cpp:106] Iteration 400, lr = 0.0192415
I1101 19:21:55.096647  2200 solver.cpp:228] Iteration 500, loss = 3.28238e-005
I1101 19:21:55.096647  2200 solver.cpp:244]     Train net output #0: loss = 3.28238e-005 (* 1 = 3.28238e-005 loss)
I1101 19:21:55.096647  2200 sgd_solver.cpp:106] Iteration 500, lr = 0.0192415
I1101 19:22:11.851826  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_600.caffemodel
I1101 19:22:12.106009  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_600.solverstate
I1101 19:22:12.170055  2200 solver.cpp:337] Iteration 600, Testing net (#0)
I1101 19:22:17.644987  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9974
I1101 19:22:17.644987  2200 solver.cpp:404]     Test net output #1: loss = 0.0130046 (* 1 = 0.0130046 loss)
I1101 19:22:17.715629  2200 solver.cpp:228] Iteration 600, loss = 2.84859e-005
I1101 19:22:17.715629  2200 solver.cpp:244]     Train net output #0: loss = 2.84859e-005 (* 1 = 2.84859e-005 loss)
I1101 19:22:17.715629  2200 sgd_solver.cpp:106] Iteration 600, lr = 0.0192415
I1101 19:22:34.496510  2200 solver.cpp:228] Iteration 700, loss = 3.308e-005
I1101 19:22:34.496510  2200 solver.cpp:244]     Train net output #0: loss = 3.30801e-005 (* 1 = 3.30801e-005 loss)
I1101 19:22:34.496510  2200 sgd_solver.cpp:106] Iteration 700, lr = 0.0192415
I1101 19:22:50.824110  2200 solver.cpp:228] Iteration 800, loss = 4.15818e-005
I1101 19:22:50.824110  2200 solver.cpp:244]     Train net output #0: loss = 4.15818e-005 (* 1 = 4.15818e-005 loss)
I1101 19:22:50.824110  2200 sgd_solver.cpp:106] Iteration 800, lr = 0.0192415
I1101 19:23:07.207763  2200 solver.cpp:228] Iteration 900, loss = 3.8547e-005
I1101 19:23:07.207763  2200 solver.cpp:244]     Train net output #0: loss = 3.8547e-005 (* 1 = 3.8547e-005 loss)
I1101 19:23:07.207763  2200 sgd_solver.cpp:106] Iteration 900, lr = 0.0192415
I1101 19:23:24.314729  2200 solver.cpp:228] Iteration 1000, loss = 1.28834e-005
I1101 19:23:24.314729  2200 solver.cpp:244]     Train net output #0: loss = 1.28835e-005 (* 1 = 1.28835e-005 loss)
I1101 19:23:24.314729  2200 sgd_solver.cpp:106] Iteration 1000, lr = 0.0192415
I1101 19:23:41.124488  2200 solver.cpp:228] Iteration 1100, loss = 2.38076e-005
I1101 19:23:41.124488  2200 solver.cpp:244]     Train net output #0: loss = 2.38076e-005 (* 1 = 2.38076e-005 loss)
I1101 19:23:41.124488  2200 sgd_solver.cpp:106] Iteration 1100, lr = 0.0192415
I1101 19:23:57.353679  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_1200.caffemodel
I1101 19:23:57.474267  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_1200.solverstate
I1101 19:23:57.524302  2200 solver.cpp:337] Iteration 1200, Testing net (#0)
I1101 19:24:02.488309  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9974
I1101 19:24:02.488309  2200 solver.cpp:404]     Test net output #1: loss = 0.0132993 (* 1 = 0.0132993 loss)
I1101 19:24:02.557327  2200 solver.cpp:228] Iteration 1200, loss = 2.18482e-005
I1101 19:24:02.557327  2200 solver.cpp:244]     Train net output #0: loss = 2.18482e-005 (* 1 = 2.18482e-005 loss)
I1101 19:24:02.557327  2200 sgd_solver.cpp:106] Iteration 1200, lr = 0.0192415
I1101 19:24:19.026741  2200 solver.cpp:228] Iteration 1300, loss = 2.50039e-005
I1101 19:24:19.026741  2200 solver.cpp:244]     Train net output #0: loss = 2.50039e-005 (* 1 = 2.50039e-005 loss)
I1101 19:24:19.026741  2200 sgd_solver.cpp:106] Iteration 1300, lr = 0.0192415
I1101 19:24:35.558301  2200 solver.cpp:228] Iteration 1400, loss = 3.21182e-005
I1101 19:24:35.558301  2200 solver.cpp:244]     Train net output #0: loss = 3.21182e-005 (* 1 = 3.21182e-005 loss)
I1101 19:24:35.558301  2200 sgd_solver.cpp:106] Iteration 1400, lr = 0.0192415
I1101 19:24:52.221812  2200 solver.cpp:228] Iteration 1500, loss = 2.93013e-005
I1101 19:24:52.221812  2200 solver.cpp:244]     Train net output #0: loss = 2.93013e-005 (* 1 = 2.93013e-005 loss)
I1101 19:24:52.221812  2200 sgd_solver.cpp:106] Iteration 1500, lr = 0.0192415
I1101 19:25:08.947427  2200 solver.cpp:228] Iteration 1600, loss = 1.01122e-005
I1101 19:25:08.947427  2200 solver.cpp:244]     Train net output #0: loss = 1.01123e-005 (* 1 = 1.01123e-005 loss)
I1101 19:25:08.947427  2200 sgd_solver.cpp:106] Iteration 1600, lr = 0.0192415
I1101 19:25:26.149894  2200 solver.cpp:228] Iteration 1700, loss = 1.85473e-005
I1101 19:25:26.149894  2200 solver.cpp:244]     Train net output #0: loss = 1.85474e-005 (* 1 = 1.85474e-005 loss)
I1101 19:25:26.149894  2200 sgd_solver.cpp:106] Iteration 1700, lr = 0.0192415
I1101 19:25:43.414420  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_1800.caffemodel
I1101 19:25:43.541012  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_1800.solverstate
I1101 19:25:43.601054  2200 solver.cpp:337] Iteration 1800, Testing net (#0)
I1101 19:25:48.653262  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 19:25:48.653262  2200 solver.cpp:404]     Test net output #1: loss = 0.0135306 (* 1 = 0.0135306 loss)
I1101 19:25:48.724952  2200 solver.cpp:228] Iteration 1800, loss = 1.75924e-005
I1101 19:25:48.724952  2200 solver.cpp:244]     Train net output #0: loss = 1.75924e-005 (* 1 = 1.75924e-005 loss)
I1101 19:25:48.724952  2200 sgd_solver.cpp:106] Iteration 1800, lr = 0.0192415
I1101 19:26:05.717737  2200 solver.cpp:228] Iteration 1900, loss = 1.99579e-005
I1101 19:26:05.717737  2200 solver.cpp:244]     Train net output #0: loss = 1.99579e-005 (* 1 = 1.99579e-005 loss)
I1101 19:26:05.717737  2200 sgd_solver.cpp:106] Iteration 1900, lr = 0.0192415
I1101 19:26:22.612866  2200 solver.cpp:228] Iteration 2000, loss = 2.57951e-005
I1101 19:26:22.612866  2200 solver.cpp:244]     Train net output #0: loss = 2.57952e-005 (* 1 = 2.57952e-005 loss)
I1101 19:26:22.612866  2200 sgd_solver.cpp:106] Iteration 2000, lr = 0.0192415
I1101 19:26:39.538413  2200 solver.cpp:228] Iteration 2100, loss = 2.36281e-005
I1101 19:26:39.538413  2200 solver.cpp:244]     Train net output #0: loss = 2.36281e-005 (* 1 = 2.36281e-005 loss)
I1101 19:26:39.538413  2200 sgd_solver.cpp:106] Iteration 2100, lr = 0.0192415
I1101 19:26:56.389612  2200 solver.cpp:228] Iteration 2200, loss = 8.26857e-006
I1101 19:26:56.389612  2200 solver.cpp:244]     Train net output #0: loss = 8.26858e-006 (* 1 = 8.26858e-006 loss)
I1101 19:26:56.389612  2200 sgd_solver.cpp:106] Iteration 2200, lr = 0.0192415
I1101 19:27:13.308787  2200 solver.cpp:228] Iteration 2300, loss = 1.51164e-005
I1101 19:27:13.308787  2200 solver.cpp:244]     Train net output #0: loss = 1.51164e-005 (* 1 = 1.51164e-005 loss)
I1101 19:27:13.308787  2200 sgd_solver.cpp:106] Iteration 2300, lr = 0.0192415
I1101 19:27:30.305543  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_2400.caffemodel
I1101 19:27:30.437665  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_2400.solverstate
I1101 19:27:30.496271  2200 solver.cpp:337] Iteration 2400, Testing net (#0)
I1101 19:27:35.468899  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 19:27:35.468899  2200 solver.cpp:404]     Test net output #1: loss = 0.0137275 (* 1 = 0.0137275 loss)
I1101 19:27:35.535029  2200 solver.cpp:228] Iteration 2400, loss = 1.46082e-005
I1101 19:27:35.535029  2200 solver.cpp:244]     Train net output #0: loss = 1.46082e-005 (* 1 = 1.46082e-005 loss)
I1101 19:27:35.535029  2200 sgd_solver.cpp:106] Iteration 2400, lr = 0.0192415
I1101 19:27:52.368244  2200 solver.cpp:228] Iteration 2500, loss = 1.64618e-005
I1101 19:27:52.368244  2200 solver.cpp:244]     Train net output #0: loss = 1.64618e-005 (* 1 = 1.64618e-005 loss)
I1101 19:27:52.368244  2200 sgd_solver.cpp:106] Iteration 2500, lr = 0.0192415
I1101 19:28:09.216174  2200 solver.cpp:228] Iteration 2600, loss = 2.14739e-005
I1101 19:28:09.216174  2200 solver.cpp:244]     Train net output #0: loss = 2.14739e-005 (* 1 = 2.14739e-005 loss)
I1101 19:28:09.216174  2200 sgd_solver.cpp:106] Iteration 2600, lr = 0.0192415
I1101 19:28:26.118089  2200 solver.cpp:228] Iteration 2700, loss = 1.97351e-005
I1101 19:28:26.118089  2200 solver.cpp:244]     Train net output #0: loss = 1.97351e-005 (* 1 = 1.97351e-005 loss)
I1101 19:28:26.118089  2200 sgd_solver.cpp:106] Iteration 2700, lr = 0.0192415
I1101 19:28:43.374538  2200 solver.cpp:228] Iteration 2800, loss = 6.97389e-006
I1101 19:28:43.374538  2200 solver.cpp:244]     Train net output #0: loss = 6.97391e-006 (* 1 = 6.97391e-006 loss)
I1101 19:28:43.374538  2200 sgd_solver.cpp:106] Iteration 2800, lr = 0.0192415
I1101 19:28:59.830627  2200 solver.cpp:228] Iteration 2900, loss = 1.27104e-005
I1101 19:28:59.830627  2200 solver.cpp:244]     Train net output #0: loss = 1.27104e-005 (* 1 = 1.27104e-005 loss)
I1101 19:28:59.830627  2200 sgd_solver.cpp:106] Iteration 2900, lr = 0.0192415
I1101 19:29:16.251366  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_3000.caffemodel
I1101 19:29:16.375455  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_3000.solverstate
I1101 19:29:16.468086  2200 solver.cpp:337] Iteration 3000, Testing net (#0)
I1101 19:29:21.478237  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 19:29:21.478237  2200 solver.cpp:404]     Test net output #1: loss = 0.0138996 (* 1 = 0.0138996 loss)
I1101 19:29:21.546972  2200 solver.cpp:228] Iteration 3000, loss = 1.24485e-005
I1101 19:29:21.546972  2200 solver.cpp:244]     Train net output #0: loss = 1.24485e-005 (* 1 = 1.24485e-005 loss)
I1101 19:29:21.546972  2200 sgd_solver.cpp:106] Iteration 3000, lr = 0.0192415
I1101 19:29:38.025161  2200 solver.cpp:228] Iteration 3100, loss = 1.39477e-005
I1101 19:29:38.025161  2200 solver.cpp:244]     Train net output #0: loss = 1.39477e-005 (* 1 = 1.39477e-005 loss)
I1101 19:29:38.025161  2200 sgd_solver.cpp:106] Iteration 3100, lr = 0.0192415
I1101 19:29:54.511631  2200 solver.cpp:228] Iteration 3200, loss = 1.82951e-005
I1101 19:29:54.511631  2200 solver.cpp:244]     Train net output #0: loss = 1.82951e-005 (* 1 = 1.82951e-005 loss)
I1101 19:29:54.511631  2200 sgd_solver.cpp:106] Iteration 3200, lr = 0.0192415
I1101 19:30:11.065526  2200 solver.cpp:228] Iteration 3300, loss = 1.69174e-005
I1101 19:30:11.065526  2200 solver.cpp:244]     Train net output #0: loss = 1.69174e-005 (* 1 = 1.69174e-005 loss)
I1101 19:30:11.065526  2200 sgd_solver.cpp:106] Iteration 3300, lr = 0.0192415
I1101 19:30:27.515401  2200 solver.cpp:228] Iteration 3400, loss = 6.02614e-006
I1101 19:30:27.515401  2200 solver.cpp:244]     Train net output #0: loss = 6.02616e-006 (* 1 = 6.02616e-006 loss)
I1101 19:30:27.515401  2200 sgd_solver.cpp:106] Iteration 3400, lr = 0.0192415
I1101 19:30:44.021549  2200 solver.cpp:228] Iteration 3500, loss = 1.09453e-005
I1101 19:30:44.021549  2200 solver.cpp:244]     Train net output #0: loss = 1.09453e-005 (* 1 = 1.09453e-005 loss)
I1101 19:30:44.021549  2200 sgd_solver.cpp:106] Iteration 3500, lr = 0.0192415
I1101 19:31:00.428139  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_3600.caffemodel
I1101 19:31:00.554229  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_3600.solverstate
I1101 19:31:00.605765  2200 solver.cpp:337] Iteration 3600, Testing net (#0)
I1101 19:31:05.586341  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 19:31:05.586341  2200 solver.cpp:404]     Test net output #1: loss = 0.0140521 (* 1 = 0.0140521 loss)
I1101 19:31:05.656658  2200 solver.cpp:228] Iteration 3600, loss = 1.08086e-005
I1101 19:31:05.656658  2200 solver.cpp:244]     Train net output #0: loss = 1.08086e-005 (* 1 = 1.08086e-005 loss)
I1101 19:31:05.656658  2200 sgd_solver.cpp:106] Iteration 3600, lr = 0.0192415
I1101 19:31:22.178452  2200 solver.cpp:228] Iteration 3700, loss = 1.20286e-005
I1101 19:31:22.178452  2200 solver.cpp:244]     Train net output #0: loss = 1.20287e-005 (* 1 = 1.20287e-005 loss)
I1101 19:31:22.178452  2200 sgd_solver.cpp:106] Iteration 3700, lr = 0.0192415
I1101 19:31:38.828171  2200 solver.cpp:228] Iteration 3800, loss = 1.58795e-005
I1101 19:31:38.828171  2200 solver.cpp:244]     Train net output #0: loss = 1.58795e-005 (* 1 = 1.58795e-005 loss)
I1101 19:31:38.828171  2200 sgd_solver.cpp:106] Iteration 3800, lr = 0.0192415
I1101 19:31:55.606956  2200 solver.cpp:228] Iteration 3900, loss = 1.47425e-005
I1101 19:31:55.606956  2200 solver.cpp:244]     Train net output #0: loss = 1.47425e-005 (* 1 = 1.47425e-005 loss)
I1101 19:31:55.606956  2200 sgd_solver.cpp:106] Iteration 3900, lr = 0.0192415
I1101 19:32:12.312060  2200 solver.cpp:228] Iteration 4000, loss = 5.27986e-006
I1101 19:32:12.312060  2200 solver.cpp:244]     Train net output #0: loss = 5.27988e-006 (* 1 = 5.27988e-006 loss)
I1101 19:32:12.312060  2200 sgd_solver.cpp:106] Iteration 4000, lr = 0.0192415
I1101 19:32:29.073114  2200 solver.cpp:228] Iteration 4100, loss = 9.56884e-006
I1101 19:32:29.073114  2200 solver.cpp:244]     Train net output #0: loss = 9.56885e-006 (* 1 = 9.56885e-006 loss)
I1101 19:32:29.073114  2200 sgd_solver.cpp:106] Iteration 4100, lr = 0.0192415
I1101 19:32:45.694991  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_4200.caffemodel
I1101 19:32:45.824585  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_4200.solverstate
I1101 19:32:45.877624  2200 solver.cpp:337] Iteration 4200, Testing net (#0)
I1101 19:32:50.932410  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 19:32:50.932410  2200 solver.cpp:404]     Test net output #1: loss = 0.014187 (* 1 = 0.014187 loss)
I1101 19:32:51.002336  2200 solver.cpp:228] Iteration 4200, loss = 9.52521e-006
I1101 19:32:51.002336  2200 solver.cpp:244]     Train net output #0: loss = 9.52522e-006 (* 1 = 9.52522e-006 loss)
I1101 19:32:51.002336  2200 sgd_solver.cpp:106] Iteration 4200, lr = 0.0192415
I1101 19:33:07.784749  2200 solver.cpp:228] Iteration 4300, loss = 1.05413e-005
I1101 19:33:07.784749  2200 solver.cpp:244]     Train net output #0: loss = 1.05413e-005 (* 1 = 1.05413e-005 loss)
I1101 19:33:07.784749  2200 sgd_solver.cpp:106] Iteration 4300, lr = 0.0192415
I1101 19:33:24.630775  2200 solver.cpp:228] Iteration 4400, loss = 1.39599e-005
I1101 19:33:24.630775  2200 solver.cpp:244]     Train net output #0: loss = 1.39599e-005 (* 1 = 1.39599e-005 loss)
I1101 19:33:24.630775  2200 sgd_solver.cpp:106] Iteration 4400, lr = 0.0192415
I1101 19:33:41.258994  2200 solver.cpp:228] Iteration 4500, loss = 1.30404e-005
I1101 19:33:41.258994  2200 solver.cpp:244]     Train net output #0: loss = 1.30404e-005 (* 1 = 1.30404e-005 loss)
I1101 19:33:41.258994  2200 sgd_solver.cpp:106] Iteration 4500, lr = 0.0192415
I1101 19:33:57.965270  2200 solver.cpp:228] Iteration 4600, loss = 4.69214e-006
I1101 19:33:57.965270  2200 solver.cpp:244]     Train net output #0: loss = 4.69216e-006 (* 1 = 4.69216e-006 loss)
I1101 19:33:57.965270  2200 sgd_solver.cpp:106] Iteration 4600, lr = 0.0192415
I1101 19:34:14.699568  2200 solver.cpp:228] Iteration 4700, loss = 8.48392e-006
I1101 19:34:14.699568  2200 solver.cpp:244]     Train net output #0: loss = 8.48394e-006 (* 1 = 8.48394e-006 loss)
I1101 19:34:14.699568  2200 sgd_solver.cpp:106] Iteration 4700, lr = 0.0192415
I1101 19:34:31.361455  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_4800.caffemodel
I1101 19:34:31.498553  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_4800.solverstate
I1101 19:34:31.555595  2200 solver.cpp:337] Iteration 4800, Testing net (#0)
I1101 19:34:36.600510  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 19:34:36.600510  2200 solver.cpp:404]     Test net output #1: loss = 0.0143105 (* 1 = 0.0143105 loss)
I1101 19:34:36.671653  2200 solver.cpp:228] Iteration 4800, loss = 8.51066e-006
I1101 19:34:36.671653  2200 solver.cpp:244]     Train net output #0: loss = 8.51067e-006 (* 1 = 8.51067e-006 loss)
I1101 19:34:36.671653  2200 sgd_solver.cpp:106] Iteration 4800, lr = 0.0192415
I1101 19:34:53.402915  2200 solver.cpp:228] Iteration 4900, loss = 9.37881e-006
I1101 19:34:53.402915  2200 solver.cpp:244]     Train net output #0: loss = 9.37882e-006 (* 1 = 9.37882e-006 loss)
I1101 19:34:53.402915  2200 sgd_solver.cpp:106] Iteration 4900, lr = 0.0192415
I1101 19:35:10.120535  2200 solver.cpp:228] Iteration 5000, loss = 1.24439e-005
I1101 19:35:10.120535  2200 solver.cpp:244]     Train net output #0: loss = 1.24439e-005 (* 1 = 1.24439e-005 loss)
I1101 19:35:10.120535  2200 sgd_solver.cpp:106] Iteration 5000, lr = 0.0192415
I1101 19:35:26.893158  2200 solver.cpp:228] Iteration 5100, loss = 1.16853e-005
I1101 19:35:26.893158  2200 solver.cpp:244]     Train net output #0: loss = 1.16853e-005 (* 1 = 1.16853e-005 loss)
I1101 19:35:26.893158  2200 sgd_solver.cpp:106] Iteration 5100, lr = 0.0192415
I1101 19:35:43.605828  2200 solver.cpp:228] Iteration 5200, loss = 4.22245e-006
I1101 19:35:43.605828  2200 solver.cpp:244]     Train net output #0: loss = 4.22246e-006 (* 1 = 4.22246e-006 loss)
I1101 19:35:43.605828  2200 sgd_solver.cpp:106] Iteration 5200, lr = 0.0192415
I1101 19:36:00.335024  2200 solver.cpp:228] Iteration 5300, loss = 7.60885e-006
I1101 19:36:00.335024  2200 solver.cpp:244]     Train net output #0: loss = 7.60886e-006 (* 1 = 7.60886e-006 loss)
I1101 19:36:00.335024  2200 sgd_solver.cpp:106] Iteration 5300, lr = 0.0192415
I1101 19:36:16.920490  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_5400.caffemodel
I1101 19:36:17.058087  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_5400.solverstate
I1101 19:36:17.114130  2200 solver.cpp:337] Iteration 5400, Testing net (#0)
I1101 19:36:22.195832  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9974
I1101 19:36:22.195832  2200 solver.cpp:404]     Test net output #1: loss = 0.0144234 (* 1 = 0.0144234 loss)
I1101 19:36:22.265107  2200 solver.cpp:228] Iteration 5400, loss = 7.67376e-006
I1101 19:36:22.265107  2200 solver.cpp:244]     Train net output #0: loss = 7.67377e-006 (* 1 = 7.67377e-006 loss)
I1101 19:36:22.265107  2200 sgd_solver.cpp:106] Iteration 5400, lr = 0.0192415
I1101 19:36:38.945612  2200 solver.cpp:228] Iteration 5500, loss = 8.4256e-006
I1101 19:36:38.945612  2200 solver.cpp:244]     Train net output #0: loss = 8.42561e-006 (* 1 = 8.42561e-006 loss)
I1101 19:36:38.945612  2200 sgd_solver.cpp:106] Iteration 5500, lr = 0.0192415
I1101 19:36:55.644788  2200 solver.cpp:228] Iteration 5600, loss = 1.12147e-005
I1101 19:36:55.644788  2200 solver.cpp:244]     Train net output #0: loss = 1.12147e-005 (* 1 = 1.12147e-005 loss)
I1101 19:36:55.644788  2200 sgd_solver.cpp:106] Iteration 5600, lr = 0.0192415
I1101 19:37:12.303963  2200 solver.cpp:228] Iteration 5700, loss = 1.05664e-005
I1101 19:37:12.303963  2200 solver.cpp:244]     Train net output #0: loss = 1.05664e-005 (* 1 = 1.05664e-005 loss)
I1101 19:37:12.303963  2200 sgd_solver.cpp:106] Iteration 5700, lr = 0.0192415
I1101 19:37:29.131188  2200 solver.cpp:228] Iteration 5800, loss = 3.83143e-006
I1101 19:37:29.131188  2200 solver.cpp:244]     Train net output #0: loss = 3.83144e-006 (* 1 = 3.83144e-006 loss)
I1101 19:37:29.131188  2200 sgd_solver.cpp:106] Iteration 5800, lr = 0.0192415
I1101 19:37:45.812775  2200 solver.cpp:228] Iteration 5900, loss = 6.8989e-006
I1101 19:37:45.812775  2200 solver.cpp:244]     Train net output #0: loss = 6.89891e-006 (* 1 = 6.89891e-006 loss)
I1101 19:37:45.812775  2200 sgd_solver.cpp:106] Iteration 5900, lr = 0.0192415
I1101 19:38:02.408699  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_6000.caffemodel
I1101 19:38:02.549805  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_6000.solverstate
I1101 19:38:02.614850  2200 solver.cpp:337] Iteration 6000, Testing net (#0)
I1101 19:38:07.647109  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 19:38:07.647109  2200 solver.cpp:404]     Test net output #1: loss = 0.0145279 (* 1 = 0.0145279 loss)
I1101 19:38:07.718111  2200 solver.cpp:228] Iteration 6000, loss = 6.99183e-006
I1101 19:38:07.718111  2200 solver.cpp:244]     Train net output #0: loss = 6.99184e-006 (* 1 = 6.99184e-006 loss)
I1101 19:38:07.718611  2200 sgd_solver.cpp:106] Iteration 6000, lr = 0.0192415
I1101 19:38:24.523119  2200 solver.cpp:228] Iteration 6100, loss = 7.61966e-006
I1101 19:38:24.523119  2200 solver.cpp:244]     Train net output #0: loss = 7.61967e-006 (* 1 = 7.61967e-006 loss)
I1101 19:38:24.523119  2200 sgd_solver.cpp:106] Iteration 6100, lr = 0.0192415
I1101 19:38:41.276914  2200 solver.cpp:228] Iteration 6200, loss = 1.01769e-005
I1101 19:38:41.276914  2200 solver.cpp:244]     Train net output #0: loss = 1.01769e-005 (* 1 = 1.01769e-005 loss)
I1101 19:38:41.276914  2200 sgd_solver.cpp:106] Iteration 6200, lr = 0.0192415
I1101 19:38:57.931259  2200 solver.cpp:228] Iteration 6300, loss = 9.61731e-006
I1101 19:38:57.931259  2200 solver.cpp:244]     Train net output #0: loss = 9.61732e-006 (* 1 = 9.61732e-006 loss)
I1101 19:38:57.931259  2200 sgd_solver.cpp:106] Iteration 6300, lr = 0.0192415
I1101 19:39:14.546257  2200 solver.cpp:228] Iteration 6400, loss = 3.50836e-006
I1101 19:39:14.546257  2200 solver.cpp:244]     Train net output #0: loss = 3.50837e-006 (* 1 = 3.50837e-006 loss)
I1101 19:39:14.546257  2200 sgd_solver.cpp:106] Iteration 6400, lr = 0.0192415
I1101 19:39:31.094202  2200 solver.cpp:228] Iteration 6500, loss = 6.29446e-006
I1101 19:39:31.094202  2200 solver.cpp:244]     Train net output #0: loss = 6.29448e-006 (* 1 = 6.29448e-006 loss)
I1101 19:39:31.095201  2200 sgd_solver.cpp:106] Iteration 6500, lr = 0.0192415
I1101 19:39:47.976639  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_6600.caffemodel
I1101 19:39:48.101727  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_6600.solverstate
I1101 19:39:48.152765  2200 solver.cpp:337] Iteration 6600, Testing net (#0)
I1101 19:39:53.130875  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 19:39:53.130875  2200 solver.cpp:404]     Test net output #1: loss = 0.0146235 (* 1 = 0.0146235 loss)
I1101 19:39:53.204427  2200 solver.cpp:228] Iteration 6600, loss = 6.40767e-006
I1101 19:39:53.204427  2200 solver.cpp:244]     Train net output #0: loss = 6.40769e-006 (* 1 = 6.40769e-006 loss)
I1101 19:39:53.204427  2200 sgd_solver.cpp:106] Iteration 6600, lr = 0.0192415
I1101 19:40:09.719221  2200 solver.cpp:228] Iteration 6700, loss = 6.95082e-006
I1101 19:40:09.719221  2200 solver.cpp:244]     Train net output #0: loss = 6.95084e-006 (* 1 = 6.95084e-006 loss)
I1101 19:40:09.719221  2200 sgd_solver.cpp:106] Iteration 6700, lr = 0.0192415
I1101 19:40:26.294486  2200 solver.cpp:228] Iteration 6800, loss = 9.31726e-006
I1101 19:40:26.294486  2200 solver.cpp:244]     Train net output #0: loss = 9.31728e-006 (* 1 = 9.31728e-006 loss)
I1101 19:40:26.294486  2200 sgd_solver.cpp:106] Iteration 6800, lr = 0.0192415
I1101 19:40:42.947537  2200 solver.cpp:228] Iteration 6900, loss = 8.82683e-006
I1101 19:40:42.947537  2200 solver.cpp:244]     Train net output #0: loss = 8.82685e-006 (* 1 = 8.82685e-006 loss)
I1101 19:40:42.947537  2200 sgd_solver.cpp:106] Iteration 6900, lr = 0.0192415
I1101 19:40:59.817242  2200 solver.cpp:228] Iteration 7000, loss = 3.22582e-006
I1101 19:40:59.817242  2200 solver.cpp:244]     Train net output #0: loss = 3.22584e-006 (* 1 = 3.22584e-006 loss)
I1101 19:40:59.817242  2200 sgd_solver.cpp:106] Iteration 7000, lr = 0.0192415
I1101 19:41:16.678194  2200 solver.cpp:228] Iteration 7100, loss = 5.79374e-006
I1101 19:41:16.678194  2200 solver.cpp:244]     Train net output #0: loss = 5.79376e-006 (* 1 = 5.79376e-006 loss)
I1101 19:41:16.678194  2200 sgd_solver.cpp:106] Iteration 7100, lr = 0.0192415
I1101 19:41:32.951486  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_7200.caffemodel
I1101 19:41:33.067695  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_7200.solverstate
I1101 19:41:33.114583  2200 solver.cpp:337] Iteration 7200, Testing net (#0)
I1101 19:41:37.982906  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 19:41:37.982906  2200 solver.cpp:404]     Test net output #1: loss = 0.0147133 (* 1 = 0.0147133 loss)
I1101 19:41:38.052777  2200 solver.cpp:228] Iteration 7200, loss = 5.90696e-006
I1101 19:41:38.052777  2200 solver.cpp:244]     Train net output #0: loss = 5.90698e-006 (* 1 = 5.90698e-006 loss)
I1101 19:41:38.052777  2200 sgd_solver.cpp:106] Iteration 7200, lr = 0.0192415
I1101 19:41:54.716953  2200 solver.cpp:228] Iteration 7300, loss = 6.38214e-006
I1101 19:41:54.716953  2200 solver.cpp:244]     Train net output #0: loss = 6.38215e-006 (* 1 = 6.38215e-006 loss)
I1101 19:41:54.716953  2200 sgd_solver.cpp:106] Iteration 7300, lr = 0.0192415
I1101 19:42:11.487326  2200 solver.cpp:228] Iteration 7400, loss = 8.5769e-006
I1101 19:42:11.487326  2200 solver.cpp:244]     Train net output #0: loss = 8.57692e-006 (* 1 = 8.57692e-006 loss)
I1101 19:42:11.487326  2200 sgd_solver.cpp:106] Iteration 7400, lr = 0.0192415
I1101 19:42:28.080570  2200 solver.cpp:228] Iteration 7500, loss = 8.13056e-006
I1101 19:42:28.080570  2200 solver.cpp:244]     Train net output #0: loss = 8.13058e-006 (* 1 = 8.13058e-006 loss)
I1101 19:42:28.080570  2200 sgd_solver.cpp:106] Iteration 7500, lr = 0.0192415
I1101 19:42:44.741461  2200 solver.cpp:228] Iteration 7600, loss = 2.97906e-006
I1101 19:42:44.741461  2200 solver.cpp:244]     Train net output #0: loss = 2.97907e-006 (* 1 = 2.97907e-006 loss)
I1101 19:42:44.741461  2200 sgd_solver.cpp:106] Iteration 7600, lr = 0.0192415
I1101 19:43:01.369724  2200 solver.cpp:228] Iteration 7700, loss = 5.3586e-006
I1101 19:43:01.369724  2200 solver.cpp:244]     Train net output #0: loss = 5.35862e-006 (* 1 = 5.35862e-006 loss)
I1101 19:43:01.369724  2200 sgd_solver.cpp:106] Iteration 7700, lr = 0.0192415
I1101 19:43:17.983569  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_7800.caffemodel
I1101 19:43:18.115164  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_7800.solverstate
I1101 19:43:18.205229  2200 solver.cpp:337] Iteration 7800, Testing net (#0)
I1101 19:43:23.207721  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 19:43:23.207721  2200 solver.cpp:404]     Test net output #1: loss = 0.0147964 (* 1 = 0.0147964 loss)
I1101 19:43:23.279089  2200 solver.cpp:228] Iteration 7800, loss = 5.46467e-006
I1101 19:43:23.279089  2200 solver.cpp:244]     Train net output #0: loss = 5.46469e-006 (* 1 = 5.46469e-006 loss)
I1101 19:43:23.279089  2200 sgd_solver.cpp:106] Iteration 7800, lr = 0.0192415
I1101 19:43:40.052970  2200 solver.cpp:228] Iteration 7900, loss = 5.90407e-006
I1101 19:43:40.053467  2200 solver.cpp:244]     Train net output #0: loss = 5.90408e-006 (* 1 = 5.90408e-006 loss)
I1101 19:43:40.053467  2200 sgd_solver.cpp:106] Iteration 7900, lr = 0.0192415
I1101 19:43:56.895511  2200 solver.cpp:228] Iteration 8000, loss = 7.93907e-006
I1101 19:43:56.895511  2200 solver.cpp:244]     Train net output #0: loss = 7.93909e-006 (* 1 = 7.93909e-006 loss)
I1101 19:43:56.895511  2200 sgd_solver.cpp:106] Iteration 8000, lr = 0.0192415
I1101 19:44:14.005417  2200 solver.cpp:228] Iteration 8100, loss = 7.53922e-006
I1101 19:44:14.005417  2200 solver.cpp:244]     Train net output #0: loss = 7.53923e-006 (* 1 = 7.53923e-006 loss)
I1101 19:44:14.005417  2200 sgd_solver.cpp:106] Iteration 8100, lr = 0.0192415
I1101 19:44:31.114558  2200 solver.cpp:228] Iteration 8200, loss = 2.77521e-006
I1101 19:44:31.115058  2200 solver.cpp:244]     Train net output #0: loss = 2.77522e-006 (* 1 = 2.77522e-006 loss)
I1101 19:44:31.115058  2200 sgd_solver.cpp:106] Iteration 8200, lr = 0.0192415
I1101 19:44:48.287907  2200 solver.cpp:228] Iteration 8300, loss = 4.964e-006
I1101 19:44:48.287907  2200 solver.cpp:244]     Train net output #0: loss = 4.96402e-006 (* 1 = 4.96402e-006 loss)
I1101 19:44:48.287907  2200 sgd_solver.cpp:106] Iteration 8300, lr = 0.0192415
I1101 19:45:05.082908  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_8400.caffemodel
I1101 19:45:05.278048  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_8400.solverstate
I1101 19:45:05.332087  2200 solver.cpp:337] Iteration 8400, Testing net (#0)
I1101 19:45:10.473383  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 19:45:10.473884  2200 solver.cpp:404]     Test net output #1: loss = 0.0148745 (* 1 = 0.0148745 loss)
I1101 19:45:10.543936  2200 solver.cpp:228] Iteration 8400, loss = 5.08796e-006
I1101 19:45:10.543936  2200 solver.cpp:244]     Train net output #0: loss = 5.08797e-006 (* 1 = 5.08797e-006 loss)
I1101 19:45:10.543936  2200 sgd_solver.cpp:106] Iteration 8400, lr = 0.0192415
I1101 19:45:27.364651  2200 solver.cpp:228] Iteration 8500, loss = 5.46892e-006
I1101 19:45:27.364651  2200 solver.cpp:244]     Train net output #0: loss = 5.46893e-006 (* 1 = 5.46893e-006 loss)
I1101 19:45:27.364651  2200 sgd_solver.cpp:106] Iteration 8500, lr = 0.0192415
I1101 19:45:44.190781  2200 solver.cpp:228] Iteration 8600, loss = 7.3853e-006
I1101 19:45:44.190781  2200 solver.cpp:244]     Train net output #0: loss = 7.38531e-006 (* 1 = 7.38531e-006 loss)
I1101 19:45:44.190781  2200 sgd_solver.cpp:106] Iteration 8600, lr = 0.0192415
I1101 19:46:00.993780  2200 solver.cpp:228] Iteration 8700, loss = 7.0206e-006
I1101 19:46:00.994280  2200 solver.cpp:244]     Train net output #0: loss = 7.02061e-006 (* 1 = 7.02061e-006 loss)
I1101 19:46:00.994280  2200 sgd_solver.cpp:106] Iteration 8700, lr = 0.0192415
I1101 19:46:17.841220  2200 solver.cpp:228] Iteration 8800, loss = 2.59758e-006
I1101 19:46:17.841220  2200 solver.cpp:244]     Train net output #0: loss = 2.5976e-006 (* 1 = 2.5976e-006 loss)
I1101 19:46:17.841220  2200 sgd_solver.cpp:106] Iteration 8800, lr = 0.0192415
I1101 19:46:34.659687  2200 solver.cpp:228] Iteration 8900, loss = 4.62901e-006
I1101 19:46:34.659687  2200 solver.cpp:244]     Train net output #0: loss = 4.62902e-006 (* 1 = 4.62902e-006 loss)
I1101 19:46:34.659687  2200 sgd_solver.cpp:106] Iteration 8900, lr = 0.0192415
I1101 19:46:51.390215  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_9000.caffemodel
I1101 19:46:51.513803  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_9000.solverstate
I1101 19:46:51.564841  2200 solver.cpp:337] Iteration 9000, Testing net (#0)
I1101 19:46:56.703948  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9974
I1101 19:46:56.703948  2200 solver.cpp:404]     Test net output #1: loss = 0.0149478 (* 1 = 0.0149478 loss)
I1101 19:46:56.776608  2200 solver.cpp:228] Iteration 9000, loss = 4.7625e-006
I1101 19:46:56.776608  2200 solver.cpp:244]     Train net output #0: loss = 4.76252e-006 (* 1 = 4.76252e-006 loss)
I1101 19:46:56.776608  2200 sgd_solver.cpp:106] Iteration 9000, lr = 0.0192415
I1101 19:47:13.592504  2200 solver.cpp:228] Iteration 9100, loss = 5.10471e-006
I1101 19:47:13.592504  2200 solver.cpp:244]     Train net output #0: loss = 5.10472e-006 (* 1 = 5.10472e-006 loss)
I1101 19:47:13.592504  2200 sgd_solver.cpp:106] Iteration 9100, lr = 0.0192415
I1101 19:47:30.381693  2200 solver.cpp:228] Iteration 9200, loss = 6.89531e-006
I1101 19:47:30.381693  2200 solver.cpp:244]     Train net output #0: loss = 6.89533e-006 (* 1 = 6.89533e-006 loss)
I1101 19:47:30.381693  2200 sgd_solver.cpp:106] Iteration 9200, lr = 0.0192415
I1101 19:47:47.199189  2200 solver.cpp:228] Iteration 9300, loss = 6.57888e-006
I1101 19:47:47.199189  2200 solver.cpp:244]     Train net output #0: loss = 6.5789e-006 (* 1 = 6.5789e-006 loss)
I1101 19:47:47.199189  2200 sgd_solver.cpp:106] Iteration 9300, lr = 0.0192415
I1101 19:48:03.992041  2200 solver.cpp:228] Iteration 9400, loss = 2.43426e-006
I1101 19:48:03.992041  2200 solver.cpp:244]     Train net output #0: loss = 2.43428e-006 (* 1 = 2.43428e-006 loss)
I1101 19:48:03.992041  2200 sgd_solver.cpp:106] Iteration 9400, lr = 0.0192415
I1101 19:48:20.775939  2200 solver.cpp:228] Iteration 9500, loss = 4.34766e-006
I1101 19:48:20.775939  2200 solver.cpp:244]     Train net output #0: loss = 4.34767e-006 (* 1 = 4.34767e-006 loss)
I1101 19:48:20.775939  2200 sgd_solver.cpp:106] Iteration 9500, lr = 0.0192415
I1101 19:48:37.818958  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_9600.caffemodel
I1101 19:48:37.968066  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_9600.solverstate
I1101 19:48:38.027107  2200 solver.cpp:337] Iteration 9600, Testing net (#0)
I1101 19:48:43.139102  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9974
I1101 19:48:43.139102  2200 solver.cpp:404]     Test net output #1: loss = 0.0150177 (* 1 = 0.0150177 loss)
I1101 19:48:43.215617  2200 solver.cpp:228] Iteration 9600, loss = 4.46566e-006
I1101 19:48:43.215617  2200 solver.cpp:244]     Train net output #0: loss = 4.46567e-006 (* 1 = 4.46567e-006 loss)
I1101 19:48:43.215617  2200 sgd_solver.cpp:106] Iteration 9600, lr = 0.0192415
I1101 19:49:00.351882  2200 solver.cpp:228] Iteration 9700, loss = 4.78282e-006
I1101 19:49:00.351882  2200 solver.cpp:244]     Train net output #0: loss = 4.78284e-006 (* 1 = 4.78284e-006 loss)
I1101 19:49:00.351882  2200 sgd_solver.cpp:106] Iteration 9700, lr = 0.0192415
I1101 19:49:17.960176  2200 solver.cpp:228] Iteration 9800, loss = 6.45421e-006
I1101 19:49:17.960176  2200 solver.cpp:244]     Train net output #0: loss = 6.45422e-006 (* 1 = 6.45422e-006 loss)
I1101 19:49:17.960676  2200 sgd_solver.cpp:106] Iteration 9800, lr = 0.0192415
I1101 19:49:34.989156  2200 solver.cpp:228] Iteration 9900, loss = 6.18128e-006
I1101 19:49:34.989156  2200 solver.cpp:244]     Train net output #0: loss = 6.18129e-006 (* 1 = 6.18129e-006 loss)
I1101 19:49:34.989156  2200 sgd_solver.cpp:106] Iteration 9900, lr = 0.0192415
I1101 19:49:52.580487  2200 solver.cpp:228] Iteration 10000, loss = 2.28763e-006
I1101 19:49:52.580986  2200 solver.cpp:244]     Train net output #0: loss = 2.28765e-006 (* 1 = 2.28765e-006 loss)
I1101 19:49:52.580986  2200 sgd_solver.cpp:106] Iteration 10000, lr = 0.0192415
I1101 19:50:10.442009  2200 solver.cpp:228] Iteration 10100, loss = 4.07943e-006
I1101 19:50:10.442009  2200 solver.cpp:244]     Train net output #0: loss = 4.07944e-006 (* 1 = 4.07944e-006 loss)
I1101 19:50:10.442510  2200 sgd_solver.cpp:106] Iteration 10100, lr = 0.0192415
I1101 19:50:27.736258  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_10200.caffemodel
I1101 19:50:28.006454  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_10200.solverstate
I1101 19:50:28.118533  2200 solver.cpp:337] Iteration 10200, Testing net (#0)
I1101 19:50:33.643292  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9974
I1101 19:50:33.643292  2200 solver.cpp:404]     Test net output #1: loss = 0.0150835 (* 1 = 0.0150835 loss)
I1101 19:50:33.715343  2200 solver.cpp:228] Iteration 10200, loss = 4.21174e-006
I1101 19:50:33.715343  2200 solver.cpp:244]     Train net output #0: loss = 4.21175e-006 (* 1 = 4.21175e-006 loss)
I1101 19:50:33.715343  2200 sgd_solver.cpp:106] Iteration 10200, lr = 0.0192415
I1101 19:50:50.891582  2200 solver.cpp:228] Iteration 10300, loss = 4.49551e-006
I1101 19:50:50.891582  2200 solver.cpp:244]     Train net output #0: loss = 4.49552e-006 (* 1 = 4.49552e-006 loss)
I1101 19:50:50.891582  2200 sgd_solver.cpp:106] Iteration 10300, lr = 0.0192415
I1101 19:51:07.787703  2200 solver.cpp:228] Iteration 10400, loss = 6.08106e-006
I1101 19:51:07.787703  2200 solver.cpp:244]     Train net output #0: loss = 6.08107e-006 (* 1 = 6.08107e-006 loss)
I1101 19:51:07.787703  2200 sgd_solver.cpp:106] Iteration 10400, lr = 0.0192415
I1101 19:51:24.640102  2200 solver.cpp:228] Iteration 10500, loss = 5.82601e-006
I1101 19:51:24.640102  2200 solver.cpp:244]     Train net output #0: loss = 5.82602e-006 (* 1 = 5.82602e-006 loss)
I1101 19:51:24.640102  2200 sgd_solver.cpp:106] Iteration 10500, lr = 0.0192415
I1101 19:51:41.659958  2200 solver.cpp:228] Iteration 10600, loss = 2.15531e-006
I1101 19:51:41.659958  2200 solver.cpp:244]     Train net output #0: loss = 2.15532e-006 (* 1 = 2.15532e-006 loss)
I1101 19:51:41.659958  2200 sgd_solver.cpp:106] Iteration 10600, lr = 0.0192415
I1101 19:51:59.043246  2200 solver.cpp:228] Iteration 10700, loss = 3.85411e-006
I1101 19:51:59.043246  2200 solver.cpp:244]     Train net output #0: loss = 3.85412e-006 (* 1 = 3.85412e-006 loss)
I1101 19:51:59.043246  2200 sgd_solver.cpp:106] Iteration 10700, lr = 0.0192415
I1101 19:52:16.760540  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_10800.caffemodel
I1101 19:52:16.963188  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_10800.solverstate
I1101 19:52:17.114295  2200 solver.cpp:337] Iteration 10800, Testing net (#0)
I1101 19:52:22.384289  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 19:52:22.384289  2200 solver.cpp:404]     Test net output #1: loss = 0.0151461 (* 1 = 0.0151461 loss)
I1101 19:52:22.456826  2200 solver.cpp:228] Iteration 10800, loss = 3.96616e-006
I1101 19:52:22.456826  2200 solver.cpp:244]     Train net output #0: loss = 3.96617e-006 (* 1 = 3.96617e-006 loss)
I1101 19:52:22.456826  2200 sgd_solver.cpp:106] Iteration 10800, lr = 0.0192415
I1101 19:52:39.707741  2200 solver.cpp:228] Iteration 10900, loss = 4.238e-006
I1101 19:52:39.708240  2200 solver.cpp:244]     Train net output #0: loss = 4.23801e-006 (* 1 = 4.23801e-006 loss)
I1101 19:52:39.708240  2200 sgd_solver.cpp:106] Iteration 10900, lr = 0.0192415
I1101 19:52:56.740458  2200 solver.cpp:228] Iteration 11000, loss = 5.7246e-006
I1101 19:52:56.740458  2200 solver.cpp:244]     Train net output #0: loss = 5.72461e-006 (* 1 = 5.72461e-006 loss)
I1101 19:52:56.740458  2200 sgd_solver.cpp:106] Iteration 11000, lr = 0.0192415
I1101 19:53:13.764305  2200 solver.cpp:228] Iteration 11100, loss = 5.50412e-006
I1101 19:53:13.764305  2200 solver.cpp:244]     Train net output #0: loss = 5.50413e-006 (* 1 = 5.50413e-006 loss)
I1101 19:53:13.764305  2200 sgd_solver.cpp:106] Iteration 11100, lr = 0.0192415
I1101 19:53:30.817566  2200 solver.cpp:228] Iteration 11200, loss = 2.03968e-006
I1101 19:53:30.817566  2200 solver.cpp:244]     Train net output #0: loss = 2.03969e-006 (* 1 = 2.03969e-006 loss)
I1101 19:53:30.817566  2200 sgd_solver.cpp:106] Iteration 11200, lr = 0.0192415
I1101 19:53:48.383031  2200 solver.cpp:228] Iteration 11300, loss = 3.63237e-006
I1101 19:53:48.383031  2200 solver.cpp:244]     Train net output #0: loss = 3.63238e-006 (* 1 = 3.63238e-006 loss)
I1101 19:53:48.383031  2200 sgd_solver.cpp:106] Iteration 11300, lr = 0.0192415
I1101 19:54:05.957175  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_11400.caffemodel
I1101 19:54:06.176631  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_11400.solverstate
I1101 19:54:06.306226  2200 solver.cpp:337] Iteration 11400, Testing net (#0)
I1101 19:54:11.654150  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 19:54:11.654150  2200 solver.cpp:404]     Test net output #1: loss = 0.0152065 (* 1 = 0.0152065 loss)
I1101 19:54:11.728476  2200 solver.cpp:228] Iteration 11400, loss = 3.75396e-006
I1101 19:54:11.728476  2200 solver.cpp:244]     Train net output #0: loss = 3.75397e-006 (* 1 = 3.75397e-006 loss)
I1101 19:54:11.728476  2200 sgd_solver.cpp:106] Iteration 11400, lr = 0.0192415
I1101 19:54:29.435688  2200 solver.cpp:228] Iteration 11500, loss = 4.00672e-006
I1101 19:54:29.436189  2200 solver.cpp:244]     Train net output #0: loss = 4.00673e-006 (* 1 = 4.00673e-006 loss)
I1101 19:54:29.436189  2200 sgd_solver.cpp:106] Iteration 11500, lr = 0.0192415
I1101 19:54:47.330142  2200 solver.cpp:228] Iteration 11600, loss = 5.41583e-006
I1101 19:54:47.330142  2200 solver.cpp:244]     Train net output #0: loss = 5.41584e-006 (* 1 = 5.41584e-006 loss)
I1101 19:54:47.330142  2200 sgd_solver.cpp:106] Iteration 11600, lr = 0.0192415
I1101 19:55:04.839514  2200 solver.cpp:228] Iteration 11700, loss = 5.21441e-006
I1101 19:55:04.839514  2200 solver.cpp:244]     Train net output #0: loss = 5.21442e-006 (* 1 = 5.21442e-006 loss)
I1101 19:55:04.839514  2200 sgd_solver.cpp:106] Iteration 11700, lr = 0.0192415
I1101 19:55:21.894462  2200 solver.cpp:228] Iteration 11800, loss = 1.94908e-006
I1101 19:55:21.894462  2200 solver.cpp:244]     Train net output #0: loss = 1.94909e-006 (* 1 = 1.94909e-006 loss)
I1101 19:55:21.894961  2200 sgd_solver.cpp:106] Iteration 11800, lr = 0.0192415
I1101 19:55:39.049335  2200 solver.cpp:228] Iteration 11900, loss = 3.44282e-006
I1101 19:55:39.049335  2200 solver.cpp:244]     Train net output #0: loss = 3.44283e-006 (* 1 = 3.44283e-006 loss)
I1101 19:55:39.049335  2200 sgd_solver.cpp:106] Iteration 11900, lr = 0.0192415
I1101 19:55:56.412633  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_12000.caffemodel
I1101 19:55:56.572247  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_12000.solverstate
I1101 19:55:56.639295  2200 solver.cpp:337] Iteration 12000, Testing net (#0)
I1101 19:56:01.768685  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 19:56:01.769186  2200 solver.cpp:404]     Test net output #1: loss = 0.0152634 (* 1 = 0.0152634 loss)
I1101 19:56:01.841238  2200 solver.cpp:228] Iteration 12000, loss = 3.5656e-006
I1101 19:56:01.841739  2200 solver.cpp:244]     Train net output #0: loss = 3.56561e-006 (* 1 = 3.56561e-006 loss)
I1101 19:56:01.841739  2200 sgd_solver.cpp:106] Iteration 12000, lr = 0.0192415
I1101 19:56:18.777207  2200 solver.cpp:228] Iteration 12100, loss = 3.79452e-006
I1101 19:56:18.777207  2200 solver.cpp:244]     Train net output #0: loss = 3.79453e-006 (* 1 = 3.79453e-006 loss)
I1101 19:56:18.777207  2200 sgd_solver.cpp:106] Iteration 12100, lr = 0.0192415
I1101 19:56:35.726852  2200 solver.cpp:228] Iteration 12200, loss = 5.14044e-006
I1101 19:56:35.726852  2200 solver.cpp:244]     Train net output #0: loss = 5.14045e-006 (* 1 = 5.14045e-006 loss)
I1101 19:56:35.726852  2200 sgd_solver.cpp:106] Iteration 12200, lr = 0.0192415
I1101 19:56:52.823235  2200 solver.cpp:228] Iteration 12300, loss = 4.94975e-006
I1101 19:56:52.823235  2200 solver.cpp:244]     Train net output #0: loss = 4.94976e-006 (* 1 = 4.94976e-006 loss)
I1101 19:56:52.823235  2200 sgd_solver.cpp:106] Iteration 12300, lr = 0.0192415
I1101 19:57:09.757871  2200 solver.cpp:228] Iteration 12400, loss = 1.84536e-006
I1101 19:57:09.757871  2200 solver.cpp:244]     Train net output #0: loss = 1.84537e-006 (* 1 = 1.84537e-006 loss)
I1101 19:57:09.757871  2200 sgd_solver.cpp:106] Iteration 12400, lr = 0.0192415
I1101 19:57:26.868062  2200 solver.cpp:228] Iteration 12500, loss = 3.27116e-006
I1101 19:57:26.868062  2200 solver.cpp:244]     Train net output #0: loss = 3.27116e-006 (* 1 = 3.27116e-006 loss)
I1101 19:57:26.868062  2200 sgd_solver.cpp:106] Iteration 12500, lr = 0.0192415
I1101 19:57:43.912413  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_12600.caffemodel
I1101 19:57:44.045007  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_12600.solverstate
I1101 19:57:44.095355  2200 solver.cpp:337] Iteration 12600, Testing net (#0)
I1101 19:57:49.202383  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 19:57:49.202383  2200 solver.cpp:404]     Test net output #1: loss = 0.0153187 (* 1 = 0.0153187 loss)
I1101 19:57:49.274528  2200 solver.cpp:228] Iteration 12600, loss = 3.38678e-006
I1101 19:57:49.274528  2200 solver.cpp:244]     Train net output #0: loss = 3.38679e-006 (* 1 = 3.38679e-006 loss)
I1101 19:57:49.274528  2200 sgd_solver.cpp:106] Iteration 12600, lr = 0.0192415
I1101 19:58:06.148705  2200 solver.cpp:228] Iteration 12700, loss = 3.60616e-006
I1101 19:58:06.148705  2200 solver.cpp:244]     Train net output #0: loss = 3.60617e-006 (* 1 = 3.60617e-006 loss)
I1101 19:58:06.148705  2200 sgd_solver.cpp:106] Iteration 12700, lr = 0.0192415
I1101 19:58:23.012643  2200 solver.cpp:228] Iteration 12800, loss = 4.88293e-006
I1101 19:58:23.012643  2200 solver.cpp:244]     Train net output #0: loss = 4.88294e-006 (* 1 = 4.88294e-006 loss)
I1101 19:58:23.012643  2200 sgd_solver.cpp:106] Iteration 12800, lr = 0.0192415
I1101 19:58:40.202241  2200 solver.cpp:228] Iteration 12900, loss = 4.7125e-006
I1101 19:58:40.202241  2200 solver.cpp:244]     Train net output #0: loss = 4.71251e-006 (* 1 = 4.71251e-006 loss)
I1101 19:58:40.202241  2200 sgd_solver.cpp:106] Iteration 12900, lr = 0.0192415
I1101 19:58:57.108614  2200 solver.cpp:228] Iteration 13000, loss = 1.75119e-006
I1101 19:58:57.108614  2200 solver.cpp:244]     Train net output #0: loss = 1.7512e-006 (* 1 = 1.7512e-006 loss)
I1101 19:58:57.108614  2200 sgd_solver.cpp:106] Iteration 13000, lr = 0.0192415
I1101 19:59:13.991114  2200 solver.cpp:228] Iteration 13100, loss = 3.11022e-006
I1101 19:59:13.991114  2200 solver.cpp:244]     Train net output #0: loss = 3.11023e-006 (* 1 = 3.11023e-006 loss)
I1101 19:59:13.991114  2200 sgd_solver.cpp:106] Iteration 13100, lr = 0.0192415
I1101 19:59:30.824945  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_13200.caffemodel
I1101 19:59:30.958544  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_13200.solverstate
I1101 19:59:31.008579  2200 solver.cpp:337] Iteration 13200, Testing net (#0)
I1101 19:59:36.137662  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 19:59:36.137662  2200 solver.cpp:404]     Test net output #1: loss = 0.0153715 (* 1 = 0.0153715 loss)
I1101 19:59:36.219516  2200 solver.cpp:228] Iteration 13200, loss = 3.23061e-006
I1101 19:59:36.219516  2200 solver.cpp:244]     Train net output #0: loss = 3.23062e-006 (* 1 = 3.23062e-006 loss)
I1101 19:59:36.219516  2200 sgd_solver.cpp:106] Iteration 13200, lr = 0.0192415
I1101 19:59:53.068148  2200 solver.cpp:228] Iteration 13300, loss = 3.43211e-006
I1101 19:59:53.068148  2200 solver.cpp:244]     Train net output #0: loss = 3.43212e-006 (* 1 = 3.43212e-006 loss)
I1101 19:59:53.068148  2200 sgd_solver.cpp:106] Iteration 13300, lr = 0.0192415
I1101 20:00:10.103785  2200 solver.cpp:228] Iteration 13400, loss = 4.64331e-006
I1101 20:00:10.103785  2200 solver.cpp:244]     Train net output #0: loss = 4.64332e-006 (* 1 = 4.64332e-006 loss)
I1101 20:00:10.103785  2200 sgd_solver.cpp:106] Iteration 13400, lr = 0.0192415
I1101 20:00:26.997830  2200 solver.cpp:228] Iteration 13500, loss = 4.4991e-006
I1101 20:00:26.997830  2200 solver.cpp:244]     Train net output #0: loss = 4.49911e-006 (* 1 = 4.49911e-006 loss)
I1101 20:00:26.997830  2200 sgd_solver.cpp:106] Iteration 13500, lr = 0.0192415
I1101 20:00:43.961882  2200 solver.cpp:228] Iteration 13600, loss = 1.68204e-006
I1101 20:00:43.961882  2200 solver.cpp:244]     Train net output #0: loss = 1.68205e-006 (* 1 = 1.68205e-006 loss)
I1101 20:00:43.961882  2200 sgd_solver.cpp:106] Iteration 13600, lr = 0.0192415
I1101 20:01:01.052481  2200 solver.cpp:228] Iteration 13700, loss = 2.97312e-006
I1101 20:01:01.052481  2200 solver.cpp:244]     Train net output #0: loss = 2.97313e-006 (* 1 = 2.97313e-006 loss)
I1101 20:01:01.052481  2200 sgd_solver.cpp:106] Iteration 13700, lr = 0.0192415
I1101 20:01:18.156797  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_13800.caffemodel
I1101 20:01:18.341931  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_13800.solverstate
I1101 20:01:18.398972  2200 solver.cpp:337] Iteration 13800, Testing net (#0)
I1101 20:01:23.517657  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 20:01:23.517657  2200 solver.cpp:404]     Test net output #1: loss = 0.0154225 (* 1 = 0.0154225 loss)
I1101 20:01:23.587843  2200 solver.cpp:228] Iteration 13800, loss = 3.08398e-006
I1101 20:01:23.587843  2200 solver.cpp:244]     Train net output #0: loss = 3.08399e-006 (* 1 = 3.08399e-006 loss)
I1101 20:01:23.587843  2200 sgd_solver.cpp:106] Iteration 13800, lr = 0.0192415
I1101 20:01:40.437891  2200 solver.cpp:228] Iteration 13900, loss = 3.28786e-006
I1101 20:01:40.437891  2200 solver.cpp:244]     Train net output #0: loss = 3.28787e-006 (* 1 = 3.28787e-006 loss)
I1101 20:01:40.437891  2200 sgd_solver.cpp:106] Iteration 13900, lr = 0.0192415
I1101 20:01:57.114167  2200 solver.cpp:228] Iteration 14000, loss = 4.44184e-006
I1101 20:01:57.114167  2200 solver.cpp:244]     Train net output #0: loss = 4.44185e-006 (* 1 = 4.44185e-006 loss)
I1101 20:01:57.114167  2200 sgd_solver.cpp:106] Iteration 14000, lr = 0.0192415
I1101 20:02:14.049993  2200 solver.cpp:228] Iteration 14100, loss = 4.29286e-006
I1101 20:02:14.049993  2200 solver.cpp:244]     Train net output #0: loss = 4.29287e-006 (* 1 = 4.29287e-006 loss)
I1101 20:02:14.049993  2200 sgd_solver.cpp:106] Iteration 14100, lr = 0.0192415
I1101 20:02:31.036067  2200 solver.cpp:228] Iteration 14200, loss = 1.60933e-006
I1101 20:02:31.036067  2200 solver.cpp:244]     Train net output #0: loss = 1.60934e-006 (* 1 = 1.60934e-006 loss)
I1101 20:02:31.036067  2200 sgd_solver.cpp:106] Iteration 14200, lr = 0.0192415
I1101 20:02:47.951346  2200 solver.cpp:228] Iteration 14300, loss = 2.8396e-006
I1101 20:02:47.951346  2200 solver.cpp:244]     Train net output #0: loss = 2.83961e-006 (* 1 = 2.83961e-006 loss)
I1101 20:02:47.951346  2200 sgd_solver.cpp:106] Iteration 14300, lr = 0.0192415
I1101 20:03:05.096266  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_14400.caffemodel
I1101 20:03:05.290905  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_14400.solverstate
I1101 20:03:05.357452  2200 solver.cpp:337] Iteration 14400, Testing net (#0)
I1101 20:03:10.549556  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 20:03:10.549556  2200 solver.cpp:404]     Test net output #1: loss = 0.0154716 (* 1 = 0.0154716 loss)
I1101 20:03:10.620962  2200 solver.cpp:228] Iteration 14400, loss = 2.94927e-006
I1101 20:03:10.621462  2200 solver.cpp:244]     Train net output #0: loss = 2.94928e-006 (* 1 = 2.94928e-006 loss)
I1101 20:03:10.621462  2200 sgd_solver.cpp:106] Iteration 14400, lr = 0.0192415
I1101 20:03:27.508383  2200 solver.cpp:228] Iteration 14500, loss = 3.13645e-006
I1101 20:03:27.508383  2200 solver.cpp:244]     Train net output #0: loss = 3.13646e-006 (* 1 = 3.13646e-006 loss)
I1101 20:03:27.508383  2200 sgd_solver.cpp:106] Iteration 14500, lr = 0.0192415
I1101 20:03:44.451122  2200 solver.cpp:228] Iteration 14600, loss = 4.25348e-006
I1101 20:03:44.451122  2200 solver.cpp:244]     Train net output #0: loss = 4.25349e-006 (* 1 = 4.25349e-006 loss)
I1101 20:03:44.451122  2200 sgd_solver.cpp:106] Iteration 14600, lr = 0.0192415
I1101 20:04:01.325871  2200 solver.cpp:228] Iteration 14700, loss = 4.11165e-006
I1101 20:04:01.325871  2200 solver.cpp:244]     Train net output #0: loss = 4.11166e-006 (* 1 = 4.11166e-006 loss)
I1101 20:04:01.325871  2200 sgd_solver.cpp:106] Iteration 14700, lr = 0.0192415
I1101 20:04:18.311437  2200 solver.cpp:228] Iteration 14800, loss = 1.54376e-006
I1101 20:04:18.311437  2200 solver.cpp:244]     Train net output #0: loss = 1.54377e-006 (* 1 = 1.54377e-006 loss)
I1101 20:04:18.311437  2200 sgd_solver.cpp:106] Iteration 14800, lr = 0.0192415
I1101 20:04:35.066931  2200 solver.cpp:228] Iteration 14900, loss = 2.70847e-006
I1101 20:04:35.066931  2200 solver.cpp:244]     Train net output #0: loss = 2.70848e-006 (* 1 = 2.70848e-006 loss)
I1101 20:04:35.066931  2200 sgd_solver.cpp:106] Iteration 14900, lr = 0.0192415
I1101 20:04:51.829342  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_15000.caffemodel
I1101 20:04:51.963438  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_15000.solverstate
I1101 20:04:52.014974  2200 solver.cpp:337] Iteration 15000, Testing net (#0)
I1101 20:04:57.102757  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 20:04:57.102757  2200 solver.cpp:404]     Test net output #1: loss = 0.0155187 (* 1 = 0.0155187 loss)
I1101 20:04:57.178812  2200 solver.cpp:228] Iteration 15000, loss = 2.82529e-006
I1101 20:04:57.178812  2200 solver.cpp:244]     Train net output #0: loss = 2.8253e-006 (* 1 = 2.8253e-006 loss)
I1101 20:04:57.178812  2200 sgd_solver.cpp:106] Iteration 15000, lr = 0.0192415
I1101 20:05:14.073324  2200 solver.cpp:228] Iteration 15100, loss = 2.99101e-006
I1101 20:05:14.073324  2200 solver.cpp:244]     Train net output #0: loss = 2.99102e-006 (* 1 = 2.99102e-006 loss)
I1101 20:05:14.073324  2200 sgd_solver.cpp:106] Iteration 15100, lr = 0.0192415
I1101 20:05:31.214262  2200 solver.cpp:228] Iteration 15200, loss = 4.05916e-006
I1101 20:05:31.214262  2200 solver.cpp:244]     Train net output #0: loss = 4.05917e-006 (* 1 = 4.05917e-006 loss)
I1101 20:05:31.214262  2200 sgd_solver.cpp:106] Iteration 15200, lr = 0.0192415
I1101 20:05:48.376556  2200 solver.cpp:228] Iteration 15300, loss = 3.94713e-006
I1101 20:05:48.376556  2200 solver.cpp:244]     Train net output #0: loss = 3.94714e-006 (* 1 = 3.94714e-006 loss)
I1101 20:05:48.376556  2200 sgd_solver.cpp:106] Iteration 15300, lr = 0.0192415
I1101 20:06:05.553151  2200 solver.cpp:228] Iteration 15400, loss = 1.48058e-006
I1101 20:06:05.553151  2200 solver.cpp:244]     Train net output #0: loss = 1.48059e-006 (* 1 = 1.48059e-006 loss)
I1101 20:06:05.553151  2200 sgd_solver.cpp:106] Iteration 15400, lr = 0.0192415
I1101 20:06:22.451189  2200 solver.cpp:228] Iteration 15500, loss = 2.60475e-006
I1101 20:06:22.451189  2200 solver.cpp:244]     Train net output #0: loss = 2.60476e-006 (* 1 = 2.60476e-006 loss)
I1101 20:06:22.451189  2200 sgd_solver.cpp:106] Iteration 15500, lr = 0.0192415
I1101 20:06:39.257860  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_15600.caffemodel
I1101 20:06:39.383450  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_15600.solverstate
I1101 20:06:39.435987  2200 solver.cpp:337] Iteration 15600, Testing net (#0)
I1101 20:06:44.598826  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 20:06:44.598826  2200 solver.cpp:404]     Test net output #1: loss = 0.0155638 (* 1 = 0.0155638 loss)
I1101 20:06:44.670133  2200 solver.cpp:228] Iteration 15600, loss = 2.71442e-006
I1101 20:06:44.670133  2200 solver.cpp:244]     Train net output #0: loss = 2.71443e-006 (* 1 = 2.71443e-006 loss)
I1101 20:06:44.670133  2200 sgd_solver.cpp:106] Iteration 15600, lr = 0.0192415
I1101 20:07:01.557545  2200 solver.cpp:228] Iteration 15700, loss = 2.86107e-006
I1101 20:07:01.557545  2200 solver.cpp:244]     Train net output #0: loss = 2.86108e-006 (* 1 = 2.86108e-006 loss)
I1101 20:07:01.557545  2200 sgd_solver.cpp:106] Iteration 15700, lr = 0.0192415
I1101 20:07:18.441495  2200 solver.cpp:228] Iteration 15800, loss = 3.89464e-006
I1101 20:07:18.441495  2200 solver.cpp:244]     Train net output #0: loss = 3.89465e-006 (* 1 = 3.89465e-006 loss)
I1101 20:07:18.441495  2200 sgd_solver.cpp:106] Iteration 15800, lr = 0.0192415
I1101 20:07:35.300339  2200 solver.cpp:228] Iteration 15900, loss = 3.78976e-006
I1101 20:07:35.300339  2200 solver.cpp:244]     Train net output #0: loss = 3.78977e-006 (* 1 = 3.78977e-006 loss)
I1101 20:07:35.300339  2200 sgd_solver.cpp:106] Iteration 15900, lr = 0.0192415
I1101 20:07:52.203632  2200 solver.cpp:228] Iteration 16000, loss = 1.42813e-006
I1101 20:07:52.203632  2200 solver.cpp:244]     Train net output #0: loss = 1.42814e-006 (* 1 = 1.42814e-006 loss)
I1101 20:07:52.203632  2200 sgd_solver.cpp:106] Iteration 16000, lr = 0.0192415
I1101 20:08:08.993154  2200 solver.cpp:228] Iteration 16100, loss = 2.50223e-006
I1101 20:08:08.993154  2200 solver.cpp:244]     Train net output #0: loss = 2.50224e-006 (* 1 = 2.50224e-006 loss)
I1101 20:08:08.993154  2200 sgd_solver.cpp:106] Iteration 16100, lr = 0.0192415
I1101 20:08:25.759668  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_16200.caffemodel
I1101 20:08:25.883256  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_16200.solverstate
I1101 20:08:25.935294  2200 solver.cpp:337] Iteration 16200, Testing net (#0)
I1101 20:08:30.994325  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 20:08:30.994325  2200 solver.cpp:404]     Test net output #1: loss = 0.0156074 (* 1 = 0.0156074 loss)
I1101 20:08:31.067055  2200 solver.cpp:228] Iteration 16200, loss = 2.60594e-006
I1101 20:08:31.067055  2200 solver.cpp:244]     Train net output #0: loss = 2.60595e-006 (* 1 = 2.60595e-006 loss)
I1101 20:08:31.067055  2200 sgd_solver.cpp:106] Iteration 16200, lr = 0.0192415
I1101 20:08:47.943800  2200 solver.cpp:228] Iteration 16300, loss = 2.74662e-006
I1101 20:08:47.943800  2200 solver.cpp:244]     Train net output #0: loss = 2.74663e-006 (* 1 = 2.74663e-006 loss)
I1101 20:08:47.943800  2200 sgd_solver.cpp:106] Iteration 16300, lr = 0.0192415
I1101 20:09:04.953896  2200 solver.cpp:228] Iteration 16400, loss = 3.73966e-006
I1101 20:09:04.954396  2200 solver.cpp:244]     Train net output #0: loss = 3.73967e-006 (* 1 = 3.73967e-006 loss)
I1101 20:09:04.954396  2200 sgd_solver.cpp:106] Iteration 16400, lr = 0.0192415
I1101 20:09:21.999933  2200 solver.cpp:228] Iteration 16500, loss = 3.64432e-006
I1101 20:09:21.999933  2200 solver.cpp:244]     Train net output #0: loss = 3.64433e-006 (* 1 = 3.64433e-006 loss)
I1101 20:09:21.999933  2200 sgd_solver.cpp:106] Iteration 16500, lr = 0.0192415
I1101 20:09:39.140638  2200 solver.cpp:228] Iteration 16600, loss = 1.36733e-006
I1101 20:09:39.140638  2200 solver.cpp:244]     Train net output #0: loss = 1.36734e-006 (* 1 = 1.36734e-006 loss)
I1101 20:09:39.140638  2200 sgd_solver.cpp:106] Iteration 16600, lr = 0.0192415
I1101 20:09:56.029381  2200 solver.cpp:228] Iteration 16700, loss = 2.41043e-006
I1101 20:09:56.029381  2200 solver.cpp:244]     Train net output #0: loss = 2.41045e-006 (* 1 = 2.41045e-006 loss)
I1101 20:09:56.029381  2200 sgd_solver.cpp:106] Iteration 16700, lr = 0.0192415
I1101 20:10:12.709908  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_16800.caffemodel
I1101 20:10:12.836998  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_16800.solverstate
I1101 20:10:12.886032  2200 solver.cpp:337] Iteration 16800, Testing net (#0)
I1101 20:10:17.906503  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9975
I1101 20:10:17.906503  2200 solver.cpp:404]     Test net output #1: loss = 0.0156495 (* 1 = 0.0156495 loss)
I1101 20:10:17.978039  2200 solver.cpp:228] Iteration 16800, loss = 2.51176e-006
I1101 20:10:17.978039  2200 solver.cpp:244]     Train net output #0: loss = 2.51177e-006 (* 1 = 2.51177e-006 loss)
I1101 20:10:17.978539  2200 sgd_solver.cpp:106] Iteration 16800, lr = 0.0192415
I1101 20:10:34.595897  2200 solver.cpp:228] Iteration 16900, loss = 2.64172e-006
I1101 20:10:34.595897  2200 solver.cpp:244]     Train net output #0: loss = 2.64173e-006 (* 1 = 2.64173e-006 loss)
I1101 20:10:34.595897  2200 sgd_solver.cpp:106] Iteration 16900, lr = 0.0192415
I1101 20:10:51.330778  2200 solver.cpp:228] Iteration 17000, loss = 3.5966e-006
I1101 20:10:51.330778  2200 solver.cpp:244]     Train net output #0: loss = 3.59661e-006 (* 1 = 3.59661e-006 loss)
I1101 20:10:51.330778  2200 sgd_solver.cpp:106] Iteration 17000, lr = 0.0192415
I1101 20:11:08.402825  2200 solver.cpp:228] Iteration 17100, loss = 3.50007e-006
I1101 20:11:08.402825  2200 solver.cpp:244]     Train net output #0: loss = 3.50008e-006 (* 1 = 3.50008e-006 loss)
I1101 20:11:08.402825  2200 sgd_solver.cpp:106] Iteration 17100, lr = 0.0192415
I1101 20:11:24.762697  2200 solver.cpp:228] Iteration 17200, loss = 1.30891e-006
I1101 20:11:24.762697  2200 solver.cpp:244]     Train net output #0: loss = 1.30892e-006 (* 1 = 1.30892e-006 loss)
I1101 20:11:24.762697  2200 sgd_solver.cpp:106] Iteration 17200, lr = 0.0192415
I1101 20:11:41.247421  2200 solver.cpp:228] Iteration 17300, loss = 2.31268e-006
I1101 20:11:41.247421  2200 solver.cpp:244]     Train net output #0: loss = 2.31269e-006 (* 1 = 2.31269e-006 loss)
I1101 20:11:41.247421  2200 sgd_solver.cpp:106] Iteration 17300, lr = 0.0192415
I1101 20:11:57.571038  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_17400.caffemodel
I1101 20:11:57.687121  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_17400.solverstate
I1101 20:11:57.735154  2200 solver.cpp:337] Iteration 17400, Testing net (#0)
I1101 20:12:02.717547  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9974
I1101 20:12:02.717547  2200 solver.cpp:404]     Test net output #1: loss = 0.0156904 (* 1 = 0.0156904 loss)
I1101 20:12:02.788622  2200 solver.cpp:228] Iteration 17400, loss = 2.41043e-006
I1101 20:12:02.788622  2200 solver.cpp:244]     Train net output #0: loss = 2.41044e-006 (* 1 = 2.41044e-006 loss)
I1101 20:12:02.788622  2200 sgd_solver.cpp:106] Iteration 17400, lr = 0.0192415
I1101 20:12:19.190259  2200 solver.cpp:228] Iteration 17500, loss = 2.53085e-006
I1101 20:12:19.190259  2200 solver.cpp:244]     Train net output #0: loss = 2.53086e-006 (* 1 = 2.53086e-006 loss)
I1101 20:12:19.190259  2200 sgd_solver.cpp:106] Iteration 17500, lr = 0.0192415
I1101 20:12:35.583964  2200 solver.cpp:228] Iteration 17600, loss = 3.46785e-006
I1101 20:12:35.583964  2200 solver.cpp:244]     Train net output #0: loss = 3.46786e-006 (* 1 = 3.46786e-006 loss)
I1101 20:12:35.583964  2200 sgd_solver.cpp:106] Iteration 17600, lr = 0.0192415
I1101 20:12:52.012624  2200 solver.cpp:228] Iteration 17700, loss = 3.37489e-006
I1101 20:12:52.012624  2200 solver.cpp:244]     Train net output #0: loss = 3.3749e-006 (* 1 = 3.3749e-006 loss)
I1101 20:12:52.012624  2200 sgd_solver.cpp:106] Iteration 17700, lr = 0.0192415
I1101 20:13:08.427342  2200 solver.cpp:228] Iteration 17800, loss = 1.26838e-006
I1101 20:13:08.427342  2200 solver.cpp:244]     Train net output #0: loss = 1.26839e-006 (* 1 = 1.26839e-006 loss)
I1101 20:13:08.427342  2200 sgd_solver.cpp:106] Iteration 17800, lr = 0.0192415
I1101 20:13:24.889292  2200 solver.cpp:228] Iteration 17900, loss = 2.23519e-006
I1101 20:13:24.889292  2200 solver.cpp:244]     Train net output #0: loss = 2.2352e-006 (* 1 = 2.2352e-006 loss)
I1101 20:13:24.889292  2200 sgd_solver.cpp:106] Iteration 17900, lr = 0.0192415
I1101 20:13:41.212009  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_18000.caffemodel
I1101 20:13:41.329093  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_18000.solverstate
I1101 20:13:41.376126  2200 solver.cpp:337] Iteration 18000, Testing net (#0)
I1101 20:13:46.337518  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9974
I1101 20:13:46.337518  2200 solver.cpp:404]     Test net output #1: loss = 0.0157302 (* 1 = 0.0157302 loss)
I1101 20:13:46.406555  2200 solver.cpp:228] Iteration 18000, loss = 2.32936e-006
I1101 20:13:46.406555  2200 solver.cpp:244]     Train net output #0: loss = 2.32938e-006 (* 1 = 2.32938e-006 loss)
I1101 20:13:46.406555  2200 sgd_solver.cpp:106] Iteration 18000, lr = 0.0192415
I1101 20:14:02.856060  2200 solver.cpp:228] Iteration 18100, loss = 2.44024e-006
I1101 20:14:02.856060  2200 solver.cpp:244]     Train net output #0: loss = 2.44026e-006 (* 1 = 2.44026e-006 loss)
I1101 20:14:02.856060  2200 sgd_solver.cpp:106] Iteration 18100, lr = 0.0192415
I1101 20:14:19.265739  2200 solver.cpp:228] Iteration 18200, loss = 3.33553e-006
I1101 20:14:19.265739  2200 solver.cpp:244]     Train net output #0: loss = 3.33554e-006 (* 1 = 3.33554e-006 loss)
I1101 20:14:19.265739  2200 sgd_solver.cpp:106] Iteration 18200, lr = 0.0192415
I1101 20:14:35.703474  2200 solver.cpp:228] Iteration 18300, loss = 3.25091e-006
I1101 20:14:35.703474  2200 solver.cpp:244]     Train net output #0: loss = 3.25092e-006 (* 1 = 3.25092e-006 loss)
I1101 20:14:35.703474  2200 sgd_solver.cpp:106] Iteration 18300, lr = 0.0192415
I1101 20:14:52.127141  2200 solver.cpp:228] Iteration 18400, loss = 1.22308e-006
I1101 20:14:52.127141  2200 solver.cpp:244]     Train net output #0: loss = 1.22309e-006 (* 1 = 1.22309e-006 loss)
I1101 20:14:52.127141  2200 sgd_solver.cpp:106] Iteration 18400, lr = 0.0192415
I1101 20:15:08.522835  2200 solver.cpp:228] Iteration 18500, loss = 2.14817e-006
I1101 20:15:08.522835  2200 solver.cpp:244]     Train net output #0: loss = 2.14818e-006 (* 1 = 2.14818e-006 loss)
I1101 20:15:08.522835  2200 sgd_solver.cpp:106] Iteration 18500, lr = 0.0192415
I1101 20:15:24.899456  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_18600.caffemodel
I1101 20:15:25.016540  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_18600.solverstate
I1101 20:15:25.062572  2200 solver.cpp:337] Iteration 18600, Testing net (#0)
I1101 20:15:30.026280  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9974
I1101 20:15:30.026280  2200 solver.cpp:404]     Test net output #1: loss = 0.0157689 (* 1 = 0.0157689 loss)
I1101 20:15:30.095330  2200 solver.cpp:228] Iteration 18600, loss = 2.24472e-006
I1101 20:15:30.095330  2200 solver.cpp:244]     Train net output #0: loss = 2.24474e-006 (* 1 = 2.24474e-006 loss)
I1101 20:15:30.095330  2200 sgd_solver.cpp:106] Iteration 18600, lr = 0.0192415
I1101 20:15:46.474025  2200 solver.cpp:228] Iteration 18700, loss = 2.3556e-006
I1101 20:15:46.474025  2200 solver.cpp:244]     Train net output #0: loss = 2.35561e-006 (* 1 = 2.35561e-006 loss)
I1101 20:15:46.474025  2200 sgd_solver.cpp:106] Iteration 18700, lr = 0.0192415
I1101 20:16:02.845677  2200 solver.cpp:228] Iteration 18800, loss = 3.2187e-006
I1101 20:16:02.845677  2200 solver.cpp:244]     Train net output #0: loss = 3.21871e-006 (* 1 = 3.21871e-006 loss)
I1101 20:16:02.845677  2200 sgd_solver.cpp:106] Iteration 18800, lr = 0.0192415
I1101 20:16:19.894016  2200 solver.cpp:228] Iteration 18900, loss = 3.14123e-006
I1101 20:16:19.894016  2200 solver.cpp:244]     Train net output #0: loss = 3.14124e-006 (* 1 = 3.14124e-006 loss)
I1101 20:16:19.894016  2200 sgd_solver.cpp:106] Iteration 18900, lr = 0.0192415
I1101 20:16:36.848904  2200 solver.cpp:228] Iteration 19000, loss = 1.18136e-006
I1101 20:16:36.848904  2200 solver.cpp:244]     Train net output #0: loss = 1.18137e-006 (* 1 = 1.18137e-006 loss)
I1101 20:16:36.848904  2200 sgd_solver.cpp:106] Iteration 19000, lr = 0.0192415
I1101 20:16:53.791719  2200 solver.cpp:228] Iteration 19100, loss = 2.07306e-006
I1101 20:16:53.791719  2200 solver.cpp:244]     Train net output #0: loss = 2.07307e-006 (* 1 = 2.07307e-006 loss)
I1101 20:16:53.791719  2200 sgd_solver.cpp:106] Iteration 19100, lr = 0.0192415
I1101 20:17:10.654994  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_19200.caffemodel
I1101 20:17:10.780583  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_19200.solverstate
I1101 20:17:10.831120  2200 solver.cpp:337] Iteration 19200, Testing net (#0)
I1101 20:17:15.907599  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:17:15.907599  2200 solver.cpp:404]     Test net output #1: loss = 0.0158065 (* 1 = 0.0158065 loss)
I1101 20:17:15.979650  2200 solver.cpp:228] Iteration 19200, loss = 2.16604e-006
I1101 20:17:15.979650  2200 solver.cpp:244]     Train net output #0: loss = 2.16606e-006 (* 1 = 2.16606e-006 loss)
I1101 20:17:15.979650  2200 sgd_solver.cpp:106] Iteration 19200, lr = 0.0192415
I1101 20:17:32.919371  2200 solver.cpp:228] Iteration 19300, loss = 2.28169e-006
I1101 20:17:32.919371  2200 solver.cpp:244]     Train net output #0: loss = 2.2817e-006 (* 1 = 2.2817e-006 loss)
I1101 20:17:32.919371  2200 sgd_solver.cpp:106] Iteration 19300, lr = 0.0192415
I1101 20:17:50.098001  2200 solver.cpp:228] Iteration 19400, loss = 3.10902e-006
I1101 20:17:50.098001  2200 solver.cpp:244]     Train net output #0: loss = 3.10903e-006 (* 1 = 3.10903e-006 loss)
I1101 20:17:50.098001  2200 sgd_solver.cpp:106] Iteration 19400, lr = 0.0192415
I1101 20:18:06.966078  2200 solver.cpp:228] Iteration 19500, loss = 3.04347e-006
I1101 20:18:06.966078  2200 solver.cpp:244]     Train net output #0: loss = 3.04348e-006 (* 1 = 3.04348e-006 loss)
I1101 20:18:06.966078  2200 sgd_solver.cpp:106] Iteration 19500, lr = 0.0192415
I1101 20:18:23.835181  2200 solver.cpp:228] Iteration 19600, loss = 1.14798e-006
I1101 20:18:23.835681  2200 solver.cpp:244]     Train net output #0: loss = 1.14799e-006 (* 1 = 1.14799e-006 loss)
I1101 20:18:23.835681  2200 sgd_solver.cpp:106] Iteration 19600, lr = 0.0192415
I1101 20:18:40.699616  2200 solver.cpp:228] Iteration 19700, loss = 2.00154e-006
I1101 20:18:40.699616  2200 solver.cpp:244]     Train net output #0: loss = 2.00155e-006 (* 1 = 2.00155e-006 loss)
I1101 20:18:40.699616  2200 sgd_solver.cpp:106] Iteration 19700, lr = 0.0192415
I1101 20:18:57.800729  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_19800.caffemodel
I1101 20:18:57.950039  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_19800.solverstate
I1101 20:18:58.014086  2200 solver.cpp:337] Iteration 19800, Testing net (#0)
I1101 20:19:03.060667  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:19:03.060667  2200 solver.cpp:404]     Test net output #1: loss = 0.0158426 (* 1 = 0.0158426 loss)
I1101 20:19:03.130717  2200 solver.cpp:228] Iteration 19800, loss = 2.09213e-006
I1101 20:19:03.130717  2200 solver.cpp:244]     Train net output #0: loss = 2.09214e-006 (* 1 = 2.09214e-006 loss)
I1101 20:19:03.130717  2200 sgd_solver.cpp:106] Iteration 19800, lr = 0.0192415
I1101 20:19:20.133080  2200 solver.cpp:228] Iteration 19900, loss = 2.19943e-006
I1101 20:19:20.133080  2200 solver.cpp:244]     Train net output #0: loss = 2.19944e-006 (* 1 = 2.19944e-006 loss)
I1101 20:19:20.133080  2200 sgd_solver.cpp:106] Iteration 19900, lr = 0.0192415
I1101 20:19:37.032003  2200 solver.cpp:228] Iteration 20000, loss = 3.01126e-006
I1101 20:19:37.032003  2200 solver.cpp:244]     Train net output #0: loss = 3.01128e-006 (* 1 = 3.01128e-006 loss)
I1101 20:19:37.032003  2200 sgd_solver.cpp:106] Iteration 20000, lr = 0.0192415
I1101 20:19:53.850359  2200 solver.cpp:228] Iteration 20100, loss = 2.94333e-006
I1101 20:19:53.850359  2200 solver.cpp:244]     Train net output #0: loss = 2.94334e-006 (* 1 = 2.94334e-006 loss)
I1101 20:19:53.850359  2200 sgd_solver.cpp:106] Iteration 20100, lr = 0.0192415
I1101 20:20:10.395236  2200 solver.cpp:228] Iteration 20200, loss = 1.10149e-006
I1101 20:20:10.395236  2200 solver.cpp:244]     Train net output #0: loss = 1.1015e-006 (* 1 = 1.1015e-006 loss)
I1101 20:20:10.395236  2200 sgd_solver.cpp:106] Iteration 20200, lr = 0.0192415
I1101 20:20:26.794677  2200 solver.cpp:228] Iteration 20300, loss = 1.93835e-006
I1101 20:20:26.794677  2200 solver.cpp:244]     Train net output #0: loss = 1.93836e-006 (* 1 = 1.93836e-006 loss)
I1101 20:20:26.794677  2200 sgd_solver.cpp:106] Iteration 20300, lr = 0.0192415
I1101 20:20:43.356925  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_20400.caffemodel
I1101 20:20:43.479012  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_20400.solverstate
I1101 20:20:43.526044  2200 solver.cpp:337] Iteration 20400, Testing net (#0)
I1101 20:20:48.540040  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:20:48.540539  2200 solver.cpp:404]     Test net output #1: loss = 0.0158782 (* 1 = 0.0158782 loss)
I1101 20:20:48.609879  2200 solver.cpp:228] Iteration 20400, loss = 2.02895e-006
I1101 20:20:48.609879  2200 solver.cpp:244]     Train net output #0: loss = 2.02896e-006 (* 1 = 2.02896e-006 loss)
I1101 20:20:48.609879  2200 sgd_solver.cpp:106] Iteration 20400, lr = 0.0192415
I1101 20:21:05.273151  2200 solver.cpp:228] Iteration 20500, loss = 2.12671e-006
I1101 20:21:05.273151  2200 solver.cpp:244]     Train net output #0: loss = 2.12672e-006 (* 1 = 2.12672e-006 loss)
I1101 20:21:05.273151  2200 sgd_solver.cpp:106] Iteration 20500, lr = 0.0192415
I1101 20:21:22.706514  2200 solver.cpp:228] Iteration 20600, loss = 2.90874e-006
I1101 20:21:22.706514  2200 solver.cpp:244]     Train net output #0: loss = 2.90875e-006 (* 1 = 2.90875e-006 loss)
I1101 20:21:22.706514  2200 sgd_solver.cpp:106] Iteration 20600, lr = 0.0192415
I1101 20:21:40.117969  2200 solver.cpp:228] Iteration 20700, loss = 2.85035e-006
I1101 20:21:40.117969  2200 solver.cpp:244]     Train net output #0: loss = 2.85036e-006 (* 1 = 2.85036e-006 loss)
I1101 20:21:40.117969  2200 sgd_solver.cpp:106] Iteration 20700, lr = 0.0192415
I1101 20:21:57.115011  2200 solver.cpp:228] Iteration 20800, loss = 1.06096e-006
I1101 20:21:57.115011  2200 solver.cpp:244]     Train net output #0: loss = 1.06097e-006 (* 1 = 1.06097e-006 loss)
I1101 20:21:57.115011  2200 sgd_solver.cpp:106] Iteration 20800, lr = 0.0192415
I1101 20:22:14.146807  2200 solver.cpp:228] Iteration 20900, loss = 1.87159e-006
I1101 20:22:14.146807  2200 solver.cpp:244]     Train net output #0: loss = 1.87161e-006 (* 1 = 1.87161e-006 loss)
I1101 20:22:14.146807  2200 sgd_solver.cpp:106] Iteration 20900, lr = 0.0192415
I1101 20:22:31.035990  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_21000.caffemodel
I1101 20:22:31.202608  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_21000.solverstate
I1101 20:22:31.293676  2200 solver.cpp:337] Iteration 21000, Testing net (#0)
I1101 20:22:36.496397  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:22:36.496397  2200 solver.cpp:404]     Test net output #1: loss = 0.0159127 (* 1 = 0.0159127 loss)
I1101 20:22:36.569461  2200 solver.cpp:228] Iteration 21000, loss = 1.97173e-006
I1101 20:22:36.569962  2200 solver.cpp:244]     Train net output #0: loss = 1.97174e-006 (* 1 = 1.97174e-006 loss)
I1101 20:22:36.569962  2200 sgd_solver.cpp:106] Iteration 21000, lr = 0.0192415
I1101 20:22:53.652396  2200 solver.cpp:228] Iteration 21100, loss = 2.05876e-006
I1101 20:22:53.652396  2200 solver.cpp:244]     Train net output #0: loss = 2.05877e-006 (* 1 = 2.05877e-006 loss)
I1101 20:22:53.652396  2200 sgd_solver.cpp:106] Iteration 21100, lr = 0.0192415
I1101 20:23:10.637485  2200 solver.cpp:228] Iteration 21200, loss = 2.82172e-006
I1101 20:23:10.637485  2200 solver.cpp:244]     Train net output #0: loss = 2.82173e-006 (* 1 = 2.82173e-006 loss)
I1101 20:23:10.637485  2200 sgd_solver.cpp:106] Iteration 21200, lr = 0.0192415
I1101 20:23:28.051458  2200 solver.cpp:228] Iteration 21300, loss = 2.76332e-006
I1101 20:23:28.051458  2200 solver.cpp:244]     Train net output #0: loss = 2.76333e-006 (* 1 = 2.76333e-006 loss)
I1101 20:23:28.051458  2200 sgd_solver.cpp:106] Iteration 21300, lr = 0.0192415
I1101 20:23:45.194751  2200 solver.cpp:228] Iteration 21400, loss = 1.03711e-006
I1101 20:23:45.194751  2200 solver.cpp:244]     Train net output #0: loss = 1.03713e-006 (* 1 = 1.03713e-006 loss)
I1101 20:23:45.194751  2200 sgd_solver.cpp:106] Iteration 21400, lr = 0.0192415
I1101 20:24:02.298545  2200 solver.cpp:228] Iteration 21500, loss = 1.82033e-006
I1101 20:24:02.298545  2200 solver.cpp:244]     Train net output #0: loss = 1.82035e-006 (* 1 = 1.82035e-006 loss)
I1101 20:24:02.298545  2200 sgd_solver.cpp:106] Iteration 21500, lr = 0.0192415
I1101 20:24:18.965327  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_21600.caffemodel
I1101 20:24:19.096420  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_21600.solverstate
I1101 20:24:19.147956  2200 solver.cpp:337] Iteration 21600, Testing net (#0)
I1101 20:24:24.193097  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:24:24.193097  2200 solver.cpp:404]     Test net output #1: loss = 0.0159465 (* 1 = 0.0159465 loss)
I1101 20:24:24.263648  2200 solver.cpp:228] Iteration 21600, loss = 1.91332e-006
I1101 20:24:24.263648  2200 solver.cpp:244]     Train net output #0: loss = 1.91333e-006 (* 1 = 1.91333e-006 loss)
I1101 20:24:24.263648  2200 sgd_solver.cpp:106] Iteration 21600, lr = 0.0192415
I1101 20:24:40.916523  2200 solver.cpp:228] Iteration 21700, loss = 1.99081e-006
I1101 20:24:40.916523  2200 solver.cpp:244]     Train net output #0: loss = 1.99082e-006 (* 1 = 1.99082e-006 loss)
I1101 20:24:40.916523  2200 sgd_solver.cpp:106] Iteration 21700, lr = 0.0192415
I1101 20:24:57.654384  2200 solver.cpp:228] Iteration 21800, loss = 2.73946e-006
I1101 20:24:57.654384  2200 solver.cpp:244]     Train net output #0: loss = 2.73947e-006 (* 1 = 2.73947e-006 loss)
I1101 20:24:57.654384  2200 sgd_solver.cpp:106] Iteration 21800, lr = 0.0192415
I1101 20:25:14.589087  2200 solver.cpp:228] Iteration 21900, loss = 2.67629e-006
I1101 20:25:14.589087  2200 solver.cpp:244]     Train net output #0: loss = 2.6763e-006 (* 1 = 2.6763e-006 loss)
I1101 20:25:14.589087  2200 sgd_solver.cpp:106] Iteration 21900, lr = 0.0192415
I1101 20:25:31.561316  2200 solver.cpp:228] Iteration 22000, loss = 1.0085e-006
I1101 20:25:31.561316  2200 solver.cpp:244]     Train net output #0: loss = 1.00851e-006 (* 1 = 1.00851e-006 loss)
I1101 20:25:31.561316  2200 sgd_solver.cpp:106] Iteration 22000, lr = 0.0192415
I1101 20:25:48.499068  2200 solver.cpp:228] Iteration 22100, loss = 1.77146e-006
I1101 20:25:48.499068  2200 solver.cpp:244]     Train net output #0: loss = 1.77147e-006 (* 1 = 1.77147e-006 loss)
I1101 20:25:48.499068  2200 sgd_solver.cpp:106] Iteration 22100, lr = 0.0192415
I1101 20:26:05.271188  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_22200.caffemodel
I1101 20:26:05.437835  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_22200.solverstate
I1101 20:26:05.518893  2200 solver.cpp:337] Iteration 22200, Testing net (#0)
I1101 20:26:10.718590  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:26:10.718590  2200 solver.cpp:404]     Test net output #1: loss = 0.0159791 (* 1 = 0.0159791 loss)
I1101 20:26:10.789844  2200 solver.cpp:228] Iteration 22200, loss = 1.86205e-006
I1101 20:26:10.789844  2200 solver.cpp:244]     Train net output #0: loss = 1.86207e-006 (* 1 = 1.86207e-006 loss)
I1101 20:26:10.789844  2200 sgd_solver.cpp:106] Iteration 22200, lr = 0.0192415
I1101 20:26:27.633119  2200 solver.cpp:228] Iteration 22300, loss = 1.93359e-006
I1101 20:26:27.633119  2200 solver.cpp:244]     Train net output #0: loss = 1.9336e-006 (* 1 = 1.9336e-006 loss)
I1101 20:26:27.633119  2200 sgd_solver.cpp:106] Iteration 22300, lr = 0.0192415
I1101 20:26:44.546717  2200 solver.cpp:228] Iteration 22400, loss = 2.65363e-006
I1101 20:26:44.546717  2200 solver.cpp:244]     Train net output #0: loss = 2.65364e-006 (* 1 = 2.65364e-006 loss)
I1101 20:26:44.546717  2200 sgd_solver.cpp:106] Iteration 22400, lr = 0.0192415
I1101 20:27:01.575745  2200 solver.cpp:228] Iteration 22500, loss = 2.60119e-006
I1101 20:27:01.575745  2200 solver.cpp:244]     Train net output #0: loss = 2.6012e-006 (* 1 = 2.6012e-006 loss)
I1101 20:27:01.575745  2200 sgd_solver.cpp:106] Iteration 22500, lr = 0.0192415
I1101 20:27:18.506428  2200 solver.cpp:228] Iteration 22600, loss = 9.85855e-007
I1101 20:27:18.506428  2200 solver.cpp:244]     Train net output #0: loss = 9.85865e-007 (* 1 = 9.85865e-007 loss)
I1101 20:27:18.506428  2200 sgd_solver.cpp:106] Iteration 22600, lr = 0.0192415
I1101 20:27:35.376498  2200 solver.cpp:228] Iteration 22700, loss = 1.71424e-006
I1101 20:27:35.376498  2200 solver.cpp:244]     Train net output #0: loss = 1.71425e-006 (* 1 = 1.71425e-006 loss)
I1101 20:27:35.376498  2200 sgd_solver.cpp:106] Iteration 22700, lr = 0.0192415
I1101 20:27:52.141613  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_22800.caffemodel
I1101 20:27:52.323243  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_22800.solverstate
I1101 20:27:52.374778  2200 solver.cpp:337] Iteration 22800, Testing net (#0)
I1101 20:27:57.456557  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:27:57.457058  2200 solver.cpp:404]     Test net output #1: loss = 0.0160113 (* 1 = 0.0160113 loss)
I1101 20:27:57.527575  2200 solver.cpp:228] Iteration 22800, loss = 1.81199e-006
I1101 20:27:57.527575  2200 solver.cpp:244]     Train net output #0: loss = 1.812e-006 (* 1 = 1.812e-006 loss)
I1101 20:27:57.527575  2200 sgd_solver.cpp:106] Iteration 22800, lr = 0.0192415
I1101 20:28:14.355612  2200 solver.cpp:228] Iteration 22900, loss = 1.88352e-006
I1101 20:28:14.355612  2200 solver.cpp:244]     Train net output #0: loss = 1.88353e-006 (* 1 = 1.88353e-006 loss)
I1101 20:28:14.355612  2200 sgd_solver.cpp:106] Iteration 22900, lr = 0.0192415
I1101 20:28:31.328495  2200 solver.cpp:228] Iteration 23000, loss = 2.57495e-006
I1101 20:28:31.328495  2200 solver.cpp:244]     Train net output #0: loss = 2.57496e-006 (* 1 = 2.57496e-006 loss)
I1101 20:28:31.328495  2200 sgd_solver.cpp:106] Iteration 23000, lr = 0.0192415
I1101 20:28:48.098433  2200 solver.cpp:228] Iteration 23100, loss = 2.52966e-006
I1101 20:28:48.098433  2200 solver.cpp:244]     Train net output #0: loss = 2.52967e-006 (* 1 = 2.52967e-006 loss)
I1101 20:28:48.098433  2200 sgd_solver.cpp:106] Iteration 23100, lr = 0.0192415
I1101 20:29:05.057405  2200 solver.cpp:228] Iteration 23200, loss = 9.63204e-007
I1101 20:29:05.057405  2200 solver.cpp:244]     Train net output #0: loss = 9.63215e-007 (* 1 = 9.63215e-007 loss)
I1101 20:29:05.057405  2200 sgd_solver.cpp:106] Iteration 23200, lr = 0.0192415
I1101 20:29:22.280752  2200 solver.cpp:228] Iteration 23300, loss = 1.65821e-006
I1101 20:29:22.280752  2200 solver.cpp:244]     Train net output #0: loss = 1.65822e-006 (* 1 = 1.65822e-006 loss)
I1101 20:29:22.280752  2200 sgd_solver.cpp:106] Iteration 23300, lr = 0.0192415
I1101 20:29:39.352274  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_23400.caffemodel
I1101 20:29:39.496877  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_23400.solverstate
I1101 20:29:39.554975  2200 solver.cpp:337] Iteration 23400, Testing net (#0)
I1101 20:29:44.715616  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:29:44.715616  2200 solver.cpp:404]     Test net output #1: loss = 0.0160426 (* 1 = 0.0160426 loss)
I1101 20:29:44.788039  2200 solver.cpp:228] Iteration 23400, loss = 1.75715e-006
I1101 20:29:44.788039  2200 solver.cpp:244]     Train net output #0: loss = 1.75716e-006 (* 1 = 1.75716e-006 loss)
I1101 20:29:44.788039  2200 sgd_solver.cpp:106] Iteration 23400, lr = 0.0192415
I1101 20:30:01.912843  2200 solver.cpp:228] Iteration 23500, loss = 1.82749e-006
I1101 20:30:01.912843  2200 solver.cpp:244]     Train net output #0: loss = 1.8275e-006 (* 1 = 1.8275e-006 loss)
I1101 20:30:01.912843  2200 sgd_solver.cpp:106] Iteration 23500, lr = 0.0192415
I1101 20:30:18.599027  2200 solver.cpp:228] Iteration 23600, loss = 2.50103e-006
I1101 20:30:18.599027  2200 solver.cpp:244]     Train net output #0: loss = 2.50104e-006 (* 1 = 2.50104e-006 loss)
I1101 20:30:18.599027  2200 sgd_solver.cpp:106] Iteration 23600, lr = 0.0192415
I1101 20:30:35.558697  2200 solver.cpp:228] Iteration 23700, loss = 2.46051e-006
I1101 20:30:35.558697  2200 solver.cpp:244]     Train net output #0: loss = 2.46053e-006 (* 1 = 2.46053e-006 loss)
I1101 20:30:35.558697  2200 sgd_solver.cpp:106] Iteration 23700, lr = 0.0192415
I1101 20:30:52.856798  2200 solver.cpp:228] Iteration 23800, loss = 9.34593e-007
I1101 20:30:52.857300  2200 solver.cpp:244]     Train net output #0: loss = 9.34604e-007 (* 1 = 9.34604e-007 loss)
I1101 20:30:52.857300  2200 sgd_solver.cpp:106] Iteration 23800, lr = 0.0192415
I1101 20:31:10.735198  2200 solver.cpp:228] Iteration 23900, loss = 1.61529e-006
I1101 20:31:10.735198  2200 solver.cpp:244]     Train net output #0: loss = 1.6153e-006 (* 1 = 1.6153e-006 loss)
I1101 20:31:10.735198  2200 sgd_solver.cpp:106] Iteration 23900, lr = 0.0192415
I1101 20:31:27.447070  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_24000.caffemodel
I1101 20:31:27.562153  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_24000.solverstate
I1101 20:31:27.610186  2200 solver.cpp:337] Iteration 24000, Testing net (#0)
I1101 20:31:32.541003  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:31:32.541003  2200 solver.cpp:404]     Test net output #1: loss = 0.0160732 (* 1 = 0.0160732 loss)
I1101 20:31:32.609088  2200 solver.cpp:228] Iteration 24000, loss = 1.70708e-006
I1101 20:31:32.609088  2200 solver.cpp:244]     Train net output #0: loss = 1.70709e-006 (* 1 = 1.70709e-006 loss)
I1101 20:31:32.609088  2200 sgd_solver.cpp:106] Iteration 24000, lr = 0.0192415
I1101 20:31:49.056422  2200 solver.cpp:228] Iteration 24100, loss = 1.77623e-006
I1101 20:31:49.056422  2200 solver.cpp:244]     Train net output #0: loss = 1.77624e-006 (* 1 = 1.77624e-006 loss)
I1101 20:31:49.056422  2200 sgd_solver.cpp:106] Iteration 24100, lr = 0.0192415
I1101 20:32:06.142746  2200 solver.cpp:228] Iteration 24200, loss = 2.43427e-006
I1101 20:32:06.143246  2200 solver.cpp:244]     Train net output #0: loss = 2.43429e-006 (* 1 = 2.43429e-006 loss)
I1101 20:32:06.143246  2200 sgd_solver.cpp:106] Iteration 24200, lr = 0.0192415
I1101 20:32:23.033453  2200 solver.cpp:228] Iteration 24300, loss = 2.39733e-006
I1101 20:32:23.033453  2200 solver.cpp:244]     Train net output #0: loss = 2.39734e-006 (* 1 = 2.39734e-006 loss)
I1101 20:32:23.033453  2200 sgd_solver.cpp:106] Iteration 24300, lr = 0.0192415
I1101 20:32:39.893971  2200 solver.cpp:228] Iteration 24400, loss = 9.07174e-007
I1101 20:32:39.893971  2200 solver.cpp:244]     Train net output #0: loss = 9.07186e-007 (* 1 = 9.07186e-007 loss)
I1101 20:32:39.893971  2200 sgd_solver.cpp:106] Iteration 24400, lr = 0.0192415
I1101 20:32:56.729132  2200 solver.cpp:228] Iteration 24500, loss = 1.57357e-006
I1101 20:32:56.729132  2200 solver.cpp:244]     Train net output #0: loss = 1.57358e-006 (* 1 = 1.57358e-006 loss)
I1101 20:32:56.729132  2200 sgd_solver.cpp:106] Iteration 24500, lr = 0.0192415
I1101 20:33:13.111436  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_24600.caffemodel
I1101 20:33:13.236515  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_24600.solverstate
I1101 20:33:13.282548  2200 solver.cpp:337] Iteration 24600, Testing net (#0)
I1101 20:33:18.300086  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:33:18.300086  2200 solver.cpp:404]     Test net output #1: loss = 0.0161036 (* 1 = 0.0161036 loss)
I1101 20:33:18.370138  2200 solver.cpp:228] Iteration 24600, loss = 1.66416e-006
I1101 20:33:18.370138  2200 solver.cpp:244]     Train net output #0: loss = 1.66417e-006 (* 1 = 1.66417e-006 loss)
I1101 20:33:18.370138  2200 sgd_solver.cpp:106] Iteration 24600, lr = 0.0192415
I1101 20:33:35.390758  2200 solver.cpp:228] Iteration 24700, loss = 1.72139e-006
I1101 20:33:35.390758  2200 solver.cpp:244]     Train net output #0: loss = 1.7214e-006 (* 1 = 1.7214e-006 loss)
I1101 20:33:35.390758  2200 sgd_solver.cpp:106] Iteration 24700, lr = 0.0192415
I1101 20:33:52.457082  2200 solver.cpp:228] Iteration 24800, loss = 2.36871e-006
I1101 20:33:52.457082  2200 solver.cpp:244]     Train net output #0: loss = 2.36872e-006 (* 1 = 2.36872e-006 loss)
I1101 20:33:52.457082  2200 sgd_solver.cpp:106] Iteration 24800, lr = 0.0192415
I1101 20:34:09.729492  2200 solver.cpp:228] Iteration 24900, loss = 2.33057e-006
I1101 20:34:09.729993  2200 solver.cpp:244]     Train net output #0: loss = 2.33058e-006 (* 1 = 2.33058e-006 loss)
I1101 20:34:09.729993  2200 sgd_solver.cpp:106] Iteration 24900, lr = 0.0192415
I1101 20:34:27.048138  2200 solver.cpp:228] Iteration 25000, loss = 8.84524e-007
I1101 20:34:27.048138  2200 solver.cpp:244]     Train net output #0: loss = 8.84536e-007 (* 1 = 8.84536e-007 loss)
I1101 20:34:27.048138  2200 sgd_solver.cpp:106] Iteration 25000, lr = 0.0192415
I1101 20:34:44.412516  2200 solver.cpp:228] Iteration 25100, loss = 1.52826e-006
I1101 20:34:44.412516  2200 solver.cpp:244]     Train net output #0: loss = 1.52828e-006 (* 1 = 1.52828e-006 loss)
I1101 20:34:44.412516  2200 sgd_solver.cpp:106] Iteration 25100, lr = 0.0192415
I1101 20:35:01.696954  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_25200.caffemodel
I1101 20:35:01.850064  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_25200.solverstate
I1101 20:35:01.916611  2200 solver.cpp:337] Iteration 25200, Testing net (#0)
I1101 20:35:07.122355  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:35:07.122355  2200 solver.cpp:404]     Test net output #1: loss = 0.0161332 (* 1 = 0.0161332 loss)
I1101 20:35:07.223028  2200 solver.cpp:228] Iteration 25200, loss = 1.62482e-006
I1101 20:35:07.223028  2200 solver.cpp:244]     Train net output #0: loss = 1.62484e-006 (* 1 = 1.62484e-006 loss)
I1101 20:35:07.223028  2200 sgd_solver.cpp:106] Iteration 25200, lr = 0.0192415
I1101 20:35:24.008481  2200 solver.cpp:228] Iteration 25300, loss = 1.67728e-006
I1101 20:35:24.008981  2200 solver.cpp:244]     Train net output #0: loss = 1.67729e-006 (* 1 = 1.67729e-006 loss)
I1101 20:35:24.008981  2200 sgd_solver.cpp:106] Iteration 25300, lr = 0.0192415
I1101 20:35:40.943958  2200 solver.cpp:228] Iteration 25400, loss = 2.30314e-006
I1101 20:35:40.943958  2200 solver.cpp:244]     Train net output #0: loss = 2.30315e-006 (* 1 = 2.30315e-006 loss)
I1101 20:35:40.943958  2200 sgd_solver.cpp:106] Iteration 25400, lr = 0.0192415
I1101 20:35:58.331393  2200 solver.cpp:228] Iteration 25500, loss = 2.27454e-006
I1101 20:35:58.331393  2200 solver.cpp:244]     Train net output #0: loss = 2.27455e-006 (* 1 = 2.27455e-006 loss)
I1101 20:35:58.331393  2200 sgd_solver.cpp:106] Iteration 25500, lr = 0.0192415
I1101 20:36:15.469092  2200 solver.cpp:228] Iteration 25600, loss = 8.58298e-007
I1101 20:36:15.469092  2200 solver.cpp:244]     Train net output #0: loss = 8.5831e-007 (* 1 = 8.5831e-007 loss)
I1101 20:36:15.469092  2200 sgd_solver.cpp:106] Iteration 25600, lr = 0.0192415
I1101 20:36:32.514132  2200 solver.cpp:228] Iteration 25700, loss = 1.48892e-006
I1101 20:36:32.514132  2200 solver.cpp:244]     Train net output #0: loss = 1.48894e-006 (* 1 = 1.48894e-006 loss)
I1101 20:36:32.514132  2200 sgd_solver.cpp:106] Iteration 25700, lr = 0.0192415
I1101 20:36:49.980123  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_25800.caffemodel
I1101 20:36:50.112216  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_25800.solverstate
I1101 20:36:50.313189  2200 solver.cpp:337] Iteration 25800, Testing net (#0)
I1101 20:36:55.499402  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:36:55.499402  2200 solver.cpp:404]     Test net output #1: loss = 0.0161619 (* 1 = 0.0161619 loss)
I1101 20:36:55.570811  2200 solver.cpp:228] Iteration 25800, loss = 1.57595e-006
I1101 20:36:55.571310  2200 solver.cpp:244]     Train net output #0: loss = 1.57596e-006 (* 1 = 1.57596e-006 loss)
I1101 20:36:55.571310  2200 sgd_solver.cpp:106] Iteration 25800, lr = 0.0192415
I1101 20:37:12.440655  2200 solver.cpp:228] Iteration 25900, loss = 1.63556e-006
I1101 20:37:12.440655  2200 solver.cpp:244]     Train net output #0: loss = 1.63557e-006 (* 1 = 1.63557e-006 loss)
I1101 20:37:12.440655  2200 sgd_solver.cpp:106] Iteration 25900, lr = 0.0192415
I1101 20:37:29.305634  2200 solver.cpp:228] Iteration 26000, loss = 2.23161e-006
I1101 20:37:29.305634  2200 solver.cpp:244]     Train net output #0: loss = 2.23162e-006 (* 1 = 2.23162e-006 loss)
I1101 20:37:29.305634  2200 sgd_solver.cpp:106] Iteration 26000, lr = 0.0192415
I1101 20:37:46.252923  2200 solver.cpp:228] Iteration 26100, loss = 2.22089e-006
I1101 20:37:46.252923  2200 solver.cpp:244]     Train net output #0: loss = 2.22091e-006 (* 1 = 2.22091e-006 loss)
I1101 20:37:46.252923  2200 sgd_solver.cpp:106] Iteration 26100, lr = 0.0192415
I1101 20:38:03.441526  2200 solver.cpp:228] Iteration 26200, loss = 8.34456e-007
I1101 20:38:03.442028  2200 solver.cpp:244]     Train net output #0: loss = 8.34468e-007 (* 1 = 8.34468e-007 loss)
I1101 20:38:03.442028  2200 sgd_solver.cpp:106] Iteration 26200, lr = 0.0192415
I1101 20:38:20.662145  2200 solver.cpp:228] Iteration 26300, loss = 1.44839e-006
I1101 20:38:20.662145  2200 solver.cpp:244]     Train net output #0: loss = 1.44841e-006 (* 1 = 1.44841e-006 loss)
I1101 20:38:20.662145  2200 sgd_solver.cpp:106] Iteration 26300, lr = 0.0192415
I1101 20:38:37.571645  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_26400.caffemodel
I1101 20:38:37.695734  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_26400.solverstate
I1101 20:38:37.747272  2200 solver.cpp:337] Iteration 26400, Testing net (#0)
I1101 20:38:42.814280  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:38:42.814280  2200 solver.cpp:404]     Test net output #1: loss = 0.0161907 (* 1 = 0.0161907 loss)
I1101 20:38:42.885838  2200 solver.cpp:228] Iteration 26400, loss = 1.5378e-006
I1101 20:38:42.885838  2200 solver.cpp:244]     Train net output #0: loss = 1.53781e-006 (* 1 = 1.53781e-006 loss)
I1101 20:38:42.885838  2200 sgd_solver.cpp:106] Iteration 26400, lr = 0.0192415
I1101 20:39:00.449101  2200 solver.cpp:228] Iteration 26500, loss = 1.58787e-006
I1101 20:39:00.449602  2200 solver.cpp:244]     Train net output #0: loss = 1.58789e-006 (* 1 = 1.58789e-006 loss)
I1101 20:39:00.449602  2200 sgd_solver.cpp:106] Iteration 26500, lr = 0.0192415
I1101 20:39:17.825146  2200 solver.cpp:228] Iteration 26600, loss = 2.17916e-006
I1101 20:39:17.825146  2200 solver.cpp:244]     Train net output #0: loss = 2.17917e-006 (* 1 = 2.17917e-006 loss)
I1101 20:39:17.825146  2200 sgd_solver.cpp:106] Iteration 26600, lr = 0.0192415
I1101 20:39:35.010426  2200 solver.cpp:228] Iteration 26700, loss = 2.16129e-006
I1101 20:39:35.010426  2200 solver.cpp:244]     Train net output #0: loss = 2.1613e-006 (* 1 = 2.1613e-006 loss)
I1101 20:39:35.010426  2200 sgd_solver.cpp:106] Iteration 26700, lr = 0.0192415
I1101 20:39:51.941534  2200 solver.cpp:228] Iteration 26800, loss = 8.12998e-007
I1101 20:39:51.941534  2200 solver.cpp:244]     Train net output #0: loss = 8.1301e-007 (* 1 = 8.1301e-007 loss)
I1101 20:39:51.941534  2200 sgd_solver.cpp:106] Iteration 26800, lr = 0.0192415
I1101 20:40:08.780922  2200 solver.cpp:228] Iteration 26900, loss = 1.4174e-006
I1101 20:40:08.780922  2200 solver.cpp:244]     Train net output #0: loss = 1.41741e-006 (* 1 = 1.41741e-006 loss)
I1101 20:40:08.780922  2200 sgd_solver.cpp:106] Iteration 26900, lr = 0.0192415
I1101 20:40:25.324213  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_27000.caffemodel
I1101 20:40:25.444299  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_27000.solverstate
I1101 20:40:25.491333  2200 solver.cpp:337] Iteration 27000, Testing net (#0)
I1101 20:40:31.053249  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:40:31.053249  2200 solver.cpp:404]     Test net output #1: loss = 0.016218 (* 1 = 0.016218 loss)
I1101 20:40:31.136809  2200 solver.cpp:228] Iteration 27000, loss = 1.49727e-006
I1101 20:40:31.136809  2200 solver.cpp:244]     Train net output #0: loss = 1.49728e-006 (* 1 = 1.49728e-006 loss)
I1101 20:40:31.136809  2200 sgd_solver.cpp:106] Iteration 27000, lr = 0.0192415
I1101 20:40:48.188482  2200 solver.cpp:228] Iteration 27100, loss = 1.55092e-006
I1101 20:40:48.188482  2200 solver.cpp:244]     Train net output #0: loss = 1.55093e-006 (* 1 = 1.55093e-006 loss)
I1101 20:40:48.188482  2200 sgd_solver.cpp:106] Iteration 27100, lr = 0.0192415
I1101 20:41:05.112918  2200 solver.cpp:228] Iteration 27200, loss = 2.12671e-006
I1101 20:41:05.112918  2200 solver.cpp:244]     Train net output #0: loss = 2.12672e-006 (* 1 = 2.12672e-006 loss)
I1101 20:41:05.112918  2200 sgd_solver.cpp:106] Iteration 27200, lr = 0.0192415
I1101 20:41:22.021335  2200 solver.cpp:228] Iteration 27300, loss = 2.10883e-006
I1101 20:41:22.021335  2200 solver.cpp:244]     Train net output #0: loss = 2.10885e-006 (* 1 = 2.10885e-006 loss)
I1101 20:41:22.021335  2200 sgd_solver.cpp:106] Iteration 27300, lr = 0.0192415
I1101 20:41:38.942399  2200 solver.cpp:228] Iteration 27400, loss = 7.975e-007
I1101 20:41:38.942399  2200 solver.cpp:244]     Train net output #0: loss = 7.97513e-007 (* 1 = 7.97513e-007 loss)
I1101 20:41:38.942399  2200 sgd_solver.cpp:106] Iteration 27400, lr = 0.0192415
I1101 20:41:55.841241  2200 solver.cpp:228] Iteration 27500, loss = 1.37567e-006
I1101 20:41:55.841241  2200 solver.cpp:244]     Train net output #0: loss = 1.37569e-006 (* 1 = 1.37569e-006 loss)
I1101 20:41:55.841241  2200 sgd_solver.cpp:106] Iteration 27500, lr = 0.0192415
I1101 20:42:12.666725  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_27600.caffemodel
I1101 20:42:12.792889  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_27600.solverstate
I1101 20:42:12.845927  2200 solver.cpp:337] Iteration 27600, Testing net (#0)
I1101 20:42:17.929694  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:42:17.929694  2200 solver.cpp:404]     Test net output #1: loss = 0.016245 (* 1 = 0.016245 loss)
I1101 20:42:18.002208  2200 solver.cpp:228] Iteration 27600, loss = 1.46985e-006
I1101 20:42:18.002208  2200 solver.cpp:244]     Train net output #0: loss = 1.46986e-006 (* 1 = 1.46986e-006 loss)
I1101 20:42:18.002208  2200 sgd_solver.cpp:106] Iteration 27600, lr = 0.0192415
I1101 20:42:35.050986  2200 solver.cpp:228] Iteration 27700, loss = 1.50681e-006
I1101 20:42:35.050986  2200 solver.cpp:244]     Train net output #0: loss = 1.50682e-006 (* 1 = 1.50682e-006 loss)
I1101 20:42:35.050986  2200 sgd_solver.cpp:106] Iteration 27700, lr = 0.0192415
I1101 20:42:51.936425  2200 solver.cpp:228] Iteration 27800, loss = 2.07187e-006
I1101 20:42:51.936425  2200 solver.cpp:244]     Train net output #0: loss = 2.07188e-006 (* 1 = 2.07188e-006 loss)
I1101 20:42:51.936425  2200 sgd_solver.cpp:106] Iteration 27800, lr = 0.0192415
I1101 20:43:08.824728  2200 solver.cpp:228] Iteration 27900, loss = 2.05757e-006
I1101 20:43:08.824728  2200 solver.cpp:244]     Train net output #0: loss = 2.05758e-006 (* 1 = 2.05758e-006 loss)
I1101 20:43:08.824728  2200 sgd_solver.cpp:106] Iteration 27900, lr = 0.0192415
I1101 20:43:25.725864  2200 solver.cpp:228] Iteration 28000, loss = 7.80811e-007
I1101 20:43:25.725864  2200 solver.cpp:244]     Train net output #0: loss = 7.80823e-007 (* 1 = 7.80823e-007 loss)
I1101 20:43:25.725864  2200 sgd_solver.cpp:106] Iteration 28000, lr = 0.0192415
I1101 20:43:42.784934  2200 solver.cpp:228] Iteration 28100, loss = 1.3411e-006
I1101 20:43:42.784934  2200 solver.cpp:244]     Train net output #0: loss = 1.34112e-006 (* 1 = 1.34112e-006 loss)
I1101 20:43:42.784934  2200 sgd_solver.cpp:106] Iteration 28100, lr = 0.0192415
I1101 20:43:59.723621  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_28200.caffemodel
I1101 20:43:59.849711  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_28200.solverstate
I1101 20:43:59.901748  2200 solver.cpp:337] Iteration 28200, Testing net (#0)
I1101 20:44:05.002187  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:44:05.002187  2200 solver.cpp:404]     Test net output #1: loss = 0.0162715 (* 1 = 0.0162715 loss)
I1101 20:44:05.074802  2200 solver.cpp:228] Iteration 28200, loss = 1.43409e-006
I1101 20:44:05.074802  2200 solver.cpp:244]     Train net output #0: loss = 1.4341e-006 (* 1 = 1.4341e-006 loss)
I1101 20:44:05.074802  2200 sgd_solver.cpp:106] Iteration 28200, lr = 0.0192415
I1101 20:44:21.983389  2200 solver.cpp:228] Iteration 28300, loss = 1.47343e-006
I1101 20:44:21.983389  2200 solver.cpp:244]     Train net output #0: loss = 1.47344e-006 (* 1 = 1.47344e-006 loss)
I1101 20:44:21.983389  2200 sgd_solver.cpp:106] Iteration 28300, lr = 0.0192415
I1101 20:44:38.897655  2200 solver.cpp:228] Iteration 28400, loss = 2.01822e-006
I1101 20:44:38.897655  2200 solver.cpp:244]     Train net output #0: loss = 2.01824e-006 (* 1 = 2.01824e-006 loss)
I1101 20:44:38.897655  2200 sgd_solver.cpp:106] Iteration 28400, lr = 0.0192415
I1101 20:44:55.801137  2200 solver.cpp:228] Iteration 28500, loss = 2.01346e-006
I1101 20:44:55.801137  2200 solver.cpp:244]     Train net output #0: loss = 2.01348e-006 (* 1 = 2.01348e-006 loss)
I1101 20:44:55.801137  2200 sgd_solver.cpp:106] Iteration 28500, lr = 0.0192415
I1101 20:45:12.680285  2200 solver.cpp:228] Iteration 28600, loss = 7.61738e-007
I1101 20:45:12.680788  2200 solver.cpp:244]     Train net output #0: loss = 7.6175e-007 (* 1 = 7.6175e-007 loss)
I1101 20:45:12.680788  2200 sgd_solver.cpp:106] Iteration 28600, lr = 0.0192415
I1101 20:45:29.588901  2200 solver.cpp:228] Iteration 28700, loss = 1.3113e-006
I1101 20:45:29.588901  2200 solver.cpp:244]     Train net output #0: loss = 1.31131e-006 (* 1 = 1.31131e-006 loss)
I1101 20:45:29.588901  2200 sgd_solver.cpp:106] Iteration 28700, lr = 0.0192415
I1101 20:45:46.406581  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_28800.caffemodel
I1101 20:45:46.534672  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_28800.solverstate
I1101 20:45:46.611923  2200 solver.cpp:337] Iteration 28800, Testing net (#0)
I1101 20:45:51.727263  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:45:51.727263  2200 solver.cpp:404]     Test net output #1: loss = 0.0162975 (* 1 = 0.0162975 loss)
I1101 20:45:51.798787  2200 solver.cpp:228] Iteration 28800, loss = 1.39475e-006
I1101 20:45:51.798787  2200 solver.cpp:244]     Train net output #0: loss = 1.39476e-006 (* 1 = 1.39476e-006 loss)
I1101 20:45:51.798787  2200 sgd_solver.cpp:106] Iteration 28800, lr = 0.0192415
I1101 20:46:08.903314  2200 solver.cpp:228] Iteration 28900, loss = 1.43767e-006
I1101 20:46:08.903314  2200 solver.cpp:244]     Train net output #0: loss = 1.43768e-006 (* 1 = 1.43768e-006 loss)
I1101 20:46:08.903314  2200 sgd_solver.cpp:106] Iteration 28900, lr = 0.0192415
I1101 20:46:25.897397  2200 solver.cpp:228] Iteration 29000, loss = 1.97412e-006
I1101 20:46:25.897397  2200 solver.cpp:244]     Train net output #0: loss = 1.97413e-006 (* 1 = 1.97413e-006 loss)
I1101 20:46:25.897397  2200 sgd_solver.cpp:106] Iteration 29000, lr = 0.0192415
I1101 20:46:42.824802  2200 solver.cpp:228] Iteration 29100, loss = 1.97055e-006
I1101 20:46:42.824802  2200 solver.cpp:244]     Train net output #0: loss = 1.97056e-006 (* 1 = 1.97056e-006 loss)
I1101 20:46:42.824802  2200 sgd_solver.cpp:106] Iteration 29100, lr = 0.0192415
I1101 20:46:59.674288  2200 solver.cpp:228] Iteration 29200, loss = 7.40281e-007
I1101 20:46:59.674288  2200 solver.cpp:244]     Train net output #0: loss = 7.40292e-007 (* 1 = 7.40292e-007 loss)
I1101 20:46:59.674288  2200 sgd_solver.cpp:106] Iteration 29200, lr = 0.0192415
I1101 20:47:16.565551  2200 solver.cpp:228] Iteration 29300, loss = 1.2815e-006
I1101 20:47:16.565551  2200 solver.cpp:244]     Train net output #0: loss = 1.28151e-006 (* 1 = 1.28151e-006 loss)
I1101 20:47:16.565551  2200 sgd_solver.cpp:106] Iteration 29300, lr = 0.0192415
I1101 20:47:33.400843  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_29400.caffemodel
I1101 20:47:33.532394  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_29400.solverstate
I1101 20:47:33.584432  2200 solver.cpp:337] Iteration 29400, Testing net (#0)
I1101 20:47:38.705224  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:47:38.705224  2200 solver.cpp:404]     Test net output #1: loss = 0.0163226 (* 1 = 0.0163226 loss)
I1101 20:47:38.777684  2200 solver.cpp:228] Iteration 29400, loss = 1.35422e-006
I1101 20:47:38.777684  2200 solver.cpp:244]     Train net output #0: loss = 1.35423e-006 (* 1 = 1.35423e-006 loss)
I1101 20:47:38.777684  2200 sgd_solver.cpp:106] Iteration 29400, lr = 0.0192415
I1101 20:47:55.643266  2200 solver.cpp:228] Iteration 29500, loss = 1.40548e-006
I1101 20:47:55.643266  2200 solver.cpp:244]     Train net output #0: loss = 1.40549e-006 (* 1 = 1.40549e-006 loss)
I1101 20:47:55.643266  2200 sgd_solver.cpp:106] Iteration 29500, lr = 0.0192415
I1101 20:48:12.569732  2200 solver.cpp:228] Iteration 29600, loss = 1.92166e-006
I1101 20:48:12.569732  2200 solver.cpp:244]     Train net output #0: loss = 1.92167e-006 (* 1 = 1.92167e-006 loss)
I1101 20:48:12.569732  2200 sgd_solver.cpp:106] Iteration 29600, lr = 0.0192415
I1101 20:48:29.493494  2200 solver.cpp:228] Iteration 29700, loss = 1.92882e-006
I1101 20:48:29.493494  2200 solver.cpp:244]     Train net output #0: loss = 1.92883e-006 (* 1 = 1.92883e-006 loss)
I1101 20:48:29.493494  2200 sgd_solver.cpp:106] Iteration 29700, lr = 0.0192415
I1101 20:48:46.387416  2200 solver.cpp:228] Iteration 29800, loss = 7.20015e-007
I1101 20:48:46.387416  2200 solver.cpp:244]     Train net output #0: loss = 7.20026e-007 (* 1 = 7.20026e-007 loss)
I1101 20:48:46.387416  2200 sgd_solver.cpp:106] Iteration 29800, lr = 0.0192415
I1101 20:49:03.345780  2200 solver.cpp:228] Iteration 29900, loss = 1.2505e-006
I1101 20:49:03.345780  2200 solver.cpp:244]     Train net output #0: loss = 1.25051e-006 (* 1 = 1.25051e-006 loss)
I1101 20:49:03.345780  2200 sgd_solver.cpp:106] Iteration 29900, lr = 0.0192415
I1101 20:49:20.236455  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_30000.caffemodel
I1101 20:49:20.369554  2200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_30000.solverstate
I1101 20:49:20.419586  2200 solver.cpp:337] Iteration 30000, Testing net (#0)
I1101 20:49:25.515178  2200 solver.cpp:404]     Test net output #0: accuracy = 0.9973
I1101 20:49:25.515178  2200 solver.cpp:404]     Test net output #1: loss = 0.0163475 (* 1 = 0.0163475 loss)
I1101 20:49:25.586289  2200 solver.cpp:228] Iteration 30000, loss = 1.3268e-006
I1101 20:49:25.586289  2200 solver.cpp:244]     Train net output #0: loss = 1.32681e-006 (* 1 = 1.32681e-006 loss)
I1101 20:49:25.586289  2200 sgd_solver.cpp:106] Iteration 30000, lr = 0.0192415
I1101 20:49:42.617622  2200 solver.cpp:228] Iteration 30100, loss = 1.36972e-006
I1101 20:49:42.617622  2200 solver.cpp:244]     Train net output #0: loss = 1.36973e-006 (* 1 = 1.36973e-006 loss)
I1101 20:49:42.617622  2200 sgd_solver.cpp:106] Iteration 30100, lr = 0.0192415
I1101 20:49:59.857139  2200 solver.cpp:228] Iteration 30200, loss = 1.87636e-006
I1101 20:49:59.857139  2200 solver.cpp:244]     Train net output #0: loss = 1.87637e-006 (* 1 = 1.87637e-006 loss)
I1101 20:49:59.857139  2200 sgd_solver.cpp:106] Iteration 30200, lr = 0.0192415
I1101 20:50:17.130494  2200 solver.cpp:228] Iteration 30300, loss = 1.88233e-006
I1101 20:50:17.130494  2200 solver.cpp:244]     Train net output #0: loss = 1.88234e-006 (* 1 = 1.88234e-006 loss)
I1101 20:50:17.130494  2200 sgd_solver.cpp:106] Iteration 30300, lr = 0.0192415
I1101 20:50:34.369896  2200 solver.cpp:228] Iteration 30400, loss = 7.06901e-007
I1101 20:50:34.369896  2200 solver.cpp:244]     Train net output #0: loss = 7.06913e-007 (* 1 = 7.06913e-007 loss)
I1101 20:50:34.369896  2200 sgd_solver.cpp:106] Iteration 30400, lr = 0.0192415
I1101 20:50:51.717574  2200 solver.cpp:228] Iteration 30500, loss = 1.21712e-006
I1101 20:50:51.717574  2200 solver.cpp:244]     Train net output #0: loss = 1.21714e-006 (* 1 = 1.21714e-006 loss)
I1101 20:50:51.717574  2200 sgd_solver.cpp:106] Iteration 30500, lr = 0.0192415
I1101 20:51:08.518488  2200 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_30600.caf